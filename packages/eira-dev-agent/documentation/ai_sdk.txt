TITLE: Generating Text with a Simple Prompt in AI SDK (TypeScript)
DESCRIPTION: This snippet demonstrates how to use a basic text prompt with the AI SDK's `generateText` function. It sets a static string as the prompt to instruct the model to invent a holiday and describe its traditions. The `yourModel` dependency represents an initialized language model.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
const result = await generateText({ model: yourModel, prompt: 'Invent a new holiday and describe its traditions.',
});
``` ---------------------------------------- TITLE: Implementing Multi-Step Tool Usage with maxSteps in TypeScript
DESCRIPTION: This example illustrates how to create an AI agent that solves math problems by iteratively using a calculator tool. It leverages the `maxSteps` parameter in `generateText` to allow the LLM to break down complex tasks and call external tools (`mathjs`) until a solution is found or the step limit is reached. The `structuredOutputs` option is used for the model.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#_snippet_5 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText, tool } from 'ai';
import * as mathjs from 'mathjs';
import { z } from 'zod'; const { text: answer } = await generateText({ model: openai('gpt-4o-2024-08-06', { structuredOutputs: true }), tools: { calculate: tool({ description: 'A tool for evaluating mathematical expressions. ' + 'Example expressions: ' + "'1.2 * (2 + 4.5)', '12.7 cm to inch', 'sin(45 deg) ^ 2'.", parameters: z.object({ expression: z.string() }), execute: async ({ expression }) => mathjs.evaluate(expression), }), }, maxSteps: 10, system: 'You are solving math problems. ' + 'Reason step by step. ' + 'Use the calculator when necessary. ' + 'When you give the final answer, ' + 'provide an explanation for how you arrived at it.', prompt: 'A taxi driver earns $9461 per 1-hour of work. ' + 'If he works 12 hours a day and in 1 hour ' + 'he uses 12 liters of petrol with a price of $134 for 1 liter. ' + 'How much money does he earn in one day?',
}); console.log(`ANSWER: ${answer}`);
``` ---------------------------------------- TITLE: Enabling Search Grounding with Gemini Models
DESCRIPTION: This snippet demonstrates how to use Gemini models with search grounding enabled to access the latest information via Google search. It shows how to generate text, retrieve sources, and access provider-specific metadata like grounding and safety ratings.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/56-web-search-agent.mdx#_snippet_2 LANGUAGE: typescript
CODE:
```
import { google } from '@ai-sdk/google';
import { generateText } from 'ai'; const { text, sources, providerMetadata } = await generateText({ model: google('gemini-1.5-pro', { useSearchGrounding: true, }), prompt: 'List the top 5 San Francisco news from the past week.' + 'You must include the date of each article.',
}); console.log(text);
console.log(sources); // access the grounding metadata.
const metadata = providerMetadata?.google;
const groundingMetadata = metadata?.groundingMetadata;
const safetyRatings = metadata?.safetyRatings;
``` ---------------------------------------- TITLE: Generate Text with OpenAI o3-mini using AI SDK
DESCRIPTION: This snippet demonstrates the fundamental way to use the AI SDK to interact with the OpenAI o3-mini model. It shows how to import necessary functions and the OpenAI provider, then call `generateText` with the specified model and a prompt to receive a text response.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/24-o3.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai'; const { text } = await generateText({ model: openai('o3-mini'), prompt: 'Explain the concept of quantum entanglement.',
});
``` ---------------------------------------- TITLE: Defining AI Weather Tool in Route Handler (TypeScript)
DESCRIPTION: This snippet updates the `app/api/chat/route.ts` file to integrate a new `weather` tool using the Vercel AI SDK. It defines the tool's purpose, specifies `location` as a required parameter using Zod for schema validation, and includes an `execute` function that simulates fetching weather data. This enables the AI model to call the tool and retrieve information based on user queries.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/03-nextjs-pages-router.mdx#_snippet_10 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText, tool } from 'ai';
import { z } from 'zod'; export const maxDuration = 30; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: openai('gpt-4o'), messages, tools: { weather: tool({ description: 'Get the weather in a location (fahrenheit)', parameters: z.object({ location: z.string().describe('The location to get the weather for'), }), execute: async ({ location }) => { const temperature = Math.round(Math.random() * (90 - 32) + 32); return { location, temperature, }; }, }), }, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Streaming AI Response with OpenAI - Next.js (Server)
DESCRIPTION: This server-side route handler streams responses from a language model. It uses `@ai-sdk/openai` and `ai` to interact with the OpenAI API, specifically `gpt-4o`, and streams the text response back to the client. The `maxDuration` is set to 60 seconds, and the system prompt instructs the AI to respond in Markdown format.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/25-markdown-chatbot-with-memoization.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai'; export const maxDuration = 60; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ system: 'You are a helpful assistant. Respond to the user in Markdown format.', model: openai('gpt-4o'), messages, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Defining AI Tool for Weather Retrieval in Next.js API
DESCRIPTION: This Next.js API route (`/api/chat`) defines a server-side endpoint for handling chat interactions and tool calls. It uses the `streamText` function from the `ai` module with the `openai` model and includes a custom `getWeather` tool. The tool's parameters are validated using Zod, and its `execute` function simulates fetching weather data for a given city and unit.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/70-call-tools.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
import { ToolInvocation, streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; interface Message { role: 'user' | 'assistant'; content: string; toolInvocations?: ToolInvocation[];
} export async function POST(req: Request) { const { messages }: { messages: Message[] } = await req.json(); const result = streamText({ model: openai('gpt-4o'), system: 'You are a helpful assistant.', messages, tools: { getWeather: { description: 'Get the weather for a location', parameters: z.object({ city: z.string().describe('The city to get the weather for'), unit: z .enum(['C', 'F']) .describe('The unit to display the temperature in'), }), execute: async ({ city, unit }) => { const weather = { value: 24, description: 'Sunny', }; return `It is currently ${weather.value}°${unit} and ${weather.description} in ${city}!`; }, }, }, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Handling OpenAI Assistant API Route with Next.js
DESCRIPTION: This server-side Next.js API route (`POST` handler) demonstrates how to interact with the OpenAI Assistant API. It handles creating threads, adding user messages, and streaming assistant responses back to the client using `AssistantResponse`. It also includes logic for handling `requires_action` status to submit tool outputs, ensuring continuous interaction with the assistant.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/10-openai-assistants.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
import { AssistantResponse } from 'ai';
import OpenAI from 'openai'; const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY || '',
}); // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export async function POST(req: Request) { // Parse the request body const input: { threadId: string | null; message: string; } = await req.json(); // Create a thread if needed const threadId = input.threadId ?? (await openai.beta.threads.create({})).id; // Add a message to the thread const createdMessage = await openai.beta.threads.messages.create(threadId, { role: 'user', content: input.message, }); return AssistantResponse( { threadId, messageId: createdMessage.id }, async ({ forwardStream, sendDataMessage }) => { // Run the assistant on the thread const runStream = openai.beta.threads.runs.stream(threadId, { assistant_id: process.env.ASSISTANT_ID ?? (() => { throw new Error('ASSISTANT_ID is not set'); })(), }); // forward run status would stream message deltas let runResult = await forwardStream(runStream); // status can be: queued, in_progress, requires_action, cancelling, cancelled, failed, completed, or expired while ( runResult?.status === 'requires_action' && runResult.required_action?.type === 'submit_tool_outputs' ) { const tool_outputs = runResult.required_action.submit_tool_outputs.tool_calls.map( (toolCall: any) => { const parameters = JSON.parse(toolCall.function.arguments); switch (toolCall.function.name) { // configure your tool calls here default: throw new Error( `Unknown tool call function: ${toolCall.function.name}`, ); } }, ); runResult = await forwardStream( openai.beta.threads.runs.submitToolOutputsStream( threadId, runResult.id, { tool_outputs }, ), ); } }, );
}
``` ---------------------------------------- TITLE: Install AI SDK Core Package
DESCRIPTION: Installs the core AI SDK package using npm, a foundational step for building AI-powered applications. Node.js 18+ and pnpm are required prerequisites for development.
SOURCE: https://github.com/vercel/ai/blob/main/packages/ai/README.md#_snippet_0 LANGUAGE: shell
CODE:
```
npm install ai
``` ---------------------------------------- TITLE: Creating a Next.js API Route Handler for Chat Streaming
DESCRIPTION: This TypeScript code defines a Next.js App Router API route handler at `/api/chat` for streaming AI responses. It imports `openai` from `@ai-sdk/openai` and `streamText` from `ai`. The `POST` function extracts `messages` from the request body, uses `streamText` with the `gpt-4o` model to generate a response, and then converts the result to a data stream response for the client. `maxDuration` is set to 30 seconds to allow for longer streaming responses.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: openai('gpt-4o'), messages, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Defining Chatbot API Route with AI SDK Tools (TypeScript)
DESCRIPTION: This snippet defines an API route (`POST`) for a chatbot using `@ai-sdk/openai` and `ai`. It demonstrates how to configure `streamText` with a language model (GPT-4o) and define various tools. It includes a server-side tool (`getWeatherInformation`) with an `execute` function, a client-side tool requiring user interaction (`askForConfirmation`), and an automatically executed client-side tool (`getLocation`), showcasing different tool types and their parameter definitions using Zod.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-tool-usage.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { z } from 'zod'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: openai('gpt-4o'), messages, tools: { // server-side tool with execute function: getWeatherInformation: { description: 'show the weather in a given city to the user', parameters: z.object({ city: z.string() }), execute: async ({}: { city: string }) => { const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy']; return weatherOptions[ Math.floor(Math.random() * weatherOptions.length) ]; }, }, // client-side tool that starts user interaction: askForConfirmation: { description: 'Ask the user for confirmation.', parameters: z.object({ message: z.string().describe('The message to ask for confirmation.'), }), }, // client-side tool that is automatically executed on the client: getLocation: { description: 'Get the user location. Always ask for confirmation before using this tool.', parameters: z.object({}), }, }, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Example: Integrating a Weather Tool with streamUI
DESCRIPTION: This code snippet demonstrates how to define and integrate a `getWeather` tool within the `streamUI` function. The tool uses Zod for parameter validation and a generator function to initially show a loading state before rendering the final weather component.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/02-streaming-react-components.mdx#_snippet_2 LANGUAGE: tsx
CODE:
```
const result = await streamUI({ model: openai('gpt-4o'), prompt: 'Get the weather for San Francisco', text: ({ content }) => <div>{content}</div>, tools: { getWeather: { description: 'Get the weather for a location', parameters: z.object({ location: z.string() }), generate: async function* ({ location }) { yield <LoadingComponent />; const weather = await getWeather(location); return <WeatherComponent weather={weather} location={location} />; }, }, },
});
``` ---------------------------------------- TITLE: Generating Text with OpenAI GPT-4o in TypeScript
DESCRIPTION: This snippet demonstrates how to use the `generateText` function from the `ai` library with an OpenAI `gpt-4o` model. It shows a basic example of prompting the model to invent a new holiday and then logging the generated text to the console. This is ideal for non-interactive text generation tasks.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/01-generate-text.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai'; const { text } = await generateText({ model: openai('gpt-4o'), prompt: 'Invent a new holiday and describe its traditions.',
}); console.log(text);
``` ---------------------------------------- TITLE: Implementing Explicit Caching with Google Generative AI Models
DESCRIPTION: This snippet demonstrates how to use `GoogleAICacheManager` for guaranteed cost savings with specific Gemini 2.5 and 2.0 models. It shows the process of creating cached content with a specified Time-To-Live (TTL) and then referencing this cached content when generating text to avoid re-sending large, static parts of the prompt.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#_snippet_13 LANGUAGE: TypeScript
CODE:
```
import { google } from '@ai-sdk/google';
import { GoogleAICacheManager } from '@google/generative-ai/server';
import { generateText } from 'ai'; const cacheManager = new GoogleAICacheManager( process.env.GOOGLE_GENERATIVE_AI_API_KEY,
); // Supported models for explicit caching
type GoogleModelCacheableId = | 'models/gemini-2.5-pro' | 'models/gemini-2.5-flash' | 'models/gemini-2.0-flash' | 'models/gemini-1.5-flash-001' | 'models/gemini-1.5-pro-001'; const model: GoogleModelCacheableId = 'models/gemini-2.5-pro'; const { name: cachedContent } = await cacheManager.create({ model, contents: [ { role: 'user', parts: [{ text: '1000 Lasagna Recipes...' }], }, ], ttlSeconds: 60 * 5,
}); const { text: veggieLasagnaRecipe } = await generateText({ model: google(model, { cachedContent }), prompt: 'Write a vegetarian lasagna recipe for 4 people.',
}); const { text: meatLasagnaRecipe } = await generateText({ model: google(model, { cachedContent }), prompt: 'Write a meat lasagna recipe for 12 people.',
});
``` ---------------------------------------- TITLE: Retrieval Augmented Generation (RAG) Middleware
DESCRIPTION: This example shows how to integrate Retrieval Augmented Generation (RAG) as middleware by transforming the input parameters. It extracts the last user message, finds relevant sources, and prepends an instruction with the retrieved information to the prompt, enhancing the model's context.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/40-middleware.mdx#_snippet_8 LANGUAGE: TypeScript
CODE:
```
import type { LanguageModelV1Middleware } from 'ai'; export const yourRagMiddleware: LanguageModelV1Middleware = { transformParams: async ({ params }) => { const lastUserMessageText = getLastUserMessageText({ prompt: params.prompt, }); if (lastUserMessageText == null) { return params; // do not use RAG (send unmodified parameters) } const instruction = 'Use the following information to answer the question:\n' + findSources({ text: lastUserMessageText }) .map(chunk => JSON.stringify(chunk)) .join('\n'); return addToLastUserMessage({ params, text: instruction }); },
};
``` ---------------------------------------- TITLE: Streaming Text Generation with AI SDK and OpenAI (TypeScript)
DESCRIPTION: This snippet demonstrates how to implement text streaming using the AI SDK's `streamText` function with OpenAI's `gpt-4-turbo` model. It shows how to initialize the model, define a prompt, and iterate over the `textStream` to log parts of the generated text as they become available, improving user experience by displaying responses incrementally.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/05-streaming.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai'; const { textStream } = streamText({ model: openai('gpt-4-turbo'), prompt: 'Write a poem about embedding models.',
}); for await (const textPart of textStream) { console.log(textPart);
}
``` ---------------------------------------- TITLE: Integrating Multiple Tools for AI Chatbot with AI SDK (TypeScript)
DESCRIPTION: This TypeScript code demonstrates how to build an AI chatbot using the Vercel AI SDK, integrating two distinct tools: a 'weather' tool to fetch location-based temperatures in Celsius and a 'convertCelsiusToFahrenheit' tool to convert temperatures. It showcases a multi-step interaction where the model can sequentially call tools to gather and process information before generating a natural language response.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/06-nodejs.mdx#_snippet_9 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { CoreMessage, streamText, tool } from 'ai';
import dotenv from 'dotenv';
import { z } from 'zod';
import * as readline from 'node:readline/promises'; dotenv.config(); const terminal = readline.createInterface({ input: process.stdin, output: process.stdout,
}); const messages: CoreMessage[] = []; async function main() { while (true) { const userInput = await terminal.question('You: '); messages.push({ role: 'user', content: userInput }); const result = streamText({ model: openai('gpt-4o'), messages, tools: { weather: tool({ description: 'Get the weather in a location (in Celsius)', parameters: z.object({ location: z .string() .describe('The location to get the weather for'), }), execute: async ({ location }) => ({ location, temperature: Math.round((Math.random() * 30 + 5) * 10) / 10 // Random temp between 5°C and 35°C }), }), convertCelsiusToFahrenheit: tool({ description: 'Convert a temperature from Celsius to Fahrenheit', parameters: z.object({ celsius: z .number() .describe('The temperature in Celsius to convert'), }), execute: async ({ celsius }) => { const fahrenheit = (celsius * 9) / 5 + 32; return { fahrenheit: Math.round(fahrenheit * 100) / 100 }; } }) }, maxSteps: 5, onStepFinish: step => { console.log(JSON.stringify(step, null, 2)); } }); let fullResponse = ''; process.stdout.write('\nAssistant: '); for await (const delta of result.textStream) { fullResponse += delta; process.stdout.write(delta); } process.stdout.write('\n\n'); messages.push({ role: 'assistant', content: fullResponse }); }
} main().catch(console.error);
``` ---------------------------------------- TITLE: Generating Text Stream Response in Next.js API Route (Backend)
DESCRIPTION: This Next.js API route demonstrates how to generate a text stream response using `streamText` from the `ai` library and `openai` from `@ai-sdk/openai`. It receives a prompt from the request body, streams text from the OpenAI model, and returns the result as a streaming HTTP response. The `maxDuration` is set to allow longer streaming responses.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/50-stream-protocol.mdx#_snippet_1 LANGUAGE: ts
CODE:
```
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export async function POST(req: Request) { const { prompt }: { prompt: string } = await req.json(); const result = streamText({ model: openai('gpt-4o'), prompt, }); return result.toTextStreamResponse();
}
``` ---------------------------------------- TITLE: Generate Text with Llama 3.1 using AI SDK
DESCRIPTION: This snippet demonstrates how to generate text using the AI SDK with Llama 3.1. It provides examples for both the DeepInfra and Amazon Bedrock providers, showcasing the `generateText` function to send a prompt and receive a complete text response. The AI SDK abstracts away provider differences, allowing easy switching between models.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
import { generateText } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra'; const { text } = await generateText({ model: deepinfra('meta-llama/Meta-Llama-3.1-405B-Instruct'), prompt: 'What is love?',
});
``` LANGUAGE: tsx
CODE:
```
import { generateText } from 'ai';
import { bedrock } from '@ai-sdk/amazon-bedrock'; const { text } = await generateText({ model: bedrock('meta.llama3-1-405b-instruct-v1'), prompt: 'What is love?',
});
``` ---------------------------------------- TITLE: Generate Structured JSON Data with AI SDK
DESCRIPTION: The AI SDK Core provides `generateObject` and `streamObject` functions to produce type-safe, structured JSON output constrained by a Zod schema. This is useful for tasks like data extraction or synthetic data generation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { generateObject } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra';
import { z } from 'zod'; const { object } = await generateObject({ model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'), schema: z.object({ recipe: z.object({ name: z.string(), ingredients: z.array(z.object({ name: z.string(), amount: z.string() })), steps: z.array(z.string()) }) }), prompt: 'Generate a lasagna recipe.'
});
``` ---------------------------------------- TITLE: Streaming Custom Data and Annotations with AI SDK (Server-side)
DESCRIPTION: This snippet demonstrates how to send custom data and message annotations from a server-side route handler using `@ai-sdk/openai` and `ai` utilities. It uses `createDataStreamResponse` to initiate a data stream, `writeData` for general call annotations, `writeMessageAnnotation` for message-specific annotations, and `streamText` to generate AI model responses, merging the text stream into the data stream. It also includes error handling.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/20-streaming-data.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateId, createDataStreamResponse, streamText } from 'ai'; export async function POST(req: Request) { const { messages } = await req.json(); // immediately start streaming (solves RAG issues with status, etc.) return createDataStreamResponse({ execute: dataStream => { dataStream.writeData('initialized call'); const result = streamText({ model: openai('gpt-4o'), messages, onChunk() { dataStream.writeMessageAnnotation({ chunk: '123' }); }, onFinish() { // message annotation: dataStream.writeMessageAnnotation({ id: generateId(), // e.g. id from saved DB record other: 'information', }); // call annotation: dataStream.writeData('call completed'); }, }); result.mergeIntoDataStream(dataStream); }, onError: error => { // Error messages are masked by default for security reasons. // If you want to expose the error message to the client, you can do so here: return error instanceof Error ? error.message : String(error); }, });
}
``` ---------------------------------------- TITLE: Streaming Array of Objects with streamObject and array Output (TypeScript)
DESCRIPTION: This snippet illustrates how to use `streamObject` with the `array` output strategy to generate and stream a list of structured objects. It defines a Zod schema for individual array elements (e.g., hero descriptions) and then iterates over the `elementStream` to process each generated object as it becomes available, suitable for generating multiple structured items.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#_snippet_4 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';
import { z } from 'zod'; const { elementStream } = streamObject({ model: openai('gpt-4-turbo'), output: 'array', schema: z.object({ name: z.string(), class: z .string() .describe('Character class, e.g. warrior, mage, or thief.'), description: z.string(), }), prompt: 'Generate 3 hero descriptions for a fantasy role playing game.',
}); for await (const hero of elementStream) { console.log(hero);
}
``` ---------------------------------------- TITLE: Handling AI Assistant Messages and Tool Calls with OpenAI (TSX)
DESCRIPTION: This asynchronous function, `submitMessage`, serves as the primary entry point for user interactions with the AI assistant. It manages the lifecycle of an OpenAI thread, either creating a new one or appending to an existing one. The function streams the assistant's responses back to the client, updating both text and a dynamic UI. It also includes logic to detect and handle tool calls, specifically demonstrating how to execute a `search_emails` function and display its results within the streamed UI. Dependencies include 'ai' for streamable UI/values, 'openai' for API interaction, and local 'searchEmails' and 'Message' components.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/121-stream-assistant-response-with-tools.mdx#_snippet_2 LANGUAGE: tsx
CODE:
```
'use server'; import { generateId } from 'ai';
import { createStreamableUI, createStreamableValue } from 'ai/rsc';
import { OpenAI } from 'openai';
import { ReactNode } from 'react';
import { searchEmails } from './function';
import { Message } from './message'; const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY,
}); export interface ClientMessage { id: string; status: ReactNode; text: ReactNode; gui: ReactNode;
} const ASSISTANT_ID = 'asst_xxxx';
let THREAD_ID = '';
let RUN_ID = ''; export async function submitMessage(question: string): Promise<ClientMessage> { const status = createStreamableUI('thread.init'); const textStream = createStreamableValue(''); const textUIStream = createStreamableUI( <Message textStream={textStream.value} />, ); const gui = createStreamableUI(); const runQueue = []; (async () => { if (THREAD_ID) { await openai.beta.threads.messages.create(THREAD_ID, { role: 'user', content: question, }); const run = await openai.beta.threads.runs.create(THREAD_ID, { assistant_id: ASSISTANT_ID, stream: true, }); runQueue.push({ id: generateId(), run }); } else { const run = await openai.beta.threads.createAndRun({ assistant_id: ASSISTANT_ID, stream: true, thread: { messages: [{ role: 'user', content: question }], }, }); runQueue.push({ id: generateId(), run }); } while (runQueue.length > 0) { const latestRun = runQueue.shift(); if (latestRun) { for await (const delta of latestRun.run) { const { data, event } = delta; status.update(event); if (event === 'thread.created') { THREAD_ID = data.id; } else if (event === 'thread.run.created') { RUN_ID = data.id; } else if (event === 'thread.message.delta') { data.delta.content?.map((part: any) => { if (part.type === 'text') { if (part.text) { textStream.append(part.text.value); } } }); } else if (event === 'thread.run.requires_action') { if (data.required_action) { if (data.required_action.type === 'submit_tool_outputs') { const { tool_calls } = data.required_action.submit_tool_outputs; const tool_outputs = []; for (const tool_call of tool_calls) { const { id: toolCallId, function: fn } = tool_call; const { name, arguments: args } = fn; if (name === 'search_emails') { const { query, has_attachments } = JSON.parse(args); gui.append( <div className="flex flex-row gap-2 items-center"> <div> Searching for emails: {query}, has_attachments: {has_attachments ? 'true' : 'false'} </div> </div>, ); await new Promise(resolve => setTimeout(resolve, 2000)); const fakeEmails = searchEmails({ query, has_attachments }); gui.append( <div className="flex flex-col gap-2"> {fakeEmails.map(email => ( <div key={email.id} className="p-2 bg-zinc-100 rounded-md flex flex-row gap-2 items-center justify-between" > <div className="flex flex-row gap-2 items-center"> <div>{email.subject}</div> </div> <div className="text-zinc-500">{email.date}</div> </div> ))} </div>, ); tool_outputs.push({
``` ---------------------------------------- TITLE: Switch AI SDK Models with Unified Interface
DESCRIPTION: This example illustrates the AI SDK's unified interface, allowing easy model switching. It shows changing the model from 'o1-mini' to 'o1' by modifying a single line, demonstrating flexibility across different OpenAI models.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/23-o1.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai'; const { text } = await generateText({ model: openai('o1'), prompt: 'Explain the concept of quantum entanglement.',
});
``` ---------------------------------------- TITLE: Generating Text with OpenAI Compatible Provider in TypeScript
DESCRIPTION: This example showcases the complete process of generating text using an OpenAI compatible provider. It involves creating a provider instance, selecting a model, and then using the `generateText` function from the AI SDK with a specified prompt to receive an AI-generated response.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/index.mdx#_snippet_5 LANGUAGE: TypeScript
CODE:
```
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { generateText } from 'ai'; const provider = createOpenAICompatible({ name: 'provider-name', apiKey: process.env.PROVIDER_API_KEY, baseURL: 'https://api.provider.com/v1'
}); const { text } = await generateText({ model: provider('model-id'), prompt: 'Write a vegetarian lasagna recipe for 4 people.'
});
``` ---------------------------------------- TITLE: Configuring OpenAI API Key
DESCRIPTION: This snippet shows how to add the OpenAI API key to the `.env.local` file. The `OPENAI_API_KEY` environment variable is used by the AI SDK's OpenAI Provider for authentication.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/03-nextjs-pages-router.mdx#_snippet_6 LANGUAGE: env
CODE:
```
OPENAI_API_KEY=xxxxxxxxx
``` ---------------------------------------- TITLE: Implementing API Rate Limiting with Vercel KV and Upstash Ratelimit (TypeScript)
DESCRIPTION: This snippet demonstrates how to implement API rate limiting for a Next.js API route using Vercel KV as the Redis store and Upstash Ratelimit. It configures a fixed window limiter allowing 5 requests per 30 seconds based on the client's IP address, returning a 429 status code if the limit is exceeded. The API also processes and streams text responses using `@ai-sdk/openai`.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/06-rate-limiting.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import kv from '@vercel/kv';
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { Ratelimit } from '@upstash/ratelimit';
import { NextRequest } from 'next/server'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; // Create Rate limit
const ratelimit = new Ratelimit({ redis: kv, limiter: Ratelimit.fixedWindow(5, '30s'),
}); export async function POST(req: NextRequest) { // call ratelimit with request ip const ip = req.ip ?? 'ip'; const { success, remaining } = await ratelimit.limit(ip); // block the request if unsuccessfull if (!success) { return new Response('Ratelimited!', { status: 429 }); } const { messages } = await req.json(); const result = streamText({ model: openai('gpt-3.5-turbo'), messages, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Next.js Frontend for AI SDK Chatbot with `useChat` Hook
DESCRIPTION: This Next.js page component (`app/page.tsx`) illustrates the frontend implementation of an AI chatbot using the `@ai-sdk/react` `useChat` hook. It manages chat messages, user input, and submission, providing a reactive UI that interacts with the backend API route to display real-time AI conversations.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/23-o1.mdx#_snippet_8 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Page() { const { messages, input, handleInputChange, handleSubmit, error } = useChat(); return ( <> {messages.map(message => ( <div key={message.id}> {message.role === 'user' ? 'User: ' : 'AI: '} {message.content} </div> ))} <form onSubmit={handleSubmit}> <input name="prompt" value={input} onChange={handleInputChange} /> <button type="submit">Submit</button> </form> </> );
}
``` ---------------------------------------- TITLE: Performing Parallel Code Review with AI-SDK in TypeScript
DESCRIPTION: This snippet demonstrates the "Parallel Processing" pattern by concurrently performing security, performance, and maintainability code reviews using multiple `generateObject` calls with specialized system prompts. It then aggregates the results into a summary using `generateText`. It leverages `@ai-sdk/openai` and `zod` for structured output.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText, generateObject } from 'ai';
import { z } from 'zod'; // Example: Parallel code review with multiple specialized reviewers
async function parallelCodeReview(code: string) { const model = openai('gpt-4o'); // Run parallel reviews const [securityReview, performanceReview, maintainabilityReview] = await Promise.all([ generateObject({ model, system: 'You are an expert in code security. Focus on identifying security vulnerabilities, injection risks, and authentication issues.', schema: z.object({ vulnerabilities: z.array(z.string()), riskLevel: z.enum(['low', 'medium', 'high']), suggestions: z.array(z.string()), }), prompt: `Review this code: ${code}`, }), generateObject({ model, system: 'You are an expert in code performance. Focus on identifying performance bottlenecks, memory leaks, and optimization opportunities.', schema: z.object({ issues: z.array(z.string()), impact: z.enum(['low', 'medium', 'high']), optimizations: z.array(z.string()), }), prompt: `Review this code: ${code}`, }), generateObject({ model, system: 'You are an expert in code quality. Focus on code structure, readability, and adherence to best practices.', schema: z.object({ concerns: z.array(z.string()), qualityScore: z.number().min(1).max(10), recommendations: z.array(z.string()), }), prompt: `Review this code: ${code}`, }), ]); const reviews = [ { ...securityReview.object, type: 'security' }, { ...performanceReview.object, type: 'performance' }, { ...maintainabilityReview.object, type: 'maintainability' }, ]; // Aggregate results using another model instance const { text: summary } = await generateText({ model, system: 'You are a technical lead summarizing multiple code reviews.', prompt: `Synthesize these code review results into a concise summary with key actions: ${JSON.stringify(reviews, null, 2)}`, }); return { reviews, summary };
}
``` ---------------------------------------- TITLE: AI Model Capability Matrix
DESCRIPTION: This table provides a comprehensive overview of the features supported by various AI models from different providers. It indicates whether a model supports image input, object generation, tool usage, and tool streaming capabilities, aiding in model selection based on required functionalities.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#_snippet_16 LANGUAGE: APIDOC
CODE:
```
| Model | Image Input | Object Generation | Tool Usage | Tool Streaming |
|---|---|---|---|---|
| amazon.titan-tg1-large | No | No | No | No |
| amazon.titan-text-express-v1 | No | No | No | No |
| amazon.nova-micro-v1:0 | No | Yes | Yes | Yes |
| amazon.nova-lite-v1:0 | Yes | Yes | Yes | Yes |
| amazon.nova-pro-v1:0 | Yes | Yes | Yes | Yes |
| anthropic.claude-4-sonnet-20250514-v1:0 | Yes | Yes | Yes | Yes |
| anthropic.claude-4-opus-20250514-v1:0 | Yes | Yes | Yes | Yes |
| anthropic.claude-3-7-sonnet-20250219-v1:0 | Yes | Yes | Yes | Yes |
| anthropic.claude-3-5-sonnet-20241022-v2:0 | Yes | Yes | Yes | Yes |
| anthropic.claude-3-5-sonnet-20240620-v1:0 | Yes | Yes | Yes | Yes |
| anthropic.claude-3-5-haiku-20241022-v1:0 | No | Yes | Yes | No |
| anthropic.claude-3-opus-20240229-v1:0 | Yes | Yes | Yes | Yes |
| anthropic.claude-3-sonnet-20240229-v1:0 | Yes | Yes | Yes | Yes |
| anthropic.claude-3-haiku-20240307-v1:0 | Yes | Yes | Yes | Yes |
| anthropic.claude-v2:1 | No | No | No | No |
| cohere.command-r-v1:0 | No | No | Yes | No |
| cohere.command-r-plus-v1:0 | No | No | Yes | No |
| deepseek.r1-v1:0 | No | No | No | No |
| meta.llama2-13b-chat-v1 | No | No | No | No |
| meta.llama2-70b-chat-v1 | No | No | No | No |
| meta.llama3-8b-instruct-v1:0 | No | No | No | No |
| meta.llama3-70b-instruct-v1:0 | No | No | No | No |
| meta.llama3-1-8b-instruct-v1:0 | No | No | No | No |
| meta.llama3-1-70b-instruct-v1:0 | No | No | No | No |
| meta.llama3-1-405b-instruct-v1:0 | No | No | No | No |
| meta.llama3-2-1b-instruct-v1:0 | No | No | No | No |
| meta.llama3-2-3b-instruct-v1:0 | No | No | No | No |
| meta.llama3-2-11b-instruct-v1:0 | No | No | No | No |
| meta.llama3-2-90b-instruct-v1:0 | No | No | No | No |
| mistral.mistral-7b-instruct-v0:2 | No | No | No | No |
| mistral.mixtral-8x7b-instruct-v0:1 | No | No | No | No |
| mistral.mistral-large-2402-v1:0 | No | No | No | No |
| mistral.mistral-small-2402-v1:0 | No | No | No | No |
``` ---------------------------------------- TITLE: Creating a Server-Side Chat API Endpoint with AI SDK
DESCRIPTION: This server-side API route (`/api/chat`) handles incoming chat messages. It uses the Vercel AI SDK's `generateText` function with the `@ai-sdk/openai` package and the `gpt-4` model to generate an AI response based on the provided conversation history. The response, including the assistant's message, is then returned as JSON.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/11-generate-text-with-chat-prompt.mdx#_snippet_1 LANGUAGE: typescript
CODE:
```
import { CoreMessage, generateText } from 'ai';
import { openai } from '@ai-sdk/openai'; export async function POST(req: Request) { const { messages }: { messages: CoreMessage[] } = await req.json(); const { response } = await generateText({ model: openai('gpt-4'), system: 'You are a helpful assistant.', messages, }); return Response.json({ messages: response.messages });
}
``` ---------------------------------------- TITLE: Implementing Retrieval Augmented Generation (RAG) with AI SDK in Node.js
DESCRIPTION: This snippet demonstrates a basic Retrieval Augmented Generation (RAG) workflow. It reads an essay, chunks it, embeds the chunks using OpenAI's 'text-embedding-3-small' model, and stores them in an in-memory vector database. It then takes a user input, embeds it, retrieves the most similar chunks from the database, and uses these chunks as context to generate an answer using 'gpt-4o'. Dependencies include 'fs', 'path', 'dotenv', '@ai-sdk/openai', and 'ai'. The input is a question, and the output is the generated answer based on the provided context.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/100-retrieval-augmented-generation.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import fs from 'fs';
import path from 'path';
import dotenv from 'dotenv';
import { openai } from '@ai-sdk/openai';
import { cosineSimilarity, embed, embedMany, generateText } from 'ai'; dotenv.config(); async function main() { const db: { embedding: number[]; value: string }[] = []; const essay = fs.readFileSync(path.join(__dirname, 'essay.txt'), 'utf8'); const chunks = essay .split('.') .map(chunk => chunk.trim()) .filter(chunk => chunk.length > 0 && chunk !== '\n'); const { embeddings } = await embedMany({ model: openai.embedding('text-embedding-3-small'), values: chunks, }); embeddings.forEach((e, i) => { db.push({ embedding: e, value: chunks[i], }); }); const input = 'What were the two main things the author worked on before college?'; const { embedding } = await embed({ model: openai.embedding('text-embedding-3-small'), value: input, }); const context = db .map(item => ({ document: item, similarity: cosineSimilarity(embedding, item.embedding), })) .sort((a, b) => b.similarity - a.similarity) .slice(0, 3) .map(r => r.document.value) .join('\n'); const { text } = await generateText({ model: openai('gpt-4o'), prompt: `Answer the following question based only on the provided context: ${context} Question: ${input}`, }); console.log(text);
} main().catch(console.error);
``` ---------------------------------------- TITLE: AI SDK Tool Definition Structure
DESCRIPTION: This section outlines the essential elements required when defining a tool within the AI SDK. Each tool consists of a description for model guidance, a Zod schema for parameter validation, and an asynchronous execute function to perform the tool's action.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#_snippet_20 LANGUAGE: APIDOC
CODE:
```
tool({ description: string // A textual description of the tool's purpose, influencing when the model picks it. parameters: ZodSchema // A Zod schema defining the input parameters required for the tool to run. execute: async (args: object) => Promise<any> // An asynchronous function called with arguments extracted from the tool call.
})
``` ---------------------------------------- TITLE: Stream Custom Sources with createUIMessageStream (TypeScript)
DESCRIPTION: Demonstrates how to use `createUIMessageStream` to send structured data, such as custom RAG sources, from the server to the client UI. It shows writing a 'source' type part with URL and title.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/01-announcing-ai-sdk-5-alpha/index.mdx#_snippet_6 LANGUAGE: ts
CODE:
```
const stream = createUIMessageStream({ execute: writer => { // stream custom sources writer.write({ type: 'source', value: { type: 'source', sourceType: 'url', id: 'source-1', url: 'https://example.com', title: 'Example Source' } }); }
});
``` ---------------------------------------- TITLE: Adding Weather Tool to AI Chat Route Handler (TypeScript)
DESCRIPTION: This snippet updates the server-side route handler to include a new 'weather' tool. It uses `@ai-sdk/openai` for the model and `ai` for `streamText` and `tool` functionality, along with `zod` for schema validation of tool parameters. The `execute` function simulates fetching weather data for a given location, demonstrating how to integrate external API calls.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#_snippet_10 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText, tool } from 'ai';
import { z } from 'zod'; export const maxDuration = 30; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: openai('gpt-4o'), messages, tools: { weather: tool({ description: 'Get the weather in a location (fahrenheit)', parameters: z.object({ location: z.string().describe('The location to get the weather for'), }), execute: async ({ location }) => { const temperature = Math.round(Math.random() * (90 - 32) + 32); return { location, temperature, }; }, }), }, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Generating Structured Objects with OpenAI (TypeScript)
DESCRIPTION: This example demonstrates how to use OpenAI's structured outputs feature with `generateObject` to produce JSON objects that strictly adhere to a defined Zod schema. It highlights enabling `structuredOutputs` in the model configuration and defining the expected output structure.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_11 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateObject } from 'ai';
import { z } from 'zod'; const result = await generateObject({ model: openai('gpt-4o-2024-08-06', { structuredOutputs: true, }), schemaName: 'recipe', schemaDescription: 'A recipe for lasagna.', schema: z.object({ name: z.string(), ingredients: z.array( z.object({ name: z.string(), amount: z.string(), }), ), steps: z.array(z.string()), }), prompt: 'Generate a lasagna recipe.',
}); console.log(JSON.stringify(result.object, null, 2));
``` ---------------------------------------- TITLE: Integrating Tool Results into Chat UI in TSX
DESCRIPTION: This TSX snippet updates the main 'Page' component to dynamically render results from AI tool invocations within a chat interface. It uses 'useChat' from '@ai-sdk/react' to manage messages and input. The component iterates through 'message.toolInvocations', conditionally rendering the 'Weather' or 'Stock' component based on the 'toolName' and 'state' of the tool call, providing a rich Generative UI experience.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#_snippet_8 LANGUAGE: TSX
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react';
import { Weather } from '@/components/weather';
import { Stock } from '@/components/stock'; export default function Page() { const { messages, input, setInput, handleSubmit } = useChat(); return ( <div> {messages.map(message => ( <div key={message.id}> <div>{message.role}</div> <div>{message.content}</div> <div> {message.toolInvocations?.map(toolInvocation => { const { toolName, toolCallId, state } = toolInvocation; if (state === 'result') { if (toolName === 'displayWeather') { const { result } = toolInvocation; return ( <div key={toolCallId}> <Weather {...result} /> </div> ); } else if (toolName === 'getStockPrice') { const { result } = toolInvocation; return <Stock key={toolCallId} {...result} />; } } else { return ( <div key={toolCallId}> {toolName === 'displayWeather' ? ( <div>Loading weather...</div> ) : toolName === 'getStockPrice' ? ( <div>Loading stock price...</div> ) : ( <div>Loading...</div> )} </div> ); } })} </div> </div> ))} <form onSubmit={handleSubmit}> <input type="text" value={input} onChange={event => { setInput(event.target.value); }} /> <button type="submit">Send</button> </form> </div> );
}
``` ---------------------------------------- TITLE: Defining a `getWeather` Tool Returning JSON (TypeScript)
DESCRIPTION: This code modifies the `getWeather` tool's `execute` function to return a structured JSON object containing weather details (temperature, unit, description, forecast) instead of a plain string. This structured output is crucial for enabling the rendering of rich UI components based on the model's tool calls.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/07-rendering-ui-with-language-models.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
const text = generateText({ model: openai('gpt-3.5-turbo'), system: 'You are a friendly assistant', prompt: 'What is the weather in SF?', tools: { getWeather: { description: 'Get the weather for a location', parameters: z.object({ city: z.string().describe('The city to get the weather for'), unit: z .enum(['C', 'F']) .describe('The unit to display the temperature in'), }), execute: async ({ city, unit }) => { const weather = getWeather({ city, unit }); const { temperature, unit, description, forecast } = weather; return { temperature, unit, description, forecast, }; }, }, },
});
``` ---------------------------------------- TITLE: Managing Conversation UI State and User Input (React Client Component)
DESCRIPTION: This client-side `Home` component manages the user interface for the conversation. It uses `useUIState` to display the conversation history and `useState` for user input. The `onClick` handler for the "Send" button updates the UI with the user's message, calls the `continueConversation` server action, and then updates the UI again with the AI's response, providing an interactive chat experience.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/61-restore-messages-from-database.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
'use client'; import { useState, useEffect } from 'react';
import { ClientMessage } from './actions';
import { useActions, useUIState } from 'ai/rsc';
import { generateId } from 'ai'; export default function Home() { const [conversation, setConversation] = useUIState(); const [input, setInput] = useState<string>(''); const { continueConversation } = useActions(); return ( <div> <div className="conversation-history"> {conversation.map((message: ClientMessage) => ( <div key={message.id} className={`message ${message.role}`}> {message.role}: {message.display} </div> ))} </div> <div className="input-area"> <input type="text" value={input} onChange={e => setInput(e.target.value)} placeholder="Type your message..." /> <button onClick={async () => { // Add user message to UI setConversation((currentConversation: ClientMessage[]) => [ ...currentConversation, { id: generateId(), role: 'user', display: input }, ]); // Get AI response const message = await continueConversation(input); // Add AI response to UI setConversation((currentConversation: ClientMessage[]) => [ ...currentConversation, message, ]); setInput(''); }} > Send </button> </div> </div> );
}
``` ---------------------------------------- TITLE: Instantiate Google Generative AI Model with Safety Settings
DESCRIPTION: Instantiate a Google Generative AI model with custom safety settings to control content generation based on specified categories and thresholds.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#_snippet_5 LANGUAGE: ts
CODE:
```
const model = google('gemini-1.5-pro-latest', { safetySettings: [ { category: 'HARM_CATEGORY_UNSPECIFIED', threshold: 'BLOCK_LOW_AND_ABOVE' }, ],
});
``` ---------------------------------------- TITLE: Implementing Multi-Step Text Streaming with AI SDK (Server)
DESCRIPTION: This server-side code demonstrates how to create a multi-step AI response using `createDataStreamResponse` and `streamText` from the AI SDK. It shows how to force a tool call in the first step and then continue the stream with a different model and system prompt in the second step, controlling the `start` and `finish` events to merge results into a single client-side message.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/24-stream-text-multistep.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { createDataStreamResponse, streamText, tool } from 'ai';
import { z } from 'zod'; export async function POST(req: Request) { const { messages } = await req.json(); return createDataStreamResponse({ execute: async dataStream => { // step 1 example: forced tool call const result1 = streamText({ model: openai('gpt-4o-mini', { structuredOutputs: true }), system: 'Extract the user goal from the conversation.', messages, toolChoice: 'required', // force the model to call a tool tools: { extractGoal: tool({ parameters: z.object({ goal: z.string() }), execute: async ({ goal }) => goal, // no-op extract tool }), }, }); // forward the initial result to the client without the finish event: result1.mergeIntoDataStream(dataStream, { experimental_sendFinish: false, // omit the finish event }); // note: you can use any programming construct here, e.g. if-else, loops, etc. // workflow programming is normal programming with this approach. // example: continue stream with forced tool call from previous step const result2 = streamText({ // different system prompt, different model, no tools: model: openai('gpt-4o'), system: 'You are a helpful assistant with a different system prompt. Repeat the extract user goal in your answer.', // continue the workflow stream with the messages from the previous step: messages: [ ...convertToCoreMessages(messages), ...(await result1.response).messages, ], }); // forward the 2nd result to the client (incl. the finish event): result2.mergeIntoDataStream(dataStream, { experimental_sendStart: false, // omit the start event }); }, });
}
``` ---------------------------------------- TITLE: Defining AI Tools and Server Action for Flight Booking in TSX
DESCRIPTION: This snippet defines two asynchronous functions, `searchFlights` and `lookupFlight`, which simulate flight data retrieval. It then exports a `submitUserMessage` server action that uses `streamUI` from `@ai-sdk/openai` to interact with a GPT-4o model. This action integrates the defined flight tools, allowing the AI to search for and look up flight details based on user prompts, streaming UI components as responses.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/04-multistep-interfaces.mdx#_snippet_0 LANGUAGE: TSX
CODE:
```
import { streamUI } from 'ai/rsc';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; const searchFlights = async ( source: string, destination: string, date: string,
) => { return [ { id: '1', flightNumber: 'AA123', }, { id: '2', flightNumber: 'AA456', }, ];
}; const lookupFlight = async (flightNumber: string) => { return { flightNumber: flightNumber, departureTime: '10:00 AM', arrivalTime: '12:00 PM', };
}; export async function submitUserMessage(input: string) { 'use server'; const ui = await streamUI({ model: openai('gpt-4o'), system: 'you are a flight booking assistant', prompt: input, text: async ({ content }) => <div>{content}</div>, tools: { searchFlights: { description: 'search for flights', parameters: z.object({ source: z.string().describe('The origin of the flight'), destination: z.string().describe('The destination of the flight'), date: z.string().describe('The date of the flight'), }), generate: async function* ({ source, destination, date }) { yield `Searching for flights from ${source} to ${destination} on ${date}...`; const results = await searchFlights(source, destination, date); return ( <div> {results.map(result => ( <div key={result.id}> <div>{result.flightNumber}</div> </div> ))} </div> ); }, }, lookupFlight: { description: 'lookup details for a flight', parameters: z.object({ flightNumber: z.string().describe('The flight number'), }), generate: async function* ({ flightNumber }) { yield `Looking up details for flight ${flightNumber}...`; const details = await lookupFlight(flightNumber); return ( <div> <div>Flight Number: {details.flightNumber}</div> <div>Departure Time: {details.departureTime}</div> <div>Arrival Time: {details.arrivalTime}</div> </div> ); }, }, }, }); return ui.value;
}
``` ---------------------------------------- TITLE: Generating Structured Objects with OpenAI Responses API (TypeScript)
DESCRIPTION: This snippet demonstrates how to use `generateObject` with the OpenAI Responses API to enforce a structured JSON output. It defines a Zod schema for a recipe, ensuring the model's response adheres to the specified object structure for name, ingredients, and steps.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_26 LANGUAGE: TypeScript
CODE:
```
// Using generateObject
const result = await generateObject({ model: openai.responses('gpt-4.1'), schema: z.object({ recipe: z.object({ name: z.string(), ingredients: z.array( z.object({ name: z.string(), amount: z.string(), }), ), steps: z.array(z.string()), }), }), prompt: 'Generate a lasagna recipe.',
});
``` ---------------------------------------- TITLE: Dynamic AI Model Routing for Multi-modal Chat (TypeScript)
DESCRIPTION: This TypeScript route handler (`app/api/chat/route.ts`) intelligently routes incoming chat messages to different AI models based on their attachments. It detects PDF attachments and directs those requests to Anthropic's Claude 3.5 Sonnet, while routing other requests (e.g., image-only) to OpenAI's GPT-4o, enabling versatile multi-modal processing.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/02-multi-modal-chatbot.mdx#_snippet_10 LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { anthropic } from '@ai-sdk/anthropic';
import { streamText, type Message } from 'ai'; export const maxDuration = 30; export async function POST(req: Request) { const { messages }: { messages: Message[] } = await req.json(); // check if user has sent a PDF const messagesHavePDF = messages.some(message => message.experimental_attachments?.some( a => a.contentType === 'application/pdf', ), ); const result = streamText({ model: messagesHavePDF ? anthropic('claude-3-5-sonnet-latest') : openai('gpt-4o'), messages, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Integrating Weather Tool with AI-SDK in TypeScript
DESCRIPTION: This snippet demonstrates how to integrate a custom 'weather' tool into an AI chatbot using `@ai-sdk/openai`. It defines the tool's description and parameters using Zod, and provides an asynchronous `execute` function that simulates fetching weather data. The `streamText` function is used to process user input, invoke the tool when appropriate, and stream the AI's text response.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/06-nodejs.mdx#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { CoreMessage, streamText, tool } from 'ai';
import dotenv from 'dotenv';
import { z } from 'zod';
import * as readline from 'node:readline/promises'; dotenv.config(); const terminal = readline.createInterface({ input: process.stdin, output: process.stdout,
}); const messages: CoreMessage[] = []; async function main() { while (true) { const userInput = await terminal.question('You: '); messages.push({ role: 'user', content: userInput }); const result = streamText({ model: openai('gpt-4o'), messages, tools: { weather: tool({ description: 'Get the weather in a location (in Celsius)', parameters: z.object({ location: z .string() .describe('The location to get the weather for'), }), execute: async ({ location }) => ({ location, temperature: Math.round((Math.random() * 30 + 5) * 10) / 10, // Random temp between 5°C and 35°C }), }), }, }); let fullResponse = ''; process.stdout.write('\nAssistant: '); for await (const delta of result.textStream) { fullResponse += delta; process.stdout.write(delta); } process.stdout.write('\n\n'); messages.push({ role: 'assistant', content: fullResponse }); }
} main().catch(console.error);
``` ---------------------------------------- TITLE: Defining AI SDK Tools for Weather and Temperature Conversion in TypeScript
DESCRIPTION: This TypeScript snippet updates the `server/api/chat.ts` file to define two new tools: `weather` and `convertFahrenheitToCelsius`. The `weather` tool simulates fetching temperature for a given location, while `convertFahrenheitToCelsius` converts a Fahrenheit temperature to Celsius. These tools are integrated into the `streamText` function, allowing the AI model to call them based on user queries. It requires `@ai-sdk/openai` and `zod` for schema validation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#_snippet_13 LANGUAGE: typescript
CODE:
```
import { streamText, tool } from 'ai';
import { createOpenAI } from '@ai-sdk/openai';
import { z } from 'zod'; export default defineLazyEventHandler(async () => { const apiKey = useRuntimeConfig().openaiApiKey; if (!apiKey) throw new Error('Missing OpenAI API key'); const openai = createOpenAI({ apiKey: apiKey, }); return defineEventHandler(async (event: any) => { const { messages } = await readBody(event); const result = streamText({ model: openai('gpt-4o-preview'), messages, tools: { weather: tool({ description: 'Get the weather in a location (fahrenheit)', parameters: z.object({ location: z .string() .describe('The location to get the weather for'), }), execute: async ({ location }) => { const temperature = Math.round(Math.random() * (90 - 32) + 32); return { location, temperature, }; }, }), convertFahrenheitToCelsius: tool({ description: 'Convert a temperature in fahrenheit to celsius', parameters: z.object({ temperature: z .number() .describe('The temperature in fahrenheit to convert'), }), execute: async ({ temperature }) => { const celsius = Math.round((temperature - 32) * (5 / 9)); return { celsius, }; }, }), }, }); return result.toDataStreamResponse(); });
});
``` ---------------------------------------- TITLE: Generating Basic Text with AI SDK's generateText (TypeScript)
DESCRIPTION: This snippet demonstrates the fundamental usage of the `generateText` function from the AI SDK. It's suitable for non-interactive scenarios where a complete text response is needed, such as drafting emails or summarizing content. The function takes a model and a prompt, returning the generated text.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { generateText } from 'ai'; const { text } = await generateText({ model: yourModel, prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
``` ---------------------------------------- TITLE: Generating an Object with a Schema (TypeScript)
DESCRIPTION: This example demonstrates how to use `generateObject` to create a structured object based on a Zod schema. It generates a lasagna recipe with a name, ingredients, and steps, then logs the resulting object as a formatted JSON string.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/03-generate-object.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateObject } from 'ai';
import { z } from 'zod'; const { object } = await generateObject({ model: openai('gpt-4-turbo'), schema: z.object({ recipe: z.object({ name: z.string(), ingredients: z.array(z.string()), steps: z.array(z.string()) }) }), prompt: 'Generate a lasagna recipe.'
}); console.log(JSON.stringify(object, null, 2));
``` ---------------------------------------- TITLE: Implement Chat UI with AI SDK `useChat` Hook in Next.js
DESCRIPTION: This code demonstrates how to build an interactive chat interface in a Next.js application using the `useChat` hook from `@ai-sdk/react`. It manages chat state, handles user input, submits messages to the API route, and displays AI responses in real-time.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/24-o3.mdx#_snippet_6 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Page() { const { messages, input, handleInputChange, handleSubmit, error } = useChat(); return ( <> {messages.map(message => ( <div key={message.id}> {message.role === 'user' ? 'User: ' : 'AI: '} {message.content} </div> ))} <form onSubmit={handleSubmit}> <input name="prompt" value={input} onChange={handleInputChange} /> <button type="submit">Submit</button> </form> </> );
}
``` ---------------------------------------- TITLE: Build Chat UI with AI SDK React Hook
DESCRIPTION: Illustrates creating a client-side chat interface in a Next.js App Router using the `useChat` hook. It displays messages, handles user input, and manages form submission for a conversational AI experience.
SOURCE: https://github.com/vercel/ai/blob/main/packages/ai/README.md#_snippet_4 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Page() { const { messages, input, handleSubmit, handleInputChange, status } = useChat(); return ( <div> {messages.map(message => ( <div key={message.id}> <strong>{`${message.role}: `}</strong> {message.parts.map((part, index) => { switch (part.type) { case 'text': return <span key={index}>{part.text}</span>; // other cases can handle images, tool calls, etc } })} </div> ))} <form onSubmit={handleSubmit}> <input value={input} placeholder="Send a message..." onChange={handleInputChange} disabled={status !== 'ready'} /> </form> </div> );
}
``` ---------------------------------------- TITLE: Generate Structured JSON Data with AI SDK and Zod
DESCRIPTION: This example shows how to generate structured JSON data using the AI SDK's `generateObject` function with the OpenAI 'o1' model. It demonstrates defining a Zod schema to constrain the output to a specific recipe structure, ensuring type-safe and predictable data generation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/23-o1.mdx#_snippet_4 LANGUAGE: tsx
CODE:
```
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; const { object } = await generateObject({ model: openai('o1'), schema: z.object({ recipe: z.object({ name: z.string(), ingredients: z.array(z.object({ name: z.string(), amount: z.string() })), steps: z.array(z.string()), }), }), prompt: 'Generate a lasagna recipe.',
});
``` ---------------------------------------- TITLE: Google Vertex AI Generative Model Capabilities API
DESCRIPTION: Lists the capabilities of various Google Vertex AI generative models, including support for image input, object generation, tool usage, and tool streaming.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#_snippet_20 LANGUAGE: APIDOC
CODE:
```
| Model | Image Input | Object Generation | Tool Usage | Tool Streaming |
| ---------------------- | ------------------- | ------------------- | ------------------- | ------------------- |
| `gemini-2.0-flash-001` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |
| `gemini-2.0-flash-exp` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |
| `gemini-1.5-flash` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |
| `gemini-1.5-pro` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |
``` ---------------------------------------- TITLE: Using Custom Tools with AI SDK `generateText` in TypeScript
DESCRIPTION: This example demonstrates how to define and integrate a custom tool, `getWeather`, with the AI SDK's `generateText` function. It shows how to specify tool parameters using `z.object` and implement the `execute` logic, allowing the LLM to perform external actions and retrieve dynamic data.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/23-o1.mdx#_snippet_6 LANGUAGE: tsx
CODE:
```
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai'; const { text } = await generateText({ model: openai('o1'), prompt: 'What is the weather like today?', tools: { getWeather: tool({ description: 'Get the weather in a location', parameters: z.object({ location: z.string().describe('The location to get the weather for'), }), execute: async ({ location }) => ({ location, temperature: 72 + Math.floor(Math.random() * 21) - 10, }), }), },
});
``` ---------------------------------------- TITLE: Authenticating Server Action for Weather Data (TSX)
DESCRIPTION: This `getWeather` Server Action demonstrates how to implement authentication for server-side operations using Next.js and the AI SDK. It retrieves a 'token' from cookies, validates it using `validateToken`, and if unauthorized, returns an error. Upon successful authentication, it uses `createStreamableUI` to stream UI components like a `Skeleton` and `Weather` component, ensuring data is only returned to authorized users.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/09-authentication.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
'use server'; import { cookies } from 'next/headers';
import { createStremableUI } from 'ai/rsc';
import { validateToken } from '../utils/auth'; export const getWeather = async () => { const token = cookies().get('token'); if (!token || !validateToken(token)) { return { error: 'This action requires authentication' }; } const streamableDisplay = createStreamableUI(null); streamableDisplay.update(<Skeleton />); streamableDisplay.done(<Weather />); return { display: streamableDisplay.value };
};
``` ---------------------------------------- TITLE: Setting up OpenTelemetry SDK with Langfuse Exporter in Node.js (TypeScript)
DESCRIPTION: This comprehensive Node.js TypeScript example demonstrates the full setup of the OpenTelemetry SDK with `LangfuseExporter`. It initializes the SDK with auto-instrumentations, starts tracing, performs a `generateText` call with detailed experimental telemetry (including `functionId` and `metadata`), logs the result, and ensures traces are flushed to Langfuse by gracefully shutting down the SDK.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langfuse.mdx#_snippet_5 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';
import { NodeSDK } from '@opentelemetry/sdk-node';
import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';
import { LangfuseExporter } from 'langfuse-vercel'; const sdk = new NodeSDK({ traceExporter: new LangfuseExporter(), instrumentations: [getNodeAutoInstrumentations()],
}); sdk.start(); async function main() { const result = await generateText({ model: openai('gpt-4o'), maxTokens: 50, prompt: 'Invent a new holiday and describe its traditions.', experimental_telemetry: { isEnabled: true, functionId: 'my-awesome-function', metadata: { something: 'custom', someOtherThing: 'other-value', }, }, }); console.log(result.text); await sdk.shutdown(); // Flushes the trace to Langfuse
} main().catch(console.error);
``` ---------------------------------------- TITLE: Create Next.js API Route Handler for AI Chat
DESCRIPTION: This TypeScript code defines a Next.js API route handler (`app/api/chat/route.ts`) that processes incoming chat messages. It uses `@ai-sdk/openai` to interact with OpenAI's `gpt-4.5-preview` model and streams the text response back to the client. The `maxDuration` is set to 30 seconds to allow for longer responses.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/22-gpt-4-5.mdx#_snippet_4 LANGUAGE: tsx
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai'; // Allow responses up to 30 seconds
export const maxDuration = 30; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: openai('gpt-4.5-preview'), messages, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Add getInformation Tool to Chat Route Handler
DESCRIPTION: This snippet modifies the `api/chat/route.ts` file to integrate a new `getInformation` tool into the AI model's capabilities. This tool allows the model to query the knowledge base for relevant information by calling the `findRelevantContent` function with the user's question. It also defines the system prompt to guide the model's behavior regarding knowledge base usage.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#_snippet_24 LANGUAGE: ts
CODE:
```
import { createResource } from '@/lib/actions/resources';
import { openai } from '@ai-sdk/openai';
import { streamText, tool } from 'ai';
import { z } from 'zod';
import { findRelevantContent } from '@/lib/ai/embedding'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: openai('gpt-4o'), messages, system: `You are a helpful assistant. Check your knowledge base before answering any questions. Only respond to questions using information from tool calls. if no relevant information is found in the tool calls, respond, "Sorry, I don't know."`, tools: { addResource: tool({ description: `add a resource to your knowledge base. If the user provides a random piece of knowledge unprompted, use this tool without asking for confirmation.`, parameters: z.object({ content: z .string() .describe('the content or resource to add to the knowledge base'), }), execute: async ({ content }) => createResource({ content }), }), getInformation: tool({ description: `get information from your knowledge base to answer questions.`, parameters: z.object({ question: z.string().describe('the users question'), }), execute: async ({ question }) => findRelevantContent(question), }), }, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Call OpenAI GPT-4.5 for Text Generation with AI SDK
DESCRIPTION: This snippet demonstrates how to use the AI SDK Core's `generateText` function to call the OpenAI GPT-4.5 model. It shows the basic setup for importing necessary modules and making a simple text generation request, explaining a concept.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/22-gpt-4-5.mdx#_snippet_0 LANGUAGE: ts
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai'; const { text } = await generateText({ model: openai('gpt-4.5-preview'), prompt: 'Explain the concept of quantum entanglement.',
});
``` ---------------------------------------- TITLE: Generate Structured JSON Data with AI SDK and Zod
DESCRIPTION: This example illustrates how to generate structured JSON data using `generateObject` from the AI SDK. It leverages Zod to define a strict schema for the output, ensuring type-safe and predictable data generation, such as a recipe object with nested arrays and strings.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/19-openai-responses.mdx#_snippet_1 LANGUAGE: ts
CODE:
```
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; const { object } = await generateObject({ model: openai.responses('gpt-4o'), schema: z.object({ recipe: z.object({ name: z.string(), ingredients: z.array(z.object({ name: z.string(), amount: z.string() })), steps: z.array(z.string()) }) }), prompt: 'Generate a lasagna recipe.',
});
``` ---------------------------------------- TITLE: Generate Text with Google Search Grounding in TypeScript
DESCRIPTION: Demonstrates how to use Google's search grounding feature with `gemini-1.5-pro` to provide models with access to the latest information. It shows how to enable `useSearchGrounding` and access `groundingMetadata` and `safetyRatings` from the `providerMetadata`.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#_snippet_14 LANGUAGE: ts
CODE:
```
import { google } from '@ai-sdk/google';
import { GoogleGenerativeAIProviderMetadata } from '@ai-sdk/google';
import { generateText } from 'ai'; const { text, providerMetadata } = await generateText({ model: google('gemini-1.5-pro', { useSearchGrounding: true, }), prompt: 'List the top 5 San Francisco news from the past week.' + 'You must include the date of each article.',
}); // access the grounding metadata. Casting to the provider metadata type
// is optional but provides autocomplete and type safety.
const metadata = providerMetadata?.google as | GoogleGenerativeAIProviderMetadata | undefined;
const groundingMetadata = metadata?.groundingMetadata;
const safetyRatings = metadata?.safetyRatings;
``` ---------------------------------------- TITLE: Implement Tool Calling with OpenAI GPT-4.5 and AI SDK
DESCRIPTION: This snippet demonstrates how to integrate tool calling capabilities with OpenAI GPT-4.5 using the AI SDK. It defines a `getWeather` tool with parameters and an execution function, allowing the model to interact with external systems (simulated here) to retrieve dynamic information based on the prompt.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/22-gpt-4-5.mdx#_snippet_2 LANGUAGE: ts
CODE:
```
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai'; const { text } = await generateText({ model: openai('gpt-4.5-preview'), prompt: 'What is the weather like today in San Francisco?', tools: { getWeather: tool({ description: 'Get the weather in a location', parameters: z.object({ location: z.string().describe('The location to get the weather for') }), execute: async ({ location }) => ({ location, temperature: 72 + Math.floor(Math.random() * 21) - 10 }) }) }
});
``` ---------------------------------------- TITLE: Implementing Chat UI with useChat Hook in Next.js (TypeScript)
DESCRIPTION: This snippet demonstrates how to integrate the `@ai-sdk/react`'s `useChat` hook into a Next.js page. It manages chat messages, user input, and form submission, rendering a dynamic chat interface where user and AI messages are displayed and an input field allows for new messages.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/03-nextjs-pages-router.mdx#_snippet_8 LANGUAGE: tsx
CODE:
```
import { useChat } from '@ai-sdk/react'; export default function Chat() { const { messages, input, handleInputChange, handleSubmit } = useChat(); return ( <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch"> {messages.map(message => ( <div key={message.id} className="whitespace-pre-wrap"> {message.role === 'user' ? 'User: ' : 'AI: '} {message.parts.map((part, i) => { switch (part.type) { case 'text': return <div key={`${message.id}-${i}`}>{part.text}</div>; } })} </div> ))} <form onSubmit={handleSubmit}> <input className="fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl" value={input} placeholder="Say something..." onChange={handleInputChange} /> </form> </div> );
}
``` ---------------------------------------- TITLE: Defining and Registering Tools with AI SDK
DESCRIPTION: This snippet demonstrates how to define and register custom tools like 'weather' and 'cityAttractions' using the AI SDK's `tool` function. It uses Zod for parameter validation and integrates with the `generateText` function from `@ai-sdk/openai` to enable the model to call these tools based on the prompt. The 'weather' tool includes an `execute` function to simulate fetching weather data.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/50-call-tools.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; const result = await generateText({ model: openai('gpt-4-turbo'), tools: { weather: tool({ description: 'Get the weather in a location', parameters: z.object({ location: z.string().describe('The location to get the weather for') }), execute: async ({ location }) => ({ location, temperature: 72 + Math.floor(Math.random() * 21) - 10 }) }), cityAttractions: tool({ parameters: z.object({ city: z.string() }) }) }, prompt: 'What is the weather in San Francisco and what attractions should I visit?'
});
``` ---------------------------------------- TITLE: Integrate External Tools with AI SDK Models
DESCRIPTION: The AI SDK allows LLMs to call external tools for discrete tasks, such as mathematical calculations or fetching real-time data. Tools are passed via the `tools` parameter in functions like `generateText` or `streamUI`, extending the model's capabilities beyond pure generation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { generateText, tool } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra';
import { getWeather } from './weatherTool'; const { text } = await generateText({ model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'), prompt: 'What is the weather like today?', tools: { weather: tool({ description: 'Get the weather in a location', parameters: z.object({ location: z.string().describe('The location to get the weather for') }), execute: async ({ location }) => ({ location, temperature: 72 + Math.floor(Math.random() * 21) - 10 }) }) }
});
``` ---------------------------------------- TITLE: Frontend Chat UI with AI SDK useChat Hook (Next.js)
DESCRIPTION: This snippet demonstrates a basic Next.js frontend component for a chatbot application. It utilizes the `@ai-sdk/react` `useChat` hook to manage chat messages, user input, and form submission, providing a reactive UI for displaying conversation history and capturing new messages.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#_snippet_0 LANGUAGE: TSX
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Chat() { const { messages, input, handleInputChange, handleSubmit } = useChat(); return ( <div> <div> {messages?.map(m => ( <div key={m.id}> <strong>{`${m.role}: `}</strong> {m.parts?.map((part, i) => { switch (part.type) { case 'text': return <div key={i}>{part.text}</div>; } })} <br /> </div> ))} </div> <form onSubmit={handleSubmit}> <input value={input} placeholder="Say something..." onChange={handleInputChange} /> </form> </div> );
}
``` ---------------------------------------- TITLE: Implement Chat UI with useChat Hook in Next.js
DESCRIPTION: This React component, designed for a Next.js application, utilizes the `useChat` hook from `@ai-sdk/react` to manage chat state, display messages (including model reasoning), and handle user input. It connects the frontend UI to the AI chat backend.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/25-r1.mdx#_snippet_5 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Page() { const { messages, input, handleInputChange, handleSubmit, error } = useChat(); return ( <> {messages.map(message => ( <div key={message.id}> {message.role === 'user' ? 'User: ' : 'AI: '} {message.reasoning && <pre>{message.reasoning}</pre>} {message.content} </div> ))} <form onSubmit={handleSubmit}> <input name="prompt" value={input} onChange={handleInputChange} /> <button type="submit">Submit</button> </form> </> );
}
``` ---------------------------------------- TITLE: Generating Structured Data with AI SDK and Zod (TypeScript)
DESCRIPTION: This snippet demonstrates how to use the `generateObject` function from the AI SDK to generate structured JSON data. It utilizes Zod to define a precise schema for a recipe object, ensuring the AI model's output conforms to the expected structure. The `openai` model is used to process the prompt and generate the structured response, which is then logged to the console.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/30-generate-object.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; const result = await generateObject({ model: openai('gpt-4-turbo'), schema: z.object({ recipe: z.object({ name: z.string(), ingredients: z.array( z.object({ name: z.string(), amount: z.string() }) ), steps: z.array(z.string()) }) }), prompt: 'Generate a lasagna recipe.'
}); console.log(JSON.stringify(result.object.recipe, null, 2));
``` ---------------------------------------- TITLE: Implement Client-Side Chat UI with AI SDK useChat Hook in Next.js
DESCRIPTION: This client-side React component utilizes the `useChat` hook from `@ai-sdk/react` to manage chat state, input, and submission. It renders messages from both the user and AI, and provides an input field and submit button to facilitate real-time chat interactions by sending requests to the AI provider endpoint.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#_snippet_6 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Page() { const { messages, input, handleInputChange, handleSubmit } = useChat(); return ( <> {messages.map(message => ( <div key={message.id}> {message.role === 'user' ? 'User: ' : 'AI: '} {message.content} </div> ))} <form onSubmit={handleSubmit}> <input name="prompt" value={input} onChange={handleInputChange} /> <button type="submit">Submit</button> </form> </> );
}
``` ---------------------------------------- TITLE: Defining `searchFlights` Tool with Zod and React Component Return (TSX)
DESCRIPTION: This snippet defines the `searchFlights` tool within an `actions.tsx` file, specifying its description and parameters using Zod for validation. The `generate` function simulates a flight search and returns a `Flights` React component to be rendered in the UI, demonstrating how server actions can yield and return React elements.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/04-multistep-interfaces.mdx#_snippet_5 LANGUAGE: TSX
CODE:
```
searchFlights: { description: 'search for flights', parameters: z.object({ source: z.string().describe('The origin of the flight'), destination: z.string().describe('The destination of the flight'), date: z.string().describe('The date of the flight'), }), generate: async function* ({ source, destination, date }) { yield `Searching for flights from ${source} to ${destination} on ${date}...`; const results = await searchFlights(source, destination, date); return (<Flights flights={results} />); },
}
``` ---------------------------------------- TITLE: Streaming Structured Data Object with AI SDK in TypeScript
DESCRIPTION: This snippet demonstrates how to use the `streamObject` function from the AI SDK to stream a large, structured data object (a recipe in this case) in real-time. It utilizes `@ai-sdk/openai` for the model and `zod` for defining the output schema. The `partialObjectStream` is then iterated over to log the incrementally generated parts of the object, enabling real-time UI updates as the data becomes available.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/40-stream-object.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';
import { z } from 'zod'; const { partialObjectStream } = streamObject({ model: openai('gpt-4-turbo'), schema: z.object({ recipe: z.object({ name: z.string(), ingredients: z.array(z.string()), steps: z.array(z.string()), }), }), prompt: 'Generate a lasagna recipe.',
}); for await (const partialObject of partialObjectStream) { console.clear(); console.log(partialObject);
}
``` ---------------------------------------- TITLE: Generating Text with User, Assistant Tool Call, and Tool Result Messages
DESCRIPTION: This comprehensive example demonstrates a full interaction flow involving user input, an assistant's `tool-call` message, and a subsequent `tool` message containing the `tool-result`. It shows how to pass tool execution results back to the model for continued conversation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#_snippet_17 LANGUAGE: TypeScript
CODE:
```
const result = await generateText({ model: yourModel, messages: [ { role: 'user', content: [ { type: 'text', text: 'How many calories are in this block of cheese?', }, { type: 'image', image: fs.readFileSync('./data/roquefort.jpg') }, ], }, { role: 'assistant', content: [ { type: 'tool-call', toolCallId: '12345', toolName: 'get-nutrition-data', args: { cheese: 'Roquefort' }, }, // there could be more tool calls here (parallel calling) ], }, { role: 'tool', content: [ { type: 'tool-result', toolCallId: '12345', // needs to match the tool call id toolName: 'get-nutrition-data', result: { name: 'Cheese, roquefort', calories: 369, fat: 31, protein: 22, }, }, // there could be more tool results here (parallel calling) ], }, ],
});
``` ---------------------------------------- TITLE: Generate Structured JSON Data with AI SDK and Zod
DESCRIPTION: This example illustrates how to use the AI SDK's `generateObject` function to produce structured JSON output from OpenAI GPT-4.5. It leverages Zod for schema definition, ensuring the generated data adheres to a predefined type-safe structure, such as a recipe.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/22-gpt-4-5.mdx#_snippet_1 LANGUAGE: ts
CODE:
```
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; const { object } = await generateObject({ model: openai('gpt-4.5-preview'), schema: z.object({ recipe: z.object({ name: z.string(), ingredients: z.array(z.object({ name: z.string(), amount: z.string() })), steps: z.array(z.string()) }) }), prompt: 'Generate a lasagna recipe.'
});
``` ---------------------------------------- TITLE: Creating Chat Route Handler with Anthropic and AI SDK in Next.js
DESCRIPTION: This TypeScript route handler defines a POST endpoint for handling chat requests. It uses `@ai-sdk/anthropic` to stream text from the Claude-4-sonnet model, forwarding messages from the client and optionally sending reasoning tokens back. It's essential for the backend processing of AI chat interactions.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/18-claude-4.mdx#_snippet_3 LANGUAGE: tsx
CODE:
```
import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';
import { streamText } from 'ai'; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: anthropic('claude-4-sonnet-20250514'), messages, headers: { 'anthropic-beta': 'interleaved-thinking-2025-05-14', }, providerOptions: { anthropic: { thinking: { type: 'enabled', budgetTokens: 15000 }, } satisfies AnthropicProviderOptions, }, }); return result.toDataStreamResponse({ sendReasoning: true, });
}
``` ---------------------------------------- TITLE: Stream Text Response from Llama 3.1 using AI SDK
DESCRIPTION: This snippet illustrates how to stream the model's response as it's being generated using the AI SDK. It utilizes the `streamText` function with Llama 3.1 via the DeepInfra provider, returning a text stream instead of a complete, single response.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
import { streamText } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra'; const { textStream } = streamText({ model: deepinfra('meta-llama/Meta-Llama-3.1-405B-Instruct'), prompt: 'What is love?',
});
``` ---------------------------------------- TITLE: Creating Nested Streamable UIs for Dynamic Components
DESCRIPTION: This function illustrates how to create nested streamable UI components. It initializes a main `ui` streamable, then asynchronously fetches stock data. A `historyChart` streamable is created and passed as a prop to `StockCard`, allowing the chart to update independently within the parent component as its data becomes available. This enables complex, dynamic UI compositions.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/05-multiple-streamables.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
async function getStockHistoryChart({ symbol: string }) { 'use server'; const ui = createStreamableUI(<Spinner />); // We need to wrap this in an async IIFE to avoid blocking. (async () => { const price = await getStockPrice({ symbol }); // Show a spinner as the history chart for now. const historyChart = createStreamableUI(<Spinner />); ui.done(<StockCard historyChart={historyChart.value} price={price} />); // Getting the history data and then update that part of the UI. const historyData = await fetch('https://my-stock-data-api.com'); historyChart.done(<HistoryChart data={historyData} />); })(); return ui;
}
``` ---------------------------------------- TITLE: Client-Side Conditional UI Rendering in React
DESCRIPTION: This snippet demonstrates client-side conditional rendering of React components based on the 'message.name' property, which typically corresponds to a tool call. It checks the message role and then dynamically renders different UI components (e.g., Courses, People, Meetings) by passing the 'message.content' as props. This approach centralizes UI logic on the client, requiring the client to manage and render all possible UI variations.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/07-rendering-ui-with-language-models.mdx#_snippet_3 LANGUAGE: tsx
CODE:
```
{ message.role === 'tool' ? ( message.name === 'api-search-course' ? ( <Courses courses={message.content} /> ) : message.name === 'api-search-profile' ? ( <People people={message.content} /> ) : message.name === 'api-meetings' ? ( <Meetings meetings={message.content} /> ) : message.name === 'api-search-building' ? ( <Buildings buildings={message.content} /> ) : message.name === 'api-events' ? ( <Events events={message.content} /> ) : message.name === 'api-meals' ? ( <Meals meals={message.content} /> ) : null ) : ( <div>{message.content}</div> );
}
``` ---------------------------------------- TITLE: Creating Environment File (Bash)
DESCRIPTION: This command creates an empty `.env` file in the project's root directory, which will be used to store sensitive information like API keys securely.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/06-nodejs.mdx#_snippet_2 LANGUAGE: bash
CODE:
```
touch .env
``` ---------------------------------------- TITLE: Initializing OpenAI Chat Model with Default Settings (TypeScript)
DESCRIPTION: This snippet demonstrates how to initialize an OpenAI chat model using the `openai.chat()` factory method. It takes the model ID, such as 'gpt-3.5-turbo', as its primary argument, creating a model instance with default configurations. This is the basic way to get started with OpenAI chat models.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_8 LANGUAGE: TypeScript
CODE:
```
const model = openai.chat('gpt-3.5-turbo');
``` ---------------------------------------- TITLE: Implementing Sequential AI Generations with AI SDK
DESCRIPTION: This snippet demonstrates how to create a sequential chain of AI generations using the AI SDK. It uses `generateText` with the `gpt-4o` model to first generate blog post ideas, then selects the best idea from the generated list, and finally creates a detailed outline based on the chosen idea. Each subsequent generation uses the output of the previous one as its input, illustrating a "chain" pattern.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/09-sequential-generations.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai'; async function sequentialActions() { // Generate blog post ideas const ideasGeneration = await generateText({ model: openai('gpt-4o'), prompt: 'Generate 10 ideas for a blog post about making spaghetti.', }); console.log('Generated Ideas:\n', ideasGeneration); // Pick the best idea const bestIdeaGeneration = await generateText({ model: openai('gpt-4o'), prompt: `Here are some blog post ideas about making spaghetti:\n${ideasGeneration}\n\nPick the best idea from the list above and explain why it's the best.`, }); console.log('\nBest Idea:\n', bestIdeaGeneration); // Generate an outline const outlineGeneration = await generateText({ model: openai('gpt-4o'), prompt: `We've chosen the following blog post idea about making spaghetti:\n${bestIdeaGeneration}\n\nCreate a detailed outline for a blog post based on this idea.`, }); console.log('\nBlog Post Outline:\n', outlineGeneration);
} sequentialActions().catch(console.error);
``` ---------------------------------------- TITLE: Defining Multi-Step Tools with streamText API (TypeScript/Next.js)
DESCRIPTION: This Next.js API route (`/api/chat`) uses the `ai` module's `streamText` function to process chat messages and execute AI tools. It defines `getLocation` and `getWeather` tools with Zod schemas for parameter validation, demonstrating how to chain tool invocations (e.g., getting location then weather) within the `maxSteps` context set on the client.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/72-call-tools-multiple-steps.mdx#_snippet_1 LANGUAGE: typescript
CODE:
```
import { ToolInvocation, streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; interface Message { role: 'user' | 'assistant'; content: string; toolInvocations?: ToolInvocation[];
} function getLocation({ lat, lon }) { return { lat: 37.7749, lon: -122.4194 };
} function getWeather({ lat, lon, unit }) { return { value: 25, description: 'Sunny' };
} export async function POST(req: Request) { const { messages }: { messages: Message[] } = await req.json(); const result = streamText({ model: openai('gpt-4o'), system: 'You are a helpful assistant.', messages, tools: { getLocation: { description: 'Get the location of the user', parameters: z.object({}), execute: async () => { const { lat, lon } = getLocation(); return `Your location is at latitude ${lat} and longitude ${lon}`; }, }, getWeather: { description: 'Get the weather for a location', parameters: z.object({ lat: z.number().describe('The latitude of the location'), lon: z.number().describe('The longitude of the location'), unit: z .enum(['C', 'F']) .describe('The unit to display the temperature in'), }), execute: async ({ lat, lon, unit }) => { const { value, description } = getWeather({ lat, lon, unit }); return `It is currently ${value}°${unit} and ${description}!`; }, }, }, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Backend API Route for AI Chat with Tool Calling (Next.js)
DESCRIPTION: This snippet defines a Next.js API route (`POST` handler) for processing chat messages. It uses the AI SDK to stream text from an OpenAI model (`gpt-4o`) and includes a `getWeatherInformation` tool. The tool is defined with a description and parameters using `zod`, and its `execute` function simulates fetching weather data, merging the AI generation into a `DataStreamResponse`.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { createDataStreamResponse, streamText, tool } from 'ai';
import { z } from 'zod'; export async function POST(req: Request) { const { messages } = await req.json(); return createDataStreamResponse({ execute: async dataStream => { const result = streamText({ model: openai('gpt-4o'), messages, tools: { getWeatherInformation: tool({ description: 'show the weather in a given city to the user', parameters: z.object({ city: z.string() }), execute: async ({}: { city: string }) => { const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy']; return weatherOptions[ Math.floor(Math.random() * weatherOptions.length) ]; }, }), }, }); result.mergeIntoDataStream(dataStream); }, });
}
``` ---------------------------------------- TITLE: Generating Marketing Copy with Quality Check using AI Chains (TypeScript)
DESCRIPTION: This snippet demonstrates a sequential workflow for generating and validating marketing copy. It first uses `generateText` to create copy, then `generateObject` to perform a quality check (call to action, emotional appeal, clarity). If the quality metrics are below a threshold, it regenerates the copy with specific instructions. It relies on `@ai-sdk/openai` and `zod` for schema validation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText, generateObject } from 'ai';
import { z } from 'zod'; async function generateMarketingCopy(input: string) { const model = openai('gpt-4o'); // First step: Generate marketing copy const { text: copy } = await generateText({ model, prompt: `Write persuasive marketing copy for: ${input}. Focus on benefits and emotional appeal.`, }); // Perform quality check on copy const { object: qualityMetrics } = await generateObject({ model, schema: z.object({ hasCallToAction: z.boolean(), emotionalAppeal: z.number().min(1).max(10), clarity: z.number().min(1).max(10), }), prompt: `Evaluate this marketing copy for: 1. Presence of call to action (true/false) 2. Emotional appeal (1-10) 3. Clarity (1-10) Copy to evaluate: ${copy}`, }); // If quality check fails, regenerate with more specific instructions if ( !qualityMetrics.hasCallToAction || qualityMetrics.emotionalAppeal < 7 || qualityMetrics.clarity < 7 ) { const { text: improvedCopy } = await generateText({ model, prompt: `Rewrite this marketing copy with: ${!qualityMetrics.hasCallToAction ? '- A clear call to action' : ''} ${qualityMetrics.emotionalAppeal < 7 ? '- Stronger emotional appeal' : ''} ${qualityMetrics.clarity < 7 ? '- Improved clarity and directness' : ''} Original copy: ${copy}`, }); return { copy: improvedCopy, qualityMetrics }; } return { copy, qualityMetrics };
}
``` ---------------------------------------- TITLE: Adding Weather Tool to AI Chat API Route (TypeScript)
DESCRIPTION: This snippet modifies the SvelteKit API route to integrate a 'weather' tool using `@ai-sdk/openai`. It defines the tool's description, parameters (location using Zod), and an `execute` function that simulates fetching weather data. This allows the AI model to call the tool when a user asks for weather information.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#_snippet_9 LANGUAGE: TypeScript
CODE:
```
import { createOpenAI } from '@ai-sdk/openai';
import { streamText, tool } from 'ai';
import { z } from 'zod'; import { OPENAI_API_KEY } from '$env/static/private'; const openai = createOpenAI({ apiKey: OPENAI_API_KEY,
}); export async function POST({ request }) { const { messages } = await request.json(); const result = streamText({ model: openai('gpt-4o'), messages, tools: { weather: tool({ description: 'Get the weather in a location (fahrenheit)', parameters: z.object({ location: z.string().describe('The location to get the weather for'), }), execute: async ({ location }) => { const temperature = Math.round(Math.random() * (90 - 32) + 32); return { location, temperature, }; }, }), }, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Server-Side Text Streaming API Route (Next.js/AI SDK)
DESCRIPTION: This Next.js API route (`/api/completion`) handles incoming `POST` requests to generate and stream text. It extracts a `prompt` from the request body, uses the `streamText` function from the `ai` module with an OpenAI GPT-4 model, and sets a system prompt. The generated text is then streamed back to the client using `toDataStreamResponse()`.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/20-stream-text.mdx#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai'; export async function POST(req: Request) { const { prompt }: { prompt: string } = await req.json(); const result = streamText({ model: openai('gpt-4'), system: 'You are a helpful assistant.', prompt, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Setting Up Chat API Route with AI SDK and OpenAI
DESCRIPTION: This snippet defines a server-side API route (`POST`) that handles chat requests. It uses `@ai-sdk/openai` and `ai`'s `streamText` function to process messages with the `gpt-4o` model and stream responses back to the client, serving as the backend for the chat interface.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai'; export async function POST(request: Request) { const { messages } = await request.json(); const result = streamText({ model: openai('gpt-4o'), system: 'You are a friendly assistant!', messages, maxSteps: 5, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Stream Custom Data Parts from Server with createUIMessageStream (TSX)
DESCRIPTION: Illustrates how to stream arbitrary, type-safe data from the server to the client using `createUIMessageStream`. It shows initial data transmission and subsequent updates to the same part using a unique ID.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/01-announcing-ai-sdk-5-alpha/index.mdx#_snippet_7 LANGUAGE: tsx
CODE:
```
// On the server
const stream = createUIMessageStream({ execute: writer => { // Initial update writer.write({ type: 'data-weather', // Custom type id: toolCallId, // ID for updates data: { city, status: 'loading' } // Your data }); // Later, update the same part writer.write({ type: 'data-weather', id: toolCallId, data: { city, weather, status: 'success' } }); }
});
``` ---------------------------------------- TITLE: Transcribe Audio with Deepgram and AI SDK
DESCRIPTION: An example demonstrating how to use the Deepgram provider with the AI SDK's `experimental_transcribe` function to transcribe an audio file from a URL. It shows how to specify the Deepgram model and provide the audio source.
SOURCE: https://github.com/vercel/ai/blob/main/packages/deepgram/README.md#_snippet_2 LANGUAGE: ts
CODE:
```
import { deepgram } from '@ai-sdk/deepgram';
import { experimental_transcribe as transcribe } from 'ai'; const { text } = await transcribe({ model: deepgram.transcription('nova-3'), audio: new URL( 'https://github.com/vercel/ai/raw/refs/heads/main/examples/ai-core/data/galileo.mp3', ),
});
``` ---------------------------------------- TITLE: Handling Stream Completion with `onFinish` Callback in AI SDK Core `streamText` (TypeScript)
DESCRIPTION: This snippet shows how to use the `onFinish` callback with `streamText`, which is invoked once the entire stream has completed. It provides comprehensive information about the finished stream, including the generated `text`, `finishReason`, `usage` statistics, and the full `response` object containing all messages, useful for saving chat history or recording usage data.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { streamText } from 'ai'; const result = streamText({ model: yourModel, prompt: 'Invent a new holiday and describe its traditions.', onFinish({ text, finishReason, usage, response }) { // your own logic, e.g. for saving the chat history or recording usage const messages = response.messages; // messages that were generated },
});
``` ---------------------------------------- TITLE: Implement Basic Tool Calling with AI SDK Core
DESCRIPTION: This TypeScript snippet demonstrates how to define and integrate a custom 'weather' tool into the `generateText` function. It uses `zod` for schema validation of tool parameters and an `execute` function to simulate fetching weather data, showcasing a single-step tool invocation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#_snippet_0 LANGUAGE: ts
CODE:
```
import { z } from 'zod';
import { generateText, tool } from 'ai'; const result = await generateText({ model: yourModel, tools: { weather: tool({ description: 'Get the weather in a location', parameters: z.object({ location: z.string().describe('The location to get the weather for'), }), execute: async ({ location }) => ({ location, temperature: 72 + Math.floor(Math.random() * 21) - 10, }), }), }, prompt: 'What is the weather in San Francisco?',
});
``` ---------------------------------------- TITLE: Calling Tools with Image Prompt using AI SDK and OpenAI
DESCRIPTION: This snippet demonstrates how to use the AI SDK's `generateText` function to interact with a multimodal model (GPT-4 Turbo) by providing both text and image content. It defines a `logFood` tool with Zod-validated parameters (`name`, `calories`) that the model can call based on the image input, simulating a meal logging application. Dependencies include `ai`, `@ai-sdk/openai`, and `zod`.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/52-call-tools-with-image-prompt.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; const result = await generateText({ model: openai('gpt-4-turbo'), messages: [ { role: 'user', content: [ { type: 'text', text: 'can you log this meal for me?' }, { type: 'image', image: new URL( 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Cheeseburger_%2817237580619%29.jpg/640px-Cheeseburger_%2817237580619%29.jpg', ), }, ], }, ], tools: { logFood: tool({ description: 'Log a food item', parameters: z.object({ name: z.string(), calories: z.number(), }), execute({ name, calories }) { storeInDatabase({ name, calories }); }, }), },
});
``` ---------------------------------------- TITLE: Defining and Using AI Tools with generateText in TypeScript
DESCRIPTION: This TypeScript snippet demonstrates how to define and use a tool (`getWeather`) with an AI model (GPT-3.5-turbo) via the `generateText` function. It shows how the model can deterministically call a predefined function based on user prompts, such as retrieving weather information for a specified location, while also illustrating cases where no function is called if the prompt is out of scope. The `z.object` and `z.string` indicate a dependency on a Zod-like schema validation library for tool parameters.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/08-model-as-router.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
const sendMessage = (prompt: string) => generateText({ model: 'gpt-3.5-turbo', system: 'you are a friendly weather assistant!', prompt, tools: { getWeather: { description: 'Get the weather in a location', parameters: z.object({ location: z.string().describe('The location to get the weather for') }), execute: async ({ location }: { location: string }) => ({ location, temperature: 72 + Math.floor(Math.random() * 21) - 10 }) } } }); sendMessage('What is the weather in San Francisco?'); // getWeather is called
sendMessage('What is the weather in New York?'); // getWeather is called
sendMessage('What events are happening in London?'); // No function is called
``` ---------------------------------------- TITLE: Implementing Chat UI with useChat Hook in Next.js
DESCRIPTION: This snippet demonstrates how to build a client-side chat interface using the `useChat` hook from `@ai-sdk/react`. It manages chat messages, user input, and form submission, rendering messages and providing an input field for new messages. Dependencies include `@ai-sdk/react`.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#_snippet_3 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Page() { const { messages, input, setInput, handleSubmit } = useChat(); return ( <div> {messages.map(message => ( <div key={message.id}> <div>{message.role}</div> <div>{message.content}</div> </div> ))} <form onSubmit={handleSubmit}> <input type="text" value={input} onChange={event => { setInput(event.target.value); }} /> <button type="submit">Send</button> </form> </div> );
}
``` ---------------------------------------- TITLE: Processing AI Tool Calls with Human Confirmation in TypeScript
DESCRIPTION: This asynchronous function processes tool invocations within a message stream, specifically handling cases where human input is required. It executes tools only when explicitly authorized (via `APPROVAL.YES`) and streams the results back to the client. It takes `messages`, a `dataStream`, and a map of `executeFunctions` for tools requiring confirmation, returning an updated array of messages.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#_snippet_7 LANGUAGE: TypeScript
CODE:
```
export async function processToolCalls< Tools extends ToolSet, ExecutableTools extends { [Tool in keyof Tools as Tools[Tool] extends { execute: Function } ? never : Tool]: Tools[Tool]; },
>( { dataStream, messages, }: { tools: Tools; // used for type inference dataStream: DataStreamWriter; messages: Message[]; }, executeFunctions: { [K in keyof Tools & keyof ExecutableTools]?: ( args: z.infer<ExecutableTools[K]['parameters']>, context: ToolExecutionOptions, ) => Promise<any>; },
): Promise<Message[]> { const lastMessage = messages[messages.length - 1]; const parts = lastMessage.parts; if (!parts) return messages; const processedParts = await Promise.all( parts.map(async part => { // Only process tool invocations parts if (part.type !== 'tool-invocation') return part; const { toolInvocation } = part; const toolName = toolInvocation.toolName; // Only continue if we have an execute function for the tool (meaning it requires confirmation) and it's in a 'result' state if (!(toolName in executeFunctions) || toolInvocation.state !== 'result') return part; let result; if (toolInvocation.result === APPROVAL.YES) { // Get the tool and check if the tool has an execute function. if ( !isValidToolName(toolName, executeFunctions) || toolInvocation.state !== 'result' ) { return part; } const toolInstance = executeFunctions[toolName]; if (toolInstance) { result = await toolInstance(toolInvocation.args, { messages: convertToCoreMessages(messages), toolCallId: toolInvocation.toolCallId, }); } else { result = 'Error: No execute function found on tool'; } } else if (toolInvocation.result === APPROVAL.NO) { result = 'Error: User denied access to tool execution'; } else { // For any unhandled responses, return the original part. return part; } // Forward updated tool result to the client. dataStream.write( formatDataStreamPart('tool_result', { toolCallId: toolInvocation.toolCallId, result, }), ); // Return updated toolInvocation with the actual result. return { ...part, toolInvocation: { ...toolInvocation, result, }, }; }), ); // Finally return the processed messages return [...messages.slice(0, -1), { ...lastMessage, parts: processedParts }];
}
``` ---------------------------------------- TITLE: Implement `generateQuery` Server Action using AI SDK and Zod
DESCRIPTION: This code implements the `generateQuery` Server Action, utilizing `generateObject` from the AI SDK to constrain the model's output to a predefined Zod schema. It ensures the AI model returns only the SQL query string, handling potential errors during generation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#_snippet_8 LANGUAGE: TypeScript
CODE:
```
/* ...other imports... */
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; /* ...rest of the file... */ export const generateQuery = async (input: string) => { 'use server'; try { const result = await generateObject({ model: openai('gpt-4o'), system: `You are a SQL (postgres) ...`, // SYSTEM PROMPT AS ABOVE - OMITTED FOR BREVITY prompt: `Generate the query necessary to retrieve the data the user wants: ${input}`, schema: z.object({ query: z.string(), }), }); return result.object.query; } catch (e) { console.error(e); throw new Error('Failed to generate query'); }
};
``` ---------------------------------------- TITLE: Using OpenAI Responses API for Web Search
DESCRIPTION: This snippet demonstrates how to use OpenAI's Responses API with the `web_search_preview` tool for native web searching. It shows how to generate text based on a prompt and retrieve search sources, leveraging the built-in search capabilities of models like `gpt-4o-mini`.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/56-web-search-agent.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai'; const { text, sources } = await generateText({ model: openai.responses('gpt-4o-mini'), prompt: 'What happened in San Francisco last week?', tools: { web_search_preview: openai.tools.webSearchPreview(), },
}); console.log(text);
console.log(sources);
``` ---------------------------------------- TITLE: Defining a Generic Error Handler Function (TSX)
DESCRIPTION: This utility function `errorHandler` takes an unknown error type and returns a string representation of the error. It handles null, string, and Error instances, falling back to JSON stringification for other types. This function is designed to provide a consistent way to format error messages before they are exposed or logged.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/12-use-chat-an-error-occurred.mdx#_snippet_0 LANGUAGE: TSX
CODE:
```
export function errorHandler(error: unknown) { if (error == null) { return 'unknown error'; } if (typeof error === 'string') { return error; } if (error instanceof Error) { return error.message; } return JSON.stringify(error);
}
``` ---------------------------------------- TITLE: Caching OpenAI Responses with Vercel KV and Next.js using AI SDK
DESCRIPTION: This snippet demonstrates how to cache OpenAI responses using the `onFinish` lifecycle callback from the AI SDK Core. It utilizes Upstash Redis (Vercel KV) to store the generated text for 1 hour, preventing redundant API calls for identical requests. It checks for a cached response before calling the model and stores the new response upon completion.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/04-caching.mdx#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { formatDataStreamPart, streamText } from 'ai';
import { Redis } from '@upstash/redis'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; const redis = new Redis({ url: process.env.KV_URL, token: process.env.KV_TOKEN,
}); export async function POST(req: Request) { const { messages } = await req.json(); // come up with a key based on the request: const key = JSON.stringify(messages); // Check if we have a cached response const cached = await redis.get(key); if (cached != null) { return new Response(formatDataStreamPart('text', cached), { status: 200, headers: { 'Content-Type': 'text/plain' }, }); } // Call the language model: const result = streamText({ model: openai('gpt-4o'), messages, async onFinish({ text }) { // Cache the response text: await redis.set(key, text); await redis.expire(key, 60 * 60); }, }); // Respond with the stream return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Generate Text with Anthropic Model using AI SDK
DESCRIPTION: An example demonstrating how to use the `generateText` function from the AI SDK with the Anthropic provider. It shows how to specify an Anthropic model (e.g., 'claude-3-haiku-20240307') and provide a text prompt to generate a response.
SOURCE: https://github.com/vercel/ai/blob/main/packages/anthropic/README.md#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { anthropic } from '@ai-sdk/anthropic';
import { generateText } from 'ai'; const { text } = await generateText({ model: anthropic('claude-3-haiku-20240307'), prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
``` ---------------------------------------- TITLE: Generate Text with AI SDK and Amazon Bedrock
DESCRIPTION: This example demonstrates how to use the AI SDK's `generateText` function with the Amazon Bedrock provider. It imports the `bedrock` instance and `generateText`, then uses a specific Llama 3 model to generate a vegetarian lasagna recipe based on a given prompt.
SOURCE: https://github.com/vercel/ai/blob/main/packages/amazon-bedrock/README.md#_snippet_2 LANGUAGE: ts
CODE:
```
import { bedrock } from '@ai-sdk/amazon-bedrock';
import { generateText } from 'ai'; const { text } = await generateText({ model: bedrock('meta.llama3-8b-instruct-v1:0'), prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
``` ---------------------------------------- TITLE: Generate Text with Zhipu AI Model
DESCRIPTION: Example demonstrating how to use the Zhipu AI provider to generate text using a specified GLM model.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/95-zhipu.mdx#_snippet_5 LANGUAGE: ts
CODE:
```
import { zhipu } from 'zhipu-ai-provider'; const { text } = await generateText({ model: zhipu('glm-4-plus'), prompt: 'Why is the sky blue?'
}); console.log(result);
``` ---------------------------------------- TITLE: Installing AI SDK Dependencies with yarn
DESCRIPTION: This command installs the core AI SDK package (`ai`), its React hooks (`@ai-sdk/react`), the OpenAI provider (`@ai-sdk/openai`), and `zod` for schema validation using yarn. These packages are essential for building AI-powered features and interacting with OpenAI models in the Next.js application.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#_snippet_4 LANGUAGE: Shell
CODE:
```
yarn add ai @ai-sdk/react @ai-sdk/openai zod
``` ---------------------------------------- TITLE: Server Action for Generating Structured Notifications (TypeScript)
DESCRIPTION: This server-side function `getNotifications` uses the `generateObject` function from the AI SDK to create structured notification data. It leverages Zod to define a precise schema for the `notifications` array, ensuring the AI model generates data conforming to the specified `name`, `message`, and `minutesAgo` fields. It uses `gpt-4-turbo` model.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/30-generate-object.mdx#_snippet_1 LANGUAGE: TypeScript
CODE:
```
'use server'; import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; export async function getNotifications(input: string) { 'use server'; const { object: notifications } = await generateObject({ model: openai('gpt-4-turbo'), system: 'You generate three notifications for a messages app.', prompt: input, schema: z.object({ notifications: z.array( z.object({ name: z.string().describe('Name of a fictional person.'), message: z.string().describe('Do not use emojis or links.'), minutesAgo: z.number() }) ) }) }); return { notifications };
}
``` ---------------------------------------- TITLE: Generating Structured Data with generateObject (TypeScript)
DESCRIPTION: This snippet demonstrates how to use the `generateObject` function from the AI SDK to generate structured data. It defines a Zod schema for a `recipe` object, including its name, ingredients, and steps, and then prompts the model to generate a lasagna recipe conforming to this schema. The `object` property of the result contains the validated, generated data.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { generateObject } from 'ai';
import { z } from 'zod'; const { object } = await generateObject({ model: yourModel, schema: z.object({ recipe: z.object({ name: z.string(), ingredients: z.array(z.object({ name: z.string(), amount: z.string() })), steps: z.array(z.string()), }), }), prompt: 'Generate a lasagna recipe.',
});
``` ---------------------------------------- TITLE: Streaming Assistant Response in Next.js Client (TSX)
DESCRIPTION: This client-side component demonstrates how to integrate the `useAssistant` hook from `@ai-sdk/react` to create a chat interface. It manages the assistant's status, displays streamed messages, and handles user input for submitting new messages. The `api` prop points to the server-side endpoint for assistant communication.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/120-stream-assistant-response.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
'use client'; import { Message, useAssistant } from '@ai-sdk/react'; export default function Page() { const { status, messages, input, submitMessage, handleInputChange } = useAssistant({ api: '/api/assistant' }); return ( <div className="flex flex-col gap-2"> <div className="p-2">status: {status}</div> <div className="flex flex-col p-2 gap-2"> {messages.map((message: Message) => ( <div key={message.id} className="flex flex-row gap-2"> <div className="w-24 text-zinc-500">{`${message.role}: `}</div> <div className="w-full">{message.content}</div> </div> ))} </div> <form onSubmit={submitMessage} className="fixed bottom-0 p-2 w-full"> <input disabled={status !== 'awaiting_message'} value={input} onChange={handleInputChange} className="bg-zinc-100 w-full p-2" /> </form> </div> );
}
``` ---------------------------------------- TITLE: Creating Stock Display Component (TypeScript/React)
DESCRIPTION: This asynchronous React component fetches and displays stock data for a given symbol and number of months. It makes an API call to `https://api.example.com/stock/{symbol}/{numOfMonths}` and renders the stock symbol along with a timeline of data points, including date and value.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/90-render-visual-interface-in-chat.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
export async function Stock({ symbol, numOfMonths }) { const data = await fetch( `https://api.example.com/stock/${symbol}/${numOfMonths}`, ); return ( <div> <div>{symbol}</div> <div> {data.timeline.map(data => ( <div> <div>{data.date}</div> <div>{data.value}</div> </div> ))} </div> </div> );
}
``` ---------------------------------------- TITLE: Handling Assistant Messages in Next.js API Route (TSX)
DESCRIPTION: This server-side API route handles incoming messages from the client, interacts with the OpenAI Assistant API, and streams responses back. It uses the `AssistantResponse` function from `ai` to manage the streaming process, creating new threads if necessary and forwarding the run stream from OpenAI to the client.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/120-stream-assistant-response.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
import OpenAI from 'openai';
import { AssistantResponse } from 'ai'; const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY || '',
}); export async function POST(req: Request) { const input: { threadId: string | null; message: string; } = await req.json(); const threadId = input.threadId ?? (await openai.beta.threads.create({})).id; const createdMessage = await openai.beta.threads.messages.create(threadId, { role: 'user', content: input.message, }); return AssistantResponse( { threadId, messageId: createdMessage.id }, async ({ forwardStream }) => { const runStream = openai.beta.threads.runs.stream(threadId, { assistant_id: process.env.ASSISTANT_ID ?? (() => { throw new Error('ASSISTANT_ID environment is not set'); })(), }); await forwardStream(runStream); }, );
}
``` ---------------------------------------- TITLE: Streaming Text with AI SDK's streamText (TypeScript)
DESCRIPTION: This code demonstrates how to use the `streamText` function for real-time text generation, which is essential for interactive applications like chatbots. It shows how to consume the `textStream` as an asynchronous iterable, allowing parts of the generated text to be processed as they become available, improving perceived performance.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { streamText } from 'ai'; const result = streamText({ model: yourModel, prompt: 'Invent a new holiday and describe its traditions.',
}); // example: use textStream as an async iterable
for await (const textPart of result.textStream) { console.log(textPart);
}
``` ---------------------------------------- TITLE: Define and Use AI SDK Tool for Adding Resources in Next.js API Route
DESCRIPTION: This snippet demonstrates how to define an `addResource` tool using the AI SDK within a Next.js API route. It integrates with `@ai-sdk/openai` for model interaction and `zod` for parameter validation. The tool's `execute` function calls a custom `createResource` action to persist content to a knowledge base. The system prompt guides the model to use this tool for unprompted knowledge additions.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#_snippet_19 LANGUAGE: tsx
CODE:
```
import { createResource } from '@/lib/actions/resources';
import { openai } from '@ai-sdk/openai';
import { streamText, tool } from 'ai';
import { z } from 'zod'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: openai('gpt-4o'), system: `You are a helpful assistant. Check your knowledge base before answering any questions. Only respond to questions using information from tool calls. if no relevant information is found in the tool calls, respond, "Sorry, I don't know."`, messages, tools: { addResource: tool({ description: `add a resource to your knowledge base. If the user provides a random piece of knowledge unprompted, use this tool without asking for confirmation.`, parameters: z.object({ content: z .string() .describe('the content or resource to add to the knowledge base'), }), execute: async ({ content }) => createResource({ content }), }), }, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Basic Chatbot Implementation with useChat
DESCRIPTION: This example demonstrates a basic chatbot application using the `useChat` hook from `@ai-sdk/react` for the frontend and a simple API route for the backend. It shows how to display messages, handle user input, and submit messages to an AI provider.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Page() { const { messages, input, handleInputChange, handleSubmit } = useChat({}); return ( <> {messages.map(message => ( <div key={message.id}> {message.role === 'user' ? 'User: ' : 'AI: '} {message.content} </div> ))} <form onSubmit={handleSubmit}> <input name="prompt" value={input} onChange={handleInputChange} /> <button type="submit">Submit</button> </form> </> );
}
``` LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: openai('gpt-4-turbo'), system: 'You are a helpful assistant.', messages, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Integrating Caching Middleware with AI SDK streamText (TypeScript)
DESCRIPTION: This TypeScript snippet demonstrates how to integrate a custom caching middleware (`cached`) with the AI SDK's `streamText` function. It configures an OpenAI `gpt-4o` model wrapped by the `cached` middleware to generate text, and then streams the output to the console, also logging token usage and finish reason. It requires `@ai-sdk/openai`, `ai`, and `dotenv` packages, and the custom `your-cache-middleware`.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/80-local-caching-middleware.mdx#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import 'dotenv/config';
import { cached } from '../middleware/your-cache-middleware'; async function main() { const result = streamText({ model: cached(openai('gpt-4o')), maxTokens: 512, temperature: 0.3, maxRetries: 5, prompt: 'Invent a new holiday and describe its traditions.', }); for await (const textPart of result.textStream) { process.stdout.write(textPart); } console.log(); console.log('Token usage:', await result.usage); console.log('Finish reason:', await result.finishReason);
} main().catch(console.error);
``` ---------------------------------------- TITLE: Defining a Weather Tool with `tool()` in TypeScript
DESCRIPTION: This snippet demonstrates how to define a `weatherTool` using the `tool()` helper from the `ai` library in TypeScript. It utilizes `zod` for defining the `parameters` schema, specifically for a `location` string. The `tool()` function enables TypeScript to correctly infer the type of `location` within the `execute` method's arguments, ensuring type safety for the tool's execution logic.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/20-tool.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { tool } from 'ai';
import { z } from 'zod'; export const weatherTool = tool({ description: 'Get the weather in a location', parameters: z.object({ location: z.string().describe('The location to get the weather for'), }), // location below is inferred to be a string: execute: async ({ location }) => ({ location, temperature: 72 + Math.floor(Math.random() * 21) - 10, }),
});
``` ---------------------------------------- TITLE: Generating Text with OpenAI GPT-3.5 Turbo in TypeScript
DESCRIPTION: This snippet demonstrates how to use the `generateText` function from the AI SDK to generate text. It imports necessary modules, initializes the OpenAI GPT-3.5 Turbo model, and sends a simple prompt. The generated text result is then logged to the console. This requires the `@ai-sdk/openai` package and an OpenAI API key.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/10-generate-text.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai'; const result = await generateText({ model: openai('gpt-3.5-turbo'), prompt: 'Why is the sky blue?',
}); console.log(result);
``` ---------------------------------------- TITLE: Configuring AI SDK with Server Actions and UI State Transformation (TypeScript)
DESCRIPTION: This `ai.ts` file configures the AI SDK using `createAI`, defining the server actions available (e.g., `continueConversation`). Crucially, the `onGetUIState` function, marked as a server action, retrieves the current AI state and transforms `ServerMessage` objects into `ClientMessage` objects suitable for UI display, handling special cases like rendering `Stock` components for function calls.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/61-restore-messages-from-database.mdx#_snippet_2 LANGUAGE: tsx
CODE:
```
import { createAI } from 'ai/rsc';
import { ServerMessage, ClientMessage, continueConversation } from './actions';
import { Stock } from '@ai-studio/components/stock';
import { generateId } from 'ai'; export const AI = createAI<ServerMessage[], ClientMessage[]> ({ actions: { continueConversation, }, onGetUIState: async () => { 'use server'; // Get the current AI state (stored messages) const history: ServerMessage[] = getAIState(); // Transform server messages into client messages return history.map(({ role, content }) => ({ id: generateId(), role, display: role === 'function' ? <Stock {...JSON.parse(content)} /> : content, })); },
});
``` ---------------------------------------- TITLE: Generating Structured Data with Reasoning and Extraction Models (TypeScript)
DESCRIPTION: This snippet demonstrates a two-step process to generate structured data using AI models. First, it uses a DeepSeek reasoning model (`deepseek-reasoner`) to generate a raw text prediction about future city populations. Second, it employs an OpenAI model (`gpt-4o-mini`) to extract and structure this information into an array of objects based on a Zod schema, effectively overcoming the lack of structured output support in reasoning models.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/30-generate-object-reasoning.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { deepseek } from '@ai-sdk/deepseek';
import { openai } from '@ai-sdk/openai';
import { generateObject, generateText } from 'ai';
import 'dotenv/config';
import { z } from 'zod'; async function main() { const { text: rawOutput } = await generateText({ model: deepseek('deepseek-reasoner'), prompt: 'Predict the top 3 largest city by 2050. For each, return the name, the country, the reason why it will on the list, and the estimated population in millions.', }); const { object } = await generateObject({ model: openai('gpt-4o-mini'), prompt: 'Extract the desired information from this text: \n' + rawOutput, schema: z.object({ name: z.string().describe('the name of the city'), country: z.string().describe('the name of the country'), reason: z .string() .describe( 'the reason why the city will be one of the largest cities by 2050', ), estimatedPopulation: z.number(), }), output: 'array', }); console.log(object);
} main().catch(console.error);
``` ---------------------------------------- TITLE: Handling Customer Queries with AI Routing (TypeScript)
DESCRIPTION: This snippet illustrates a routing pattern where an AI model dynamically directs the workflow. It first classifies a customer query using `generateObject` to determine its type (general, refund, technical) and complexity. Based on this classification, it then uses `generateText` with a dynamically selected model and a tailored system prompt to generate a response. It relies on `@ai-sdk/openai` and `zod`.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateObject, generateText } from 'ai';
import { z } from 'zod'; async function handleCustomerQuery(query: string) { const model = openai('gpt-4o'); // First step: Classify the query type const { object: classification } = await generateObject({ model, schema: z.object({ reasoning: z.string(), type: z.enum(['general', 'refund', 'technical']), complexity: z.enum(['simple', 'complex']), }), prompt: `Classify this customer query: ${query} Determine: 1. Query type (general, refund, or technical) 2. Complexity (simple or complex) 3. Brief reasoning for classification`, }); // Route based on classification // Set model and system prompt based on query type and complexity const { text: response } = await generateText({ model: classification.complexity === 'simple' ? openai('gpt-4o-mini') : openai('o3-mini'), system: { general: 'You are an expert customer service agent handling general inquiries.', refund: 'You are a customer service agent specializing in refund requests. Follow company policy and collect necessary information.', technical: 'You are a technical support specialist with deep product knowledge. Focus on clear step-by-step troubleshooting.', }[classification.type], prompt: query, }); return { response, classification };
}
``` ---------------------------------------- TITLE: Streaming AI Text Completions with AI SDK UI Route Handler
DESCRIPTION: This Next.js API route handler demonstrates the recommended approach with AI SDK UI, separating AI text generation from UI rendering. It uses `streamText` to generate and stream the AI response based on incoming messages, returning a `DataStreamResponse` that can be consumed by client-side hooks like `useChat`.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#_snippet_2 LANGUAGE: ts
CODE:
```
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai'; export async function POST(request) { const { messages } = await request.json(); const result = streamText({ model: openai('gpt-4o'), system: 'you are a friendly assistant!', messages, tools: { // tool definitions } }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Intercepting and Confirming AI SDK Tool Calls in React (TSX)
DESCRIPTION: This frontend React component (using `useChat` from `@ai-sdk/react`) shows how to intercept tool invocations from the AI model. It specifically checks for the `getWeatherInformation` tool call, presents a confirmation UI to the user, and uses `addToolResult` to send the user's "Yes" or "No" decision back to the model, completing the Human-in-the-Loop flow.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#_snippet_3 LANGUAGE: TSX
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Chat() { const { messages, input, handleInputChange, handleSubmit, addToolResult } = useChat(); return ( <div> <div> {messages?.map(m => ( <div key={m.id}> <strong>{`${m.role}: `}</strong> {m.parts?.map((part, i) => { switch (part.type) { case 'text': return <div key={i}>{part.text}</div>; case 'tool-invocation': const toolInvocation = part.toolInvocation; const toolCallId = toolInvocation.toolCallId; // render confirmation tool (client-side tool with user interaction) if ( toolInvocation.toolName === 'getWeatherInformation' && toolInvocation.state === 'call' ) { return ( <div key={toolCallId}> Get weather information for {toolInvocation.args.city}? <div> <button onClick={() => addToolResult({ toolCallId, result: 'Yes, confirmed.', }) } > Yes </button> <button onClick={() => addToolResult({ toolCallId, result: 'No, denied.', }) } > No </button> </div> </div> ); } } })} <br /> </div> ))} </div> <form onSubmit={handleSubmit}> <input value={input} placeholder="Say something..." onChange={handleInputChange} /> </form> </div> );
}
``` ---------------------------------------- TITLE: LLM Query With RAG Context
DESCRIPTION: An example illustrating how providing relevant context through RAG enables an LLM to answer specific user queries accurately.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#_snippet_1 LANGUAGE: txt
CODE:
```
**input**
Respond to the user's prompt using only the provided context.
user prompt: 'What is my favorite food?'
context: user loves chicken nuggets **generation**
Your favorite food is chicken nuggets!
``` ---------------------------------------- TITLE: Creating SvelteKit Chat API Endpoint (TypeScript)
DESCRIPTION: This TypeScript code defines a SvelteKit API endpoint (`+server.ts`) responsible for handling chat requests. It initializes the OpenAI provider using the API key, extracts conversation messages from the incoming request, and then streams text responses from the `gpt-4o` model back to the client.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { createOpenAI } from '@ai-sdk/openai';
import { streamText } from 'ai'; import { OPENAI_API_KEY } from '$env/static/private'; const openai = createOpenAI({ apiKey: OPENAI_API_KEY,
}); export async function POST({ request }) { const { messages } = await request.json(); const result = streamText({ model: openai('gpt-4o'), messages, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Embedding a Single Value using AI SDK with OpenAI
DESCRIPTION: This snippet demonstrates how to embed a single text value using the `embed` function from the AI SDK. It utilizes the OpenAI `text-embedding-3-small` model to convert the input string into a numerical vector (embedding). The result is a single embedding object, which is an array of numbers.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/30-embeddings.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
import { embed } from 'ai';
import { openai } from '@ai-sdk/openai'; // 'embedding' is a single embedding object (number[])
const { embedding } = await embed({ model: openai.embedding('text-embedding-3-small'), value: 'sunny day at the beach',
});
``` ---------------------------------------- TITLE: Accessing Full Stream Events with `fullStream` Property in AI SDK Core `streamText` (TypeScript)
DESCRIPTION: This snippet demonstrates how to access the `fullStream` property of the `streamText` result, which provides an asynchronous iterator over all stream events. This allows for custom handling of different event types such as `text-delta`, `reasoning`, `source`, `tool-call`, `tool-result`, `finish`, and `error`, enabling highly customized UI implementations or complex stream processing logic.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { streamText } from 'ai';
import { z } from 'zod'; const result = streamText({ model: yourModel, tools: { cityAttractions: { parameters: z.object({ city: z.string() }), execute: async ({ city }) => ({ attractions: ['attraction1', 'attraction2', 'attraction3'], }), }, }, prompt: 'What are some San Francisco tourist attractions?',
}); for await (const part of result.fullStream) { switch (part.type) { case 'text-delta': { // handle text delta here break; } case 'reasoning': { // handle reasoning here break; } case 'source': { // handle source here break; } case 'tool-call': { switch (part.toolName) { case 'cityAttractions': { // handle tool call here break; } } break; } case 'tool-result': { switch (part.toolName) { case 'cityAttractions': { // handle tool result here break; } } break; } case 'finish': { // handle finish here break; } case 'error': { // handle error here break; } }
}
``` ---------------------------------------- TITLE: Reading Streamable Value in Client Component (AI SDK RSC) - TSX
DESCRIPTION: This client-side React component showcases the usage of `readStreamableValue` to process data from a server-side stream. Upon a button click, it fetches the stream and asynchronously iterates over its emitted deltas, updating the component's state to display the accumulating generation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/05-read-streamable-value.mdx#_snippet_2 LANGUAGE: TSX
CODE:
```
import { readStreamableValue } from 'ai/rsc'; export default function Page() { const [generation, setGeneration] = useState(''); return ( <div> <button onClick={async () => { const stream = await generate(); for await (const delta of readStreamableValue(stream)) { setGeneration(generation => generation + delta); } }} > Generate </button> </div> );
}
``` ---------------------------------------- TITLE: Submitting User Messages and Streaming AI Responses (OpenAI, TSX)
DESCRIPTION: This server action `submitMessage` handles user input, creates or updates an OpenAI thread, initiates a run with the Assistant API, and streams the assistant's response back to the client using `createStreamableUI` and `createStreamableValue`. It manages multiple runs in a queue and processes events from the OpenAI stream.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/120-stream-assistant-response.mdx#_snippet_2 LANGUAGE: TSX
CODE:
```
'use server'; import { generateId } from 'ai';
import { createStreamableUI, createStreamableValue } from 'ai/rsc';
import { OpenAI } from 'openai';
import { ReactNode } from 'react';
import { Message } from './message'; const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY,
}); export interface ClientMessage { id: string; status: ReactNode; text: ReactNode;
} const ASSISTANT_ID = 'asst_xxxx';
let THREAD_ID = '';
let RUN_ID = ''; export async function submitMessage(question: string): Promise<ClientMessage> { const statusUIStream = createStreamableUI('thread.init'); const textStream = createStreamableValue(''); const textUIStream = createStreamableUI( <Message textStream={textStream.value} />, ); const runQueue = []; (async () => { if (THREAD_ID) { await openai.beta.threads.messages.create(THREAD_ID, { role: 'user', content: question, }); const run = await openai.beta.threads.runs.create(THREAD_ID, { assistant_id: ASSISTANT_ID, stream: true, }); runQueue.push({ id: generateId(), run }); } else { const run = await openai.beta.threads.createAndRun({ assistant_id: ASSISTANT_ID, stream: true, thread: { messages: [{ role: 'user', content: question }], }, }); runQueue.push({ id: generateId(), run }); } while (runQueue.length > 0) { const latestRun = runQueue.shift(); if (latestRun) { for await (const delta of latestRun.run) { const { data, event } = delta; statusUIStream.update(event); if (event === 'thread.created') { THREAD_ID = data.id; } else if (event === 'thread.run.created') { RUN_ID = data.id; } else if (event === 'thread.message.delta') { data.delta.content?.map(part => { if (part.type === 'text') { if (part.text) { textStream.append(part.text.value as string); } } }); } else if (event === 'thread.run.failed') { console.error(data); } } } } statusUIStream.done(); textStream.done(); })(); return { id: generateId(), status: statusUIStream.value, text: textUIStream.value, };
}
``` ---------------------------------------- TITLE: Set Laminar Project API Key in .env
DESCRIPTION: After signing up or self-hosting Laminar and creating a project, obtain your API key from project settings and set it as an environment variable `LMNR_PROJECT_API_KEY` in your `.env` file.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/laminar.mdx#_snippet_1 LANGUAGE: bash
CODE:
```
LMNR_PROJECT_API_KEY=...
``` ---------------------------------------- TITLE: Create Next.js Server Action for Streaming UI Components with AI SDK RSC
DESCRIPTION: This server-side Next.js action uses `streamUI` from `ai/rsc` to dynamically stream React components based on AI model decisions and tool calls. It defines a `getWeather` tool that yields a loading state before returning a weather component, showcasing how to build generative user interfaces that adapt to AI outputs and external data.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#_snippet_7 LANGUAGE: tsx
CODE:
```
'use server'; import { streamUI } from 'ai/rsc';
import { deepinfra } from '@ai-sdk/deepinfra';
import { z } from 'zod'; export async function streamComponent() { const result = await streamUI({ model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'), prompt: 'Get the weather for San Francisco', text: ({ content }) => <div>{content}</div>, tools: { getWeather: { description: 'Get the weather for a location', parameters: z.object({ location: z.string() }), generate: async function* ({ location }) { yield <div>loading...</div>; const weather = '25c'; // await getWeather(location); return ( <div> the weather in {location} is {weather}. </div> ); }, }, }, }); return result.value;
}
``` ---------------------------------------- TITLE: Set Requesty API Key Environment Variable
DESCRIPTION: Securely configure your Requesty API key by setting it as an environment variable named `REQUESTY_API_KEY`. Examples are provided for Linux/Mac (bash), Windows Command Prompt, and Windows PowerShell.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/5-requesty.mdx#_snippet_1 LANGUAGE: bash
CODE:
```
# Linux/Mac
export REQUESTY_API_KEY=your_api_key_here
``` LANGUAGE: cmd
CODE:
```
# Windows Command Prompt
set REQUESTY_API_KEY=your_api_key_here
``` LANGUAGE: powershell
CODE:
```
# Windows PowerShell
$env:REQUESTY_API_KEY="your_api_key_here"
``` ---------------------------------------- TITLE: Streaming Text Generations with OpenAI GPT-4o (TypeScript)
DESCRIPTION: This snippet demonstrates how to use `streamText` to generate text from an OpenAI GPT-4o model. It initializes the model with a prompt and then iterates asynchronously over the `textStream` to print each part of the generated text to standard output, suitable for real-time display in interactive applications.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/02-stream-text.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai'; const { textStream } = streamText({ model: openai('gpt-4o'), prompt: 'Invent a new holiday and describe its traditions.',
}); for await (const textPart of textStream) { process.stdout.write(textPart);
}
``` ---------------------------------------- TITLE: Calculating Cosine Similarity between Embeddings with AI SDK
DESCRIPTION: This snippet demonstrates how to calculate the cosine similarity between two embeddings using the `cosineSimilarity` function from the AI SDK. It first embeds two distinct phrases using `embedMany` and then computes their similarity score, which can be used to measure how related the phrases are.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/30-embeddings.mdx#_snippet_2 LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { cosineSimilarity, embedMany } from 'ai'; const { embeddings } = await embedMany({ model: openai.embedding('text-embedding-3-small'), values: ['sunny day at the beach', 'rainy afternoon in the city'],
}); console.log( `cosine similarity: ${cosineSimilarity(embeddings[0], embeddings[1])}`,
);
``` ---------------------------------------- TITLE: Generate Text with Claude 3.7 Sonnet using AI SDK
DESCRIPTION: This snippet demonstrates how to generate text using Claude 3.7 Sonnet via the `@ai-sdk/anthropic` package. It imports the necessary modules, initializes the model, and calls `generateText` with a prompt to receive a text response.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/20-sonnet-3-7.mdx#_snippet_0 LANGUAGE: ts
CODE:
```
import { anthropic } from '@ai-sdk/anthropic';
import { generateText } from 'ai'; const { text, reasoning, reasoningDetails } = await generateText({ model: anthropic('claude-3-7-sonnet-20250219'), prompt: 'How many people will live in the world in 2040?',
});
console.log(text); // text response
``` ---------------------------------------- TITLE: Implement Chat UI with AI SDK `useChat` Hook in Next.js
DESCRIPTION: This React component (`app/page.tsx`) demonstrates how to integrate the `useChat` hook from `@ai-sdk/react` into a Next.js application. It manages chat state, handles user input, submits messages to the API route, and displays the conversation history. Errors during chat interaction are also handled.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/22-gpt-4-5.mdx#_snippet_5 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Page() { const { messages, input, handleInputChange, handleSubmit, error } = useChat(); return ( <> {messages.map(message => ( <div key={message.id}> {message.role === 'user' ? 'User: ' : 'AI: '} {message.content} </div> ))} <form onSubmit={handleSubmit}> <input name="prompt" value={input} onChange={handleInputChange} /> <button type="submit">Submit</button> </form> </> );
}
``` ---------------------------------------- TITLE: Generating Structured Data on Server-side with Next.js API Route
DESCRIPTION: This Next.js API route handles POST requests to generate structured notification objects using the `generateObject` function from the `ai` module. It defines a Zod schema (`z.object`) to enforce the structure of the generated JSON, including `name`, `message`, and `minutesAgo` fields. The route uses OpenAI's GPT-4 model and returns the generated object as a JSON response.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/30-generate-object.mdx#_snippet_1 LANGUAGE: typescript
CODE:
```
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; export async function POST(req: Request) { const { prompt }: { prompt: string } = await req.json(); const result = await generateObject({ model: openai('gpt-4'), system: 'You generate three notifications for a messages app.', prompt, schema: z.object({ notifications: z.array( z.object({ name: z.string().describe('Name of a fictional person.'), message: z.string().describe('Do not use emojis or links.'), minutesAgo: z.number(), }), ), }), }); return result.toJsonResponse();
}
``` ---------------------------------------- TITLE: Process File Inputs with Amazon Bedrock Model (TypeScript)
DESCRIPTION: This snippet illustrates how to send file inputs, such as PDF documents, to an Amazon Bedrock model using the `generateText` function. It demonstrates structuring messages to include both text and file content for multimodal processing.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#_snippet_10 LANGUAGE: ts
CODE:
```
import { bedrock } from '@ai-sdk/amazon-bedrock';
import { generateText } from 'ai'; const result = await generateText({ model: bedrock('anthropic.claude-3-haiku-20240307-v1:0'), messages: [ { role: 'user', content: [ { type: 'text', text: 'Describe the pdf in detail.' }, { type: 'file', data: fs.readFileSync('./data/ai.pdf'), mimeType: 'application/pdf', }, ], }, ],
});
``` ---------------------------------------- TITLE: Intercepting OpenAI AI SDK Fetch Requests for Logging in TypeScript
DESCRIPTION: This TypeScript snippet demonstrates how to intercept outgoing fetch requests made by the `@ai-sdk/openai` client. It defines a custom `fetch` function within `createOpenAI` that logs the request URL, headers, and parsed JSON body to the console before forwarding the request. This is useful for debugging, monitoring, or auditing API calls to the OpenAI service.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/70-intercept-fetch-requests.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { generateText } from 'ai';
import { createOpenAI } from '@ai-sdk/openai'; const openai = createOpenAI({ // example fetch wrapper that logs the input to the API call: fetch: async (url, options) => { console.log('URL', url); console.log('Headers', JSON.stringify(options!.headers, null, 2)); console.log( `Body ${JSON.stringify(JSON.parse(options!.body! as string), null, 2)}`, ); return await fetch(url, options); },
}); const { text } = await generateText({ model: openai('gpt-3.5-turbo'), prompt: 'Why is the sky blue?',
});
``` ---------------------------------------- TITLE: Handling Server-Side Assistant Responses and Tool Calls (Next.js API)
DESCRIPTION: This Next.js API route (`app/api/assistant/route.ts`) handles incoming chat messages, interacts with the OpenAI Assistant API, and streams responses back to the client. It manages thread creation, message submission, and crucially, processes tool calls (like 'celsiusToFahrenheit') by executing the tool's logic and submitting the outputs back to the assistant to continue the conversation.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/121-stream-assistant-response-with-tools.mdx#_snippet_2 LANGUAGE: tsx
CODE:
```
import { AssistantResponse } from 'ai';
import OpenAI from 'openai'; const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY || '',
}); export async function POST(req: Request) { const input: { threadId: string | null; message: string; } = await req.json(); const threadId = input.threadId ?? (await openai.beta.threads.create({})).id; const createdMessage = await openai.beta.threads.messages.create(threadId, { role: 'user', content: input.message, }); return AssistantResponse( { threadId, messageId: createdMessage.id }, async ({ forwardStream }) => { const runStream = openai.beta.threads.runs.stream(threadId, { assistant_id: process.env.ASSISTANT_ID ?? (() => { throw new Error('ASSISTANT_ID is not set'); })(), }); let runResult = await forwardStream(runStream); while ( runResult?.status === 'requires_action' && runResult.required_action?.type === 'submit_tool_outputs' ) { const tool_outputs = runResult.required_action.submit_tool_outputs.tool_calls.map( (toolCall: any) => { const parameters = JSON.parse(toolCall.function.arguments); switch (toolCall.function.name) { case 'celsiusToFahrenheit': const celsius = parseFloat(parameters.value); const fahrenheit = celsius * (9 / 5) + 32; return { tool_call_id: toolCall.id, output: `${celsius}°C is ${fahrenheit.toFixed(2)}°F`, }; default: throw new Error( `Unknown tool call function: ${toolCall.function.name}`, ); } }, ); runResult = await forwardStream( openai.beta.threads.runs.submitToolOutputsStream( threadId, runResult.id, { tool_outputs }, ), ); } }, );
}
``` ---------------------------------------- TITLE: Consuming Streamed Text in React Client Component (app/page.tsx)
DESCRIPTION: This React client component demonstrates how to initiate a text generation request to a server action and consume the streamed output. It uses `readStreamableValue` from `ai/rsc` to process text deltas in real-time, updating the component's state and displaying the generated text incrementally as it arrives.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/20-stream-text.mdx#_snippet_0 LANGUAGE: TSX
CODE:
```
'use client'; import { useState } from 'react';
import { generate } from './actions';
import { readStreamableValue } from 'ai/rsc'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export default function Home() { const [generation, setGeneration] = useState<string>(''); return ( <div> <button onClick={async () => { const { output } = await generate('Why is the sky blue?'); for await (const delta of readStreamableValue(output)) { setGeneration(currentGeneration => `${currentGeneration}${delta}`); } }} > Ask </button> <div>{generation}</div> </div> );
}
``` ---------------------------------------- TITLE: Implementing Streaming AI Chatbot (TypeScript)
DESCRIPTION: This TypeScript code sets up a command-line chatbot using the AI SDK. It initializes a readline interface for user input, maintains conversation history, streams responses from the OpenAI model, and prints the assistant's output to the terminal in real-time.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/06-nodejs.mdx#_snippet_4 LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { CoreMessage, streamText } from 'ai';
import dotenv from 'dotenv';
import * as readline from 'node:readline/promises'; dotenv.config(); const terminal = readline.createInterface({ input: process.stdin, output: process.stdout,
}); const messages: CoreMessage[] = []; async function main() { while (true) { const userInput = await terminal.question('You: '); messages.push({ role: 'user', content: userInput }); const result = streamText({ model: openai('gpt-4o'), messages, }); let fullResponse = ''; process.stdout.write('\nAssistant: '); for await (const delta of result.textStream) { fullResponse += delta; process.stdout.write(delta); } process.stdout.write('\n\n'); messages.push({ role: 'assistant', content: fullResponse }); }
} main().catch(console.error);
``` ---------------------------------------- TITLE: Handling Chat Conversation in React Client Component
DESCRIPTION: This React client component manages the state of a user-model conversation. It displays messages, captures user input, and sends the updated conversation history to a server action (`continueConversation`) to generate new responses. The component uses `useState` to manage the `conversation` array and the current `input` message.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/11-generate-text-with-chat-prompt.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
'use client'; import { useState } from 'react';
import { Message, continueConversation } from './actions'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export default function Home() { const [conversation, setConversation] = useState<Message[]>([]); const [input, setInput] = useState<string>(''); return ( <div> <div> {conversation.map((message, index) => ( <div key={index}> {message.role}: {message.content} </div> ))} </div> <div> <input type="text" value={input} onChange={event => { setInput(event.target.value); }} /> <button onClick={async () => { const { messages } = await continueConversation([ ...conversation, { role: 'user', content: input }, ]); setConversation(messages); }} > Send Message </button> </div> </div> );
}
``` ---------------------------------------- TITLE: Creating Next.js Chat Route Handler
DESCRIPTION: This TypeScript code defines a Next.js Route Handler for a chat API endpoint (`/api/chat`). It uses the AI SDK to stream text responses from the OpenAI `gpt-4o` model based on incoming messages, setting a maximum duration for streaming.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/03-nextjs-pages-router.mdx#_snippet_7 LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: openai('gpt-4o'), messages, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Define Zod Schema for Chart Configuration
DESCRIPTION: This TypeScript snippet defines the `configSchema` using Zod, outlining the structure for chart configurations. It includes fields like `description`, `takeaway`, `type`, `xKey`, `yKeys`, and styling options, leveraging Zod's `.describe()` for AI model context and type inference for `Config`.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#_snippet_15 LANGUAGE: ts
CODE:
```
/* ...rest of the file... */ export const configSchema = z .object({ description: z .string() .describe( 'Describe the chart. What is it showing? What is interesting about the way the data is displayed?', ), takeaway: z.string().describe('What is the main takeaway from the chart?'), type: z.enum(['bar', 'line', 'area', 'pie']).describe('Type of chart'), title: z.string(), xKey: z.string().describe('Key for x-axis or category'), yKeys: z .array(z.string()) .describe( 'Key(s) for y-axis values this is typically the quantitative column', ), multipleLines: z .boolean() .describe( 'For line charts only: whether the chart is comparing groups of data.', ) .optional(), measurementColumn: z .string() .describe( 'For line charts only: key for quantitative y-axis column to measure against (eg. values, counts etc.)', ) .optional(), lineCategories: z .array(z.string()) .describe( 'For line charts only: Categories used to compare different lines or data series. Each category represents a distinct line in the chart.', ) .optional(), colors: z .record( z.string().describe('Any of the yKeys'), z.string().describe('Color value in CSS format (e.g., hex, rgb, hsl)') ) .describe('Mapping of data keys to color values for chart elements') .optional(), legend: z.boolean().describe('Whether to show legend') }) .describe('Chart configuration object'); export type Config = z.infer<typeof configSchema>;
``` ---------------------------------------- TITLE: Implementing Multi-Step Tool Calls with useChat Hook (React/Next.js)
DESCRIPTION: This React component uses the `@ai-sdk/react` `useChat` hook to manage conversation state and interact with a chat API. It configures `maxSteps: 5` to enable multiple sequential tool calls within a single generation turn, allowing the AI to perform complex tasks requiring chained tool executions. It handles user input and displays messages.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/72-call-tools-multiple-steps.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Page() { const { messages, input, setInput, append } = useChat({ api: '/api/chat', maxSteps: 5, }); return ( <div> <input value={input} onChange={event => { setInput(event.target.value); }} onKeyDown={async event => { if (event.key === 'Enter') { append({ content: input, role: 'user' }); } }} /> {messages.map((message, index) => ( <div key={index}>{message.content}</div> ))} </div> );
}
``` ---------------------------------------- TITLE: Implementing Caching with AI SDK Language Model Middleware in TypeScript
DESCRIPTION: This TypeScript code defines `cacheMiddleware`, a `LanguageModelV1Middleware` that integrates Redis for caching AI model responses. It implements `wrapGenerate` to cache direct responses and `wrapStream` to cache and simulate streamed responses using `simulateReadableStream`. Dependencies include `@upstash/redis` and `ai` SDK types, requiring `KV_URL` and `KV_TOKEN` environment variables for Redis connection.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/04-caching.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { Redis } from '@upstash/redis';
import { type LanguageModelV1, type LanguageModelV1Middleware, type LanguageModelV1StreamPart, simulateReadableStream,
} from 'ai'; const redis = new Redis({ url: process.env.KV_URL, token: process.env.KV_TOKEN,
}); export const cacheMiddleware: LanguageModelV1Middleware = { wrapGenerate: async ({ doGenerate, params }) => { const cacheKey = JSON.stringify(params); const cached = (await redis.get(cacheKey)) as Awaited< ReturnType<LanguageModelV1['doGenerate']> > | null; if (cached !== null) { return { ...cached, response: { ...cached.response, timestamp: cached?.response?.timestamp ? new Date(cached?.response?.timestamp) : undefined, }, }; } const result = await doGenerate(); redis.set(cacheKey, result); return result; }, wrapStream: async ({ doStream, params }) => { const cacheKey = JSON.stringify(params); // Check if the result is in the cache const cached = await redis.get(cacheKey); // If cached, return a simulated ReadableStream that yields the cached result if (cached !== null) { // Format the timestamps in the cached response const formattedChunks = (cached as LanguageModelV1StreamPart[]).map(p => { if (p.type === 'response-metadata' && p.timestamp) { return { ...p, timestamp: new Date(p.timestamp) }; } else return p; }); return { stream: simulateReadableStream({ initialDelayInMs: 0, chunkDelayInMs: 10, chunks: formattedChunks, }), rawCall: { rawPrompt: null, rawSettings: {} }, }; } // If not cached, proceed with streaming const { stream, ...rest } = await doStream(); const fullResponse: LanguageModelV1StreamPart[] = []; const transformStream = new TransformStream< LanguageModelV1StreamPart, LanguageModelV1StreamPart >({ transform(chunk, controller) { fullResponse.push(chunk); controller.enqueue(chunk); }, flush() { // Store the full response in the cache after streaming is complete redis.set(cacheKey, fullResponse); }, }); return { stream: stream.pipeThrough(transformStream), ...rest, }; },
};
``` ---------------------------------------- TITLE: Handling Streamed Text and Loading State in Next.js Client (AI SDK)
DESCRIPTION: This client-side React component demonstrates how to consume streamed text responses and a separate loading state from a server action. It uses `readStreamableValue` to iteratively update the UI with incoming text deltas and to manage a loading indicator, providing detailed feedback to the user during the AI generation process. The `maxDuration` export ensures the page remains dynamic for streaming.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/06-loading-state.mdx#_snippet_3 LANGUAGE: tsx
CODE:
```
'use client'; import { useState } from 'react';
import { generateResponse } from './actions';
import { readStreamableValue } from 'ai/rsc'; // Force the page to be dynamic and allow streaming responses up to 30 seconds
export const maxDuration = 30; export default function Home() { const [input, setInput] = useState<string>(''); const [generation, setGeneration] = useState<string>(''); const [loading, setLoading] = useState<boolean>(false); return ( <div> <div>{generation}</div> <form onSubmit={async e => { e.preventDefault(); setLoading(true); const { response, loadingState } = await generateResponse(input); let textContent = ''; for await (const responseDelta of readStreamableValue(response)) { textContent = `${textContent}${responseDelta}`; setGeneration(textContent); } for await (const loadingDelta of readStreamableValue(loadingState)) { if (loadingDelta) { setLoading(loadingDelta.loading); } } setInput(''); setLoading(false); }} > <input type="text" value={input} disabled={loading} className="disabled:opacity-50" onChange={event => { setInput(event.target.value); }} /> <button>Send Message</button> </form> </div> );
}
``` ---------------------------------------- TITLE: Updating Route Handler for AI Tool Processing in Next.js API Routes
DESCRIPTION: This snippet updates a Next.js API route handler ('POST') to process AI tool calls using the 'processToolCalls' utility function from the 'ai' SDK. It demonstrates how to handle tools requiring human confirmation by providing an 'execute' function for 'getWeatherInformation' within the 'processToolCalls' configuration, and then streams text responses using 'streamText'.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#_snippet_10 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { createDataStreamResponse, Message, streamText } from 'ai';
import { processToolCalls } from './utils';
import { tools } from './tools'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export async function POST(req: Request) { const { messages }: { messages: Message[] } = await req.json(); return createDataStreamResponse({ execute: async dataStream => { // Utility function to handle tools that require human confirmation // Checks for confirmation in last message and then runs associated tool const processedMessages = await processToolCalls( { messages, dataStream, tools, }, { // type-safe object for tools without an execute function getWeatherInformation: async ({ city }) => { const conditions = ['sunny', 'cloudy', 'rainy', 'snowy']; return `The weather in ${city} is ${ conditions[Math.floor(Math.random() * conditions.length)] }.`; }, }, ); const result = streamText({ model: openai('gpt-4o'), messages: processedMessages, tools, }); result.mergeIntoDataStream(dataStream); }, });
}
``` ---------------------------------------- TITLE: Server-Side UI Streaming with AI SDK RSC createStreamableUI
DESCRIPTION: This snippet illustrates how to use 'createStreamableUI' from 'ai/rsc' to stream React components directly from the server. Within a tool's 'execute' function (e.g., 'getWeather'), a React component (<WeatherCard/>) is rendered with dynamic data and then streamed to the client using 'uiStream.done()'. This shifts UI rendering logic to the server, simplifying client-side code and enabling server-driven UI updates.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/07-rendering-ui-with-language-models.mdx#_snippet_4 LANGUAGE: tsx
CODE:
```
import { createStreamableUI } from 'ai/rsc' const uiStream = createStreamableUI(); const text = generateText({ model: openai('gpt-3.5-turbo'), system: 'you are a friendly assistant' prompt: 'what is the weather in SF?' tools: { getWeather: { description: 'Get the weather for a location', parameters: z.object({ city: z.string().describe('The city to get the weather for'), unit: z .enum(['C', 'F']) .describe('The unit to display the temperature in') }), execute: async ({ city, unit }) => { const weather = getWeather({ city, unit }) const { temperature, unit, description, forecast } = weather uiStream.done( <WeatherCard weather={{ temperature: 47, unit: 'F', description: 'sunny' forecast, }} /> ) } } }
}) return { display: uiStream.value
}
``` ---------------------------------------- TITLE: Handling Tool Confirmation Response in AI API Route (TypeScript)
DESCRIPTION: This TypeScript snippet demonstrates how an API route handler processes tool invocation results, specifically focusing on confirmation states. It checks if a 'tool-invocation' part is for 'getWeatherInformation' and in a 'result' state. Based on the confirmation ('Yes, confirmed.' or 'No, denied.'), it either executes the tool via 'executeWeatherTool' or sets an error message, then updates the tool result and forwards it to the client. The updated messages are then streamed to the language model.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#_snippet_4 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { createDataStreamResponse, formatDataStreamPart, Message, streamText, tool,
} from 'ai';
import { z } from 'zod'; export async function POST(req: Request) { const { messages }: { messages: Message[] } = await req.json(); return createDataStreamResponse({ execute: async dataStream => { // pull out last message const lastMessage = messages[messages.length - 1]; lastMessage.parts = await Promise.all( // map through all message parts lastMessage.parts?.map(async part => { if (part.type !== 'tool-invocation') { return part; } const toolInvocation = part.toolInvocation; // return if tool isn't weather tool or in a result state if ( toolInvocation.toolName !== 'getWeatherInformation' || toolInvocation.state !== 'result' ) { return part; } // switch through tool result states (set on the frontend) switch (toolInvocation.result) { case 'Yes, confirmed.': { const result = await executeWeatherTool(toolInvocation.args); // forward updated tool result to the client: dataStream.write( formatDataStreamPart('tool_result', { toolCallId: toolInvocation.toolCallId, result, }), ); // update the message part: return { ...part, toolInvocation: { ...toolInvocation, result } }; } case 'No, denied.': { const result = 'Error: User denied access to weather information'; // forward updated tool result to the client: dataStream.write( formatDataStreamPart('tool_result', { toolCallId: toolInvocation.toolCallId, result, }), ); // update the message part: return { ...part, toolInvocation: { ...toolInvocation, result } }; } default: return part; } }) ?? [], ); const result = streamText({ model: openai('gpt-4o'), messages, tools: { getWeatherInformation: tool({ description: 'show the weather in a given city to the user', parameters: z.object({ city: z.string() }), }), }, }); result.mergeIntoDataStream(dataStream); }, });
} async function executeWeatherTool({}: { city: string }) { const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy']; return weatherOptions[Math.floor(Math.random() * weatherOptions.length)];
}
``` ---------------------------------------- TITLE: Implementing Client-Side Assistant Chat Interface (React/Next.js)
DESCRIPTION: This React component (`app/page.tsx`) sets up a client-side chat interface using the `useAssistant` hook from `@ai-sdk/react`. It displays the assistant's status and messages, and provides an input field for users to send messages. The input is disabled when the assistant is not awaiting a message, ensuring proper interaction flow.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/121-stream-assistant-response-with-tools.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
'use client'; import { Message, useAssistant } from '@ai-sdk/react'; export default function Page() { const { status, messages, input, submitMessage, handleInputChange } = useAssistant({ api: '/api/assistant' }); return ( <div className="flex flex-col gap-2"> <div className="p-2">status: {status}</div> <div className="flex flex-col p-2 gap-2"> {messages.map((message: Message) => ( <div key={message.id} className="flex flex-row gap-2"> <div className="w-24 text-zinc-500">{`${message.role}: `}</div> <div className="w-full">{message.content}</div> </div> ))} </div> <form onSubmit={submitMessage} className="fixed bottom-0 p-2 w-full"> <input disabled={status !== 'awaiting_message'} value={input} onChange={handleInputChange} className="bg-zinc-100 w-full p-2" /> </form> </div> );
}
``` ---------------------------------------- TITLE: Server-side AI Conversation Actions (TSX)
DESCRIPTION: This server-side module defines AI actions for continuing a conversation. It uses `getMutableAIState` and `streamUI` from `ai/rsc` with OpenAI's GPT-3.5-turbo model. It includes a `deploy` tool that simulates a deployment process, streaming UI updates.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/91-stream-updates-to-visual-interfaces.mdx#_snippet_1 LANGUAGE: TSX
CODE:
```
'use server'; import { getMutableAIState, streamUI } from 'ai/rsc';
import { openai } from '@ai-sdk/openai';
import { ReactNode } from 'react';
import { z } from 'zod';
import { generateId } from 'ai'; export interface ServerMessage { role: 'user' | 'assistant'; content: string;
} export interface ClientMessage { id: string; role: 'user' | 'assistant'; display: ReactNode;
} export async function continueConversation( input: string,
): Promise<ClientMessage> { 'use server'; const history = getMutableAIState(); const result = await streamUI({ model: openai('gpt-3.5-turbo'), messages: [...history.get(), { role: 'user', content: input }], text: ({ content, done }) => { if (done) { history.done((messages: ServerMessage[]) => [ ...messages, { role: 'assistant', content }, ]); } return <div>{content}</div>; }, tools: { deploy: { description: 'Deploy repository to vercel', parameters: z.object({ repositoryName: z .string() .describe('The name of the repository, example: vercel/ai-chatbot'), }), generate: async function* ({ repositoryName }) { yield <div>Cloning repository {repositoryName}...</div>; // [!code highlight:5] await new Promise(resolve => setTimeout(resolve, 3000)); yield <div>Building repository {repositoryName}...</div>; await new Promise(resolve => setTimeout(resolve, 2000)); return <div>{repositoryName} deployed!</div>; }, }, }, }); return { id: generateId(), role: 'assistant', display: result.value, };
}
``` ---------------------------------------- TITLE: Generating Multiple Embeddings with AI SDK and OpenAI (TypeScript)
DESCRIPTION: This snippet demonstrates how to use the `embedMany` function to generate embeddings for a list of text values. It imports `openai` from `@ai-sdk/openai` and `embedMany` from `ai`, then calls `embedMany` with an OpenAI embedding model and an array of strings. The function returns the generated `embeddings` which are in the same order as the input `values`.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/06-embed-many.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { embedMany } from 'ai'; const { embeddings } = await embedMany({ model: openai.embedding('text-embedding-3-small'), values: [ 'sunny day at the beach', 'rainy afternoon in the city', 'snowy night in the mountains', ],
});
``` ---------------------------------------- TITLE: Stream Chat Responses with AI SDK API Route
DESCRIPTION: Implements a Next.js App Router API route (`POST`) to stream text responses from an OpenAI model. It uses `streamText` to process client messages with 'gpt-4o' and returns a data stream, enabling real-time chat.
SOURCE: https://github.com/vercel/ai/blob/main/packages/ai/README.md#_snippet_5 LANGUAGE: typescript
CODE:
```
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai'; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: openai('gpt-4o'), system: 'You are a helpful assistant.', messages, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Persisting Chat History Across Requests (TypeScript/TSX)
DESCRIPTION: Demonstrates how to persist chat history using the Responses API by linking subsequent requests with a `previousResponseId`. This enables OpenAI to access the full conversation context, even when only the last message is sent.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/19-openai-responses.mdx#_snippet_5 LANGUAGE: tsx
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai'; const result1 = await generateText({ model: openai.responses('gpt-4o-mini'), prompt: 'Invent a new holiday and describe its traditions.',
}); const result2 = await generateText({ model: openai.responses('gpt-4o-mini'), prompt: 'Summarize in 2 sentences', providerOptions: { openai: { previousResponseId: result1.providerMetadata?.openai.responseId as string, }, },
});
``` ---------------------------------------- TITLE: Building a Client-Side Chat Interface with Next.js and React
DESCRIPTION: This client-side component creates a basic chat user interface. It uses React's `useState` hook to manage the input field and the conversation history. User messages are sent to the `/api/chat` endpoint, and the assistant's responses are fetched and displayed, updating the chat history dynamically.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/11-generate-text-with-chat-prompt.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
'use client'; import { CoreMessage } from 'ai';
import { useState } from 'react'; export default function Page() { const [input, setInput] = useState(''); const [messages, setMessages] = useState<CoreMessage[]>([]); return ( <div> <input value={input} onChange={event => { setInput(event.target.value); }} onKeyDown={async event => { if (event.key === 'Enter') { setMessages(currentMessages => [ ...currentMessages, { role: 'user', content: input }, ]); const response = await fetch('/api/chat', { method: 'POST', body: JSON.stringify({ messages: [...messages, { role: 'user', content: input }], }), }); const { messages: newMessages } = await response.json(); setMessages(currentMessages => [ ...currentMessages, ...newMessages, ]); } }} /> {messages.map((message, index) => ( <div key={`${message.role}-${index}`}> {typeof message.content === 'string' ? message.content : message.content .filter(part => part.type === 'text') .map((part, partIndex) => ( <div key={partIndex}>{part.text}</div> ))} </div> ))} </div> );
}
``` ---------------------------------------- TITLE: Configuring Common Settings for AI SDK generateText (TypeScript)
DESCRIPTION: This snippet demonstrates how to use the `generateText` function with common settings such as `maxTokens`, `temperature`, and `maxRetries`. These settings allow fine-grained control over the LLM's output, influencing aspects like response length, randomness, and robustness against transient errors.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/25-settings.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
const result = await generateText({ model: yourModel, maxTokens: 512, temperature: 0.3, maxRetries: 5, prompt: 'Invent a new holiday and describe its traditions.',
});
``` ---------------------------------------- TITLE: Define Server Action to Stream UI Components with AI SDK Tools
DESCRIPTION: This server action (`app/actions.tsx`) demonstrates how to use `ai/rsc` to stream React components. It defines a `getWeather` tool with a loading state and a final `WeatherComponent` for rendering. The `streamComponent` function orchestrates the AI model interaction, tool execution, and UI streaming.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/02-streaming-react-components.mdx#_snippet_3 LANGUAGE: tsx
CODE:
```
'use server'; import { streamUI } from 'ai/rsc';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; const LoadingComponent = () => ( <div className="animate-pulse p-4">getting weather...</div>
); const getWeather = async (location: string) => { await new Promise(resolve => setTimeout(resolve, 2000)); return '82°F️ ☀️';
}; interface WeatherProps { location: string; weather: string;
} const WeatherComponent = (props: WeatherProps) => ( <div className="border border-neutral-200 p-4 rounded-lg max-w-fit"> The weather in {props.location} is {props.weather} </div>
); export async function streamComponent() { const result = await streamUI({ model: openai('gpt-4o'), prompt: 'Get the weather for San Francisco', text: ({ content }) => <div>{content}</div>, tools: { getWeather: { description: 'Get the weather for a location', parameters: z.object({ location: z.string(), }), generate: async function* ({ location }) { yield <LoadingComponent />; const weather = await getWeather(location); return <WeatherComponent weather={weather} location={location} />; } } } }); return result.value;
}
``` ---------------------------------------- TITLE: Client-side useObject Hook for Array Generation (TSX)
DESCRIPTION: This client-side component uses the experimental_useObject hook from @ai-sdk/react to generate an array of notification objects based on the notificationSchema. It demonstrates how to submit a prompt, handle loading states, stop generation, and render the received objects, wrapping the schema in z.array() for array output.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/40-stream-object.mdx#_snippet_5 LANGUAGE: tsx
CODE:
```
'use client'; import { experimental_useObject as useObject } from '@ai-sdk/react';
import { notificationSchema } from './api/use-object/schema'; export default function Page() { const { object, submit, isLoading, stop } = useObject({ api: '/api/use-object', schema: z.array(notificationSchema), }); return ( <div> <button onClick={() => submit('Messages during finals week.')} disabled={isLoading} > Generate notifications </button> {isLoading && ( <div> <div>Loading...</div> <button type="button" onClick={() => stop()}> Stop </button> </div> )} {object?.map((notification, index) => ( <div key={index}> <p>{notification.name}</p> <p>{notification.message}</p> </div> ))} </div> );
}
``` ---------------------------------------- TITLE: Displaying Multi-Step AI Responses with AI SDK (Client)
DESCRIPTION: This client-side React component demonstrates how to consume and display multi-part AI responses using the `useChat` hook from `@ai-sdk/react`. It iterates through message parts, rendering text directly and formatting tool invocations as JSON, providing a unified UI for multi-step AI interactions.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/24-stream-text-multistep.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Chat() { const { messages, input, handleInputChange, handleSubmit } = useChat(); return ( <div> {messages?.map(message => ( <div key={message.id}> <strong>{`${message.role}: `}</strong> {message.parts.map((part, index) => { switch (part.type) { case 'text': return <span key={index}>{part.text}</span>; case 'tool-invocation': { return ( <pre key={index}> {JSON.stringify(part.toolInvocation, null, 2)} </pre> ); } } })} </div> ))} <form onSubmit={handleSubmit}> <input value={input} onChange={handleInputChange} /> </form> </div> );
}
``` ---------------------------------------- TITLE: Throttling UI Updates with useChat Hook (TypeScript)
DESCRIPTION: This snippet demonstrates how to resolve the 'Maximum update depth exceeded' error when using the `useChat` hook from the AI SDK. By setting `experimental_throttle` to 50ms, UI updates are limited to every 50 milliseconds, preventing excessive re-renders caused by streaming AI responses and improving performance.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/50-react-maximum-update-depth-exceeded.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
const { messages, ... } = useChat({ // Throttle the messages and data updates to 50ms: experimental_throttle: 50
})
``` ---------------------------------------- TITLE: Implementing Chat UI with useChat Hook in Next.js
DESCRIPTION: This React component, designed for a Next.js client page, utilizes the `useChat` hook from `@ai-sdk/react` to manage chat state, input, and submission. It renders messages, including user and AI responses, and provides an input form for user interaction, demonstrating a complete frontend chat interface.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/18-claude-4.mdx#_snippet_4 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Page() { const { messages, input, handleInputChange, handleSubmit, error } = useChat(); return ( <div className="flex flex-col h-screen max-w-2xl mx-auto p-4"> <div className="flex-1 overflow-y-auto space-y-4 mb-4"> {messages.map(message => ( <div key={message.id} className={`p-3 rounded-lg ${ message.role === 'user' ? 'bg-blue-50 ml-auto' : 'bg-gray-50' }`} > <p className="font-semibold"> {message.role === 'user' ? 'You' : 'Claude 4'} </p> {message.parts.map((part, index) => { if (part.type === 'text') { return ( <div key={index} className="mt-1"> {part.text} </div> ); } if (part.type === 'reasoning') { return ( <pre key={index} className="bg-gray-100 p-2 rounded mt-2 text-xs overflow-x-auto" > <details> <summary className="cursor-pointer"> View reasoning </summary> {part.details.map(detail => detail.type === 'text' ? detail.text : '<redacted>', )} </details> </pre> ); } })} </div> ))} </div> <form onSubmit={handleSubmit} className="flex gap-2"> <input name="prompt" value={input} onChange={handleInputChange} className="flex-1 p-2 border rounded focus:outline-none focus:ring-2 focus:ring-blue-500" placeholder="Ask Claude 4 something..." /> <button type="submit" className="bg-blue-500 text-white px-4 py-2 rounded hover:bg-blue-600" > Send </button> </form> </div> );
}
``` ---------------------------------------- TITLE: Forcing Structured Outputs with Answer Tool in AI SDK
DESCRIPTION: This snippet demonstrates how to use an 'answer' tool with 'toolChoice: 'required'' to force the LLM to provide a structured final output. It also includes a 'calculate' tool for mathematical evaluations. The 'answer' tool has no 'execute' function, causing the agent to terminate upon its invocation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText, tool } from 'ai';
import 'dotenv/config';
import { z } from 'zod'; const { toolCalls } = await generateText({ model: openai('gpt-4o-2024-08-06', { structuredOutputs: true }), tools: { calculate: tool({ description: 'A tool for evaluating mathematical expressions. Example expressions: ' + "'1.2 * (2 + 4.5)', '12.7 cm to inch', 'sin(45 deg) ^ 2'.", parameters: z.object({ expression: z.string() }), execute: async ({ expression }) => mathjs.evaluate(expression), }), // answer tool: the LLM will provide a structured answer answer: tool({ description: 'A tool for providing the final answer.', parameters: z.object({ steps: z.array( z.object({ calculation: z.string(), reasoning: z.string(), }), ), answer: z.string(), }), // no execute function - invoking it will terminate the agent }), }, toolChoice: 'required', maxSteps: 10, system: 'You are solving math problems. ' + 'Reason step by step. ' + 'Use the calculator when necessary. ' + 'The calculator can only do simple additions, subtractions, multiplications, and divisions. ' + 'When you give the final answer, provide an explanation for how you got it.', prompt: 'A taxi driver earns $9461 per 1-hour work. ' + 'If he works 12 hours a day and in 1 hour he uses 14-liters petrol with price $134 for 1-liter. ' + 'How much money does he earn in one day?',
}); console.log(`FINAL TOOL CALLS: ${JSON.stringify(toolCalls, null, 2)}`);
``` ---------------------------------------- TITLE: Implementing Guardrails for Language Model Output
DESCRIPTION: This example demonstrates how to implement basic guardrails as middleware to ensure the generated text is safe and appropriate. It uses the `wrapGenerate` function to intercept the model's output and perform filtering, such as redacting sensitive words.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/40-middleware.mdx#_snippet_9 LANGUAGE: TypeScript
CODE:
```
import type { LanguageModelV1Middleware } from 'ai'; export const yourGuardrailMiddleware: LanguageModelV1Middleware = { wrapGenerate: async ({ doGenerate }) => { const { text, ...rest } = await doGenerate(); // filtering approach, e.g. for PII or other sensitive information: const cleanedText = text?.replace(/badword/g, '<REDACTED>'); return { text: cleanedText, ...rest }; }, // here you would implement the guardrail logic for streaming // Note: streaming guardrails are difficult to implement, because // you do not know the full content of the stream until it's finished.
};
``` ---------------------------------------- TITLE: Streaming Text with Image Prompt using AI SDK and Anthropic (TypeScript)
DESCRIPTION: This TypeScript snippet demonstrates how to use the AI SDK to stream text responses from a vision-language model. It utilizes Anthropic's Claude 3.5 Sonnet model, providing both a text prompt and an image (read from a local file) as input. The generated text is then streamed part by part to standard output.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/22-stream-text-with-image-prompt.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { anthropic } from '@ai-sdk/anthropic';
import { streamText } from 'ai';
import 'dotenv/config';
import fs from 'node:fs'; async function main() { const result = streamText({ model: anthropic('claude-3-5-sonnet-20240620'), messages: [ { role: 'user', content: [ { type: 'text', text: 'Describe the image in detail.' }, { type: 'image', image: fs.readFileSync('./data/comic-cat.png') } ] } ] }); for await (const textPart of result.textStream) { process.stdout.write(textPart); }
} main().catch(console.error);
``` ---------------------------------------- TITLE: Implementing Client-Side Chat with AI SDK UI and Tool Invocations (TypeScript)
DESCRIPTION: This TypeScript React component demonstrates a client-side chatbot using the `useChat` hook from `@ai-sdk/react`. It configures `maxSteps` for multi-turn interactions, defines an `onToolCall` callback for automatic client-side tool execution (e.g., `getLocation`), and renders chat messages by distinguishing between text and various tool invocation states (e.g., `askForConfirmation`, `getLocation`, `getWeatherInformation`), allowing user interaction to add tool results.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-tool-usage.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
'use client'; import { ToolInvocation } from 'ai';
import { useChat } from '@ai-sdk/react'; export default function Chat() { const { messages, input, handleInputChange, handleSubmit, addToolResult } = useChat({ maxSteps: 5, // run client-side tools that are automatically executed: async onToolCall({ toolCall }) { if (toolCall.toolName === 'getLocation') { const cities = [ 'New York', 'Los Angeles', 'Chicago', 'San Francisco', ]; return cities[Math.floor(Math.random() * cities.length)]; } }, }); return ( <> {messages?.map(message => ( <div key={message.id}> <strong>{`${message.role}: `}</strong> {message.parts.map(part => { switch (part.type) { // render text parts as simple text: case 'text': return part.text; // for tool invocations, distinguish between the tools and the state: case 'tool-invocation': { const callId = part.toolInvocation.toolCallId; switch (part.toolInvocation.toolName) { case 'askForConfirmation': { switch (part.toolInvocation.state) { case 'call': return ( <div key={callId}> {part.toolInvocation.args.message} <div> <button onClick={() => addToolResult({ toolCallId: callId, result: 'Yes, confirmed.', }) } > Yes </button> <button onClick={() => addToolResult({ toolCallId: callId, result: 'No, denied', }) } > No </button> </div> </div> ); case 'result': return ( <div key={callId}> Location access allowed:{' '} {part.toolInvocation.result} </div> ); } break; } case 'getLocation': { switch (part.toolInvocation.state) { case 'call': return <div key={callId}>Getting location...</div>; case 'result': return ( <div key={callId}> Location: {part.toolInvocation.result} </div> ); } break; } case 'getWeatherInformation': { switch (part.toolInvocation.state) { // example of pre-rendering streaming tool calls: case 'partial-call': return ( <pre key={callId}> {JSON.stringify(part.toolInvocation, null, 2)} </pre> ); case 'call': return (
``` ---------------------------------------- TITLE: Processing Audio Input with GPT-4o-Audio-Preview (TypeScript)
DESCRIPTION: This snippet demonstrates how to use the gpt-4o-audio-preview model from @ai-sdk/openai to process audio files. It shows how to construct a message with both text and audio file content, using fs.readFileSync to load the audio data. This model is specifically designed for audio inputs.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_18 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai'; const result = await generateText({ model: openai('gpt-4o-audio-preview'), messages: [ { role: 'user', content: [ { type: 'text', text: 'What is the audio saying?' }, { type: 'file', mimeType: 'audio/mpeg', data: fs.readFileSync('./data/galileo.mp3'), }, ], }, ],
});
``` ---------------------------------------- TITLE: Creating a LlamaIndex Completion API Route with AI SDK (Next.js)
DESCRIPTION: This Next.js API route (`app/api/completion/route.ts`) demonstrates how to integrate LlamaIndex with the AI SDK to handle completion requests. It initializes an OpenAI LLM and a `SimpleChatEngine` from LlamaIndex, then streams the chat engine's response using `LlamaIndexAdapter.toDataStreamResponse` from the AI SDK. This setup pipes the LLM's output directly to the client, requiring `llamaindex` and `ai` packages.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/04-adapters/02-llamaindex.mdx#_snippet_0 LANGUAGE: TSX
CODE:
```
import { OpenAI, SimpleChatEngine } from 'llamaindex';
import { LlamaIndexAdapter } from 'ai'; export const maxDuration = 60; export async function POST(req: Request) { const { prompt } = await req.json(); const llm = new OpenAI({ model: 'gpt-4o' }); const chatEngine = new SimpleChatEngine({ llm }); const stream = await chatEngine.chat({ message: prompt, stream: true, }); return LlamaIndexAdapter.toDataStreamResponse(stream);
}
``` ---------------------------------------- TITLE: React Client for Streaming Chat with AI SDK
DESCRIPTION: This React component (`Home`) manages the conversation state and user input for a chat interface. It leverages `continueConversation` (a server action) to send user messages and `readStreamableValue` from `ai/rsc` to asynchronously stream and display the AI assistant's responses in real-time, providing an immediate user experience. It also configures `maxDuration` for the streaming response.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/21-stream-text-with-chat-prompt.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
'use client'; import { useState } from 'react';
import { Message, continueConversation } from './actions';
import { readStreamableValue } from 'ai/rsc'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export default function Home() { const [conversation, setConversation] = useState<Message[]>([]); const [input, setInput] = useState<string>(''); return ( <div> <div> {conversation.map((message, index) => ( <div key={index}> {message.role}: {message.content} </div> ))} </div> <div> <input type="text" value={input} onChange={event => { setInput(event.target.value); }} /> <button onClick={async () => { const { messages, newMessage } = await continueConversation([ ...conversation, { role: 'user', content: input }, ]); let textContent = ''; for await (const delta of readStreamableValue(newMessage)) { textContent = `${textContent}${delta}`; setConversation([ ...messages, { role: 'assistant', content: textContent }, ]); } }} > Send Message </button> </div> </div> );
}
``` ---------------------------------------- TITLE: Call OpenAI o1-mini Model with AI SDK
DESCRIPTION: This snippet demonstrates how to call OpenAI's o1-mini model using the AI SDK's `generateText` function. It shows importing necessary modules and making a basic text generation call with a specified model and prompt.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/23-o1.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai'; const { text } = await generateText({ model: openai('o1-mini'), prompt: 'Explain the concept of quantum entanglement.',
});
``` ---------------------------------------- TITLE: Forwarding AI SDK Tool Calls to Client in TypeScript
DESCRIPTION: This snippet demonstrates how to configure an AI SDK tool on the backend (Node.js API route) to forward its invocation to the client instead of executing it automatically. By omitting the `execute` function from the `tool` definition, the tool call is passed to the frontend for Human-in-the-Loop (HITL) interaction, allowing the client to provide the final tool result.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { createDataStreamResponse, streamText, tool } from 'ai';
import { z } from 'zod'; export async function POST(req: Request) { const { messages } = await req.json(); return createDataStreamResponse({ execute: async dataStream => { const result = streamText({ model: openai('gpt-4o'), messages, tools: { getWeatherInformation: tool({ description: 'show the weather in a given city to the user', parameters: z.object({ city: z.string() }), // execute function removed to stop automatic execution }), }, }); result.mergeIntoDataStream(dataStream); }, });
}
``` ---------------------------------------- TITLE: Displaying Generated Images in Next.js Chat UI (React)
DESCRIPTION: This client-side React component creates a chat interface using the `useChat` hook. It renders conversation messages and dynamically displays images generated by the AI. When a `generateImage` tool invocation result is received, the component uses the Next.js `Image` component to render the base64 image data, providing a visual representation of the AI's output.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/12-generate-image-with-chat-prompt.mdx#_snippet_1 LANGUAGE: React
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react';
import Image from 'next/image'; export default function Chat() { const { messages, input, handleInputChange, handleSubmit } = useChat(); return ( <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch"> <div className="space-y-4"> {messages.map(m => ( <div key={m.id} className="whitespace-pre-wrap"> <div key={m.id}> <div className="font-bold">{m.role}</div> {m.toolInvocations ? ( m.toolInvocations.map(ti => ti.toolName === 'generateImage' ? ( ti.state === 'result' ? ( <Image key={ti.toolCallId} src={`data:image/png;base64,${ti.result.image}`} alt={ti.result.prompt} height={400} width={400} /> ) : ( <div key={ti.toolCallId} className="animate-pulse"> Generating image... </div> ) ) : null, ) ) : ( <p>{m.content}</p> )} </div> </div> ))} </div> <form onSubmit={handleSubmit}> <input className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl" value={input} placeholder="Say something..." onChange={handleInputChange} /> </form> </div> );
}
``` ---------------------------------------- TITLE: Define Drizzle ORM Schema for Embeddings Table
DESCRIPTION: This snippet defines the 'embeddings' table schema using Drizzle ORM for PostgreSQL. It includes columns for 'id', 'resourceId' (foreign key), 'content' (plain text chunk), and 'embedding' (vector representation), along with an HNSW index for similarity search performance.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#_snippet_6 LANGUAGE: tsx
CODE:
```
import { nanoid } from '@/lib/utils';
import { index, pgTable, text, varchar, vector } from 'drizzle-orm/pg-core';
import { resources } from './resources'; export const embeddings = pgTable( 'embeddings', { id: varchar('id', { length: 191 }) .primaryKey() .$defaultFn(() => nanoid()), resourceId: varchar('resource_id', { length: 191 }).references( () => resources.id, { onDelete: 'cascade' } ), content: text('content').notNull(), embedding: vector('embedding', { dimensions: 1536 }).notNull() }, table => ({ embeddingIndex: index('embeddingIndex').using( 'hnsw', table.embedding.op('vector_cosine_ops') ) })
);
``` ---------------------------------------- TITLE: Creating AI Context for State Management in TypeScript
DESCRIPTION: This snippet initializes the AI context using `createAI` from `ai/rsc`. It defines the initial states for both UI and AI, and registers the `submitUserMessage` server action. This context is crucial for managing the conversational state across the application, enabling the AI to maintain continuity and interact with the UI.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/04-multistep-interfaces.mdx#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { createAI } from 'ai/rsc';
import { submitUserMessage } from './actions'; export const AI = createAI<any[], React.ReactNode[]>( { initialUIState: [], initialAIState: [], actions: { submitUserMessage, }, },
); ``` ---------------------------------------- TITLE: Example Usage of pipeDataStreamToResponse - TSX
DESCRIPTION: This example demonstrates the usage of `pipeDataStreamToResponse` to pipe streaming data to a `serverResponse` object. It shows how to configure the response status, status text, and custom headers, and how to use the `execute` function to write data, message annotations, and merge other data streams. It also includes a custom `onError` handler for error management.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/02-ai-sdk-ui/42-pipe-data-stream-to-response.mdx#_snippet_1 LANGUAGE: TSX
CODE:
```
pipeDataStreamToResponse(serverResponse, { status: 200, statusText: 'OK', headers: { 'Custom-Header': 'value' }, async execute(dataStream) { // Write data dataStream.writeData({ value: 'Hello' }); // Write annotation dataStream.writeMessageAnnotation({ type: 'status', value: 'processing' }); // Merge another stream const otherStream = getAnotherStream(); dataStream.merge(otherStream); }, onError: error => `Custom error: ${error.message}`
});
``` ---------------------------------------- TITLE: Generate Structured JSON Data with AI SDK Core
DESCRIPTION: This snippet demonstrates how to use `generateObject` from AI SDK Core to generate type-safe JSON data conforming to a Zod schema. It shows an example of generating a recipe object with nested fields and arrays, ensuring the model output adheres to a predefined structure.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/24-o3.mdx#_snippet_2 LANGUAGE: tsx
CODE:
```
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; const { object } = await generateObject({ model: openai('o3-mini'), schema: z.object({ recipe: z.object({ name: z.string(), ingredients: z.array(z.object({ name: z.string(), amount: z.string() })), steps: z.array(z.string()) }) }), prompt: 'Generate a lasagna recipe.'
});
``` ---------------------------------------- TITLE: Generating Structured Objects with ChromeAI
DESCRIPTION: This example demonstrates using `generateObject` with ChromeAI to produce structured JSON output based on a Zod schema. It prompts the model to generate a lasagna recipe, ensuring the output conforms to the defined `recipe` object structure, which is useful for integrating AI responses into applications requiring specific data formats.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/04-chrome-ai.mdx#_snippet_7 LANGUAGE: javascript
CODE:
```
import { generateObject } from 'ai';
import { chromeai } from 'chrome-ai';
import { z } from 'zod'; const { object } = await generateObject({ model: chromeai('text'), schema: z.object({ recipe: z.object({ name: z.string(), ingredients: z.array( z.object({ name: z.string(), amount: z.string() }) ), steps: z.array(z.string()) }) }), prompt: 'Generate a lasagna recipe.'
}); console.log(object);
// { recipe: {...} }
``` ---------------------------------------- TITLE: Defining Custom Tools for AI Chatbot Route Handler with AI SDK
DESCRIPTION: This snippet demonstrates how to define and integrate multiple custom tools (`weather` and `convertFahrenheitToCelsius`) within an AI SDK route handler. These tools, defined using `ai`'s `tool` function and `zod` for parameter validation, allow the AI model to perform specific actions like fetching weather data or converting units, enhancing its ability to respond to complex queries.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#_snippet_13 LANGUAGE: tsx
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText, tool } from 'ai';
import { z } from 'zod'; export const maxDuration = 30; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: openai('gpt-4o'), messages, tools: { weather: tool({ description: 'Get the weather in a location (fahrenheit)', parameters: z.object({ location: z.string().describe('The location to get the weather for'), }), execute: async ({ location }) => { const temperature = Math.round(Math.random() * (90 - 32) + 32); return { location, temperature, }; }, }), convertFahrenheitToCelsius: tool({ description: 'Convert a temperature in fahrenheit to celsius', parameters: z.object({ temperature: z .number() .describe('The temperature in fahrenheit to convert'), }), execute: async ({ temperature }) => { const celsius = Math.round((temperature - 32) * (5 / 9)); return { celsius, }; }, }), }, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Generating Text with OpenAI Language Model
DESCRIPTION: This example illustrates how to use an OpenAI language model with the `generateText` function from the AI SDK. It takes a model instance and a prompt to generate a text response, such as a recipe.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_7 LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai'; const { text } = await generateText({ model: openai('gpt-4-turbo'), prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
``` ---------------------------------------- TITLE: Create Next.js API Route for AI Chat Streaming with AI SDK
DESCRIPTION: This server-side Next.js API route uses the AI SDK to stream text responses from a DeepInfra Llama 3.1 model. It accepts incoming messages, processes them with the AI model, and returns the streamed result as a data stream response, allowing for real-time chat interactions.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#_snippet_5 LANGUAGE: tsx
CODE:
```
import { streamText } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'), system: 'You are a helpful assistant.', messages, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Configuring Multi-Step Tool Calls with AI SDK and OpenAI
DESCRIPTION: This snippet demonstrates how to configure `generateText` for multi-step tool calls using the AI SDK and OpenAI's GPT-4-turbo model. It sets `maxSteps` to 5, enabling the model to make up to five iterative calls to tools. A `weather` tool is defined with Zod for parameter validation, simulating a weather lookup and showcasing how to integrate custom tool execution logic.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/53-call-tools-multiple-steps.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; const { text } = await generateText({ model: openai('gpt-4-turbo'), maxSteps: 5, tools: { weather: tool({ description: 'Get the weather in a location', parameters: z.object({ location: z.string().describe('The location to get the weather for'), }), execute: async ({ location }: { location: string }) => ({ location, temperature: 72 + Math.floor(Math.random() * 21) - 10, }), }), }, prompt: 'What is the weather in San Francisco?',
});
``` ---------------------------------------- TITLE: Throttling UI Updates with useCompletion Hook (TypeScript)
DESCRIPTION: This snippet illustrates how to apply throttling to the `useCompletion` hook from the AI SDK to prevent the 'Maximum update depth exceeded' error. Setting `experimental_throttle` to 50ms ensures that the UI only re-renders every 50 milliseconds, effectively managing updates from streaming AI completions and optimizing application responsiveness.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/50-react-maximum-update-depth-exceeded.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
const { completion, ... } = useCompletion({ // Throttle the completion and data updates to 50ms: experimental_throttle: 50
})
``` ---------------------------------------- TITLE: Adding Chunked Transfer-Encoding and Keep-Alive Headers in TSX
DESCRIPTION: This snippet demonstrates how to add 'Transfer-Encoding: chunked' and 'Connection: keep-alive' headers to the response using the AI SDK's 'toDataStreamResponse' method. These headers are crucial for ensuring proper streaming behavior in deployed environments, as they can prevent buffering issues that cause the full response to be sent at once instead of streamed.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/06-streaming-not-working-when-deployed.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
return result.toDataStreamResponse({ headers: { 'Transfer-Encoding': 'chunked', Connection: 'keep-alive' }
});
``` ---------------------------------------- TITLE: Server-side streamObject for Array Output (TypeScript)
DESCRIPTION: This server-side API route uses streamObject from ai to generate an array of notification objects. It configures the model (GPT-4 Turbo), specifies 'output: 'array'', and uses the notificationSchema to guide the generation, returning the result as a text stream response. It also sets maxDuration for the API route.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/40-stream-object.mdx#_snippet_6 LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';
import { notificationSchema } from './schema'; export const maxDuration = 30; export async function POST(req: Request) { const context = await req.json(); const result = streamObject({ model: openai('gpt-4-turbo'), output: 'array', schema: notificationSchema, prompt: `Generate 3 notifications for a messages app in this context:` + context, }); return result.toTextStreamResponse();
}
``` ---------------------------------------- TITLE: Handling Errors in AI SDK streamText (TSX)
DESCRIPTION: This snippet demonstrates how to use the `onError` callback with the `streamText` function to log errors that occur during streaming. Since `streamText` streams errors rather than throwing them, the `onError` callback provides a mechanism to capture and process these errors, preventing silent failures and enabling custom error logging or handling logic.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/15-stream-text-not-working.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
import { streamText } from 'ai'; const result = streamText({ model: yourModel, prompt: 'Invent a new holiday and describe its traditions.', onError({ error }) { console.error(error); // your error logging logic here },
});
``` ---------------------------------------- TITLE: Displaying AI Tool Invocations in Chat UI (React/TypeScript)
DESCRIPTION: This code updates the React UI component to display different types of message parts received from the AI model. It uses the `useChat` hook from `@ai-sdk/react` to manage chat state. For 'text' parts, it renders the text, and for 'tool-invocation' parts, it serializes and displays the tool call details as JSON, allowing users to see when and how tools are invoked.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#_snippet_11 LANGUAGE: TypeScript
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Chat() { const { messages, input, handleInputChange, handleSubmit } = useChat(); return ( <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch"> {messages.map(message => ( <div key={message.id} className="whitespace-pre-wrap"> {message.role === 'user' ? 'User: ' : 'AI: '} {message.parts.map((part, i) => { switch (part.type) { case 'text': return <div key={`${message.id}-${i}`}>{part.text}</div>; case 'tool-invocation': return ( <pre key={`${message.id}-${i}`}> {JSON.stringify(part.toolInvocation, null, 2)} </pre> ); } })} </div> ))} <form onSubmit={handleSubmit}> <input className="fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl" value={input} placeholder="Say something..." onChange={handleInputChange} /> </form> </div> );
}
``` ---------------------------------------- TITLE: Disabling Content Encoding for AI SDK Streaming Responses (TypeScript/React)
DESCRIPTION: This code snippet demonstrates how to configure the AI SDK's `toDataStreamResponse` method to prevent proxy middleware from compressing the streaming response. By explicitly setting the `'Content-Encoding'` header to `'none'`, it ensures that the stream is delivered without compression, resolving issues where streaming fails and only a full response is returned after a delay. This solution specifically targets the streaming API.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/06-streaming-not-working-when-proxied.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
return result.toDataStreamResponse({ headers: { 'Content-Encoding': 'none', }, });
``` ---------------------------------------- TITLE: Configuring OpenAI Completion Model with Advanced Settings (TypeScript)
DESCRIPTION: This example illustrates how to apply model-specific settings to an OpenAI completion model during its creation. It showcases optional parameters like `echo` (to include the prompt in the completion), `logitBias` (to modify token likelihood), `suffix` (to append text after completion), and `user` (for abuse monitoring).
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_29 LANGUAGE: TypeScript
CODE:
```
const model = openai.completion('gpt-3.5-turbo-instruct', { echo: true, // optional, echo the prompt in addition to the completion logitBias: { // optional likelihood for specific tokens '50256': -100, }, suffix: 'some text', // optional suffix that comes after a completion of inserted text user: 'test-user', // optional unique user identifier
});
``` ---------------------------------------- TITLE: Create Root Page with `useChat` Hook
DESCRIPTION: This code sets up the frontend root page (`app/page.tsx`) to create a conversational user interface using the AI SDK's `useChat` hook. It handles chat message streaming, manages input state, and automatically updates the UI. By default, it sends POST requests to the `/api/chat` endpoint.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#_snippet_14 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Chat() { const { messages, input, handleInputChange, handleSubmit } = useChat(); return ( <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch"> <div className="space-y-4"> {messages.map(m => ( <div key={m.id} className="whitespace-pre-wrap"> <div> <div className="font-bold">{m.role}</div> <p>{m.content}</p> </div> </div> ))} </div> <form onSubmit={handleSubmit}> <input className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl" value={input} placeholder="Say something..." onChange={handleInputChange} /> </form> </div> );
}
``` ---------------------------------------- TITLE: Generating AI Responses in TypeScript Server Action
DESCRIPTION: This server action, `continueConversation`, is responsible for interacting with the AI model to generate text responses. It takes the conversation `history` as input, uses the `@ai-sdk/openai` package to call the `gpt-3.5-turbo` model with a system prompt, and appends the AI's generated response to the conversation history before returning it. It also defines the `Message` interface for conversation objects.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/11-generate-text-with-chat-prompt.mdx#_snippet_1 LANGUAGE: typescript
CODE:
```
'use server'; import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai'; export interface Message { role: 'user' | 'assistant'; content: string;
} export async function continueConversation(history: Message[]) { 'use server'; const { text } = await generateText({ model: openai('gpt-3.5-turbo'), system: 'You are a friendly assistant!', messages: history, }); return { messages: [ ...history, { role: 'assistant' as const, content: text, }, ], };
}
``` ---------------------------------------- TITLE: Recording Final Object with onFinish Callback using AI SDK (TypeScript)
DESCRIPTION: This snippet demonstrates how to use the `onFinish` callback with `streamObject` from the AI SDK to capture the complete structured object after streaming. It shows how to handle both successful object retrieval and type validation errors, logging the final object or the error.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/46-stream-object-record-final-object.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';
import { z } from 'zod'; const result = streamObject({ model: openai('gpt-4-turbo'), schema: z.object({ recipe: z.object({ name: z.string(), ingredients: z.array(z.string()), steps: z.array(z.string()) }) }), prompt: 'Generate a lasagna recipe.', onFinish({ object, error }) { // handle type validation failure (when the object does not match the schema): if (object === undefined) { console.error('Error:', error); return; } console.log('Final object:', JSON.stringify(object, null, 2)); }
});
``` ---------------------------------------- TITLE: Server-Side AI Conversation Logic with Token Usage Tracking (AI SDK)
DESCRIPTION: This server action (`app/actions.tsx`) implements the core AI conversation logic using `ai/rsc` and `@ai-sdk/openai`. It streams UI updates back to the client, defines a 'deploy' tool, and critically, utilizes the `onFinish` callback within `streamUI` to log `promptTokens`, `completionTokens`, and `totalTokens` for usage tracking after the AI stream completes. It depends on `createAI`, `getMutableAIState`, `streamUI`, and `openai`.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/92-stream-ui-record-token-usage.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
'use server'; import { createAI, getMutableAIState, streamUI } from 'ai/rsc';
import { openai } from '@ai-sdk/openai';
import { ReactNode } from 'react';
import { z } from 'zod';
import { generateId } from 'ai'; export interface ServerMessage { role: 'user' | 'assistant'; content: string;
} export interface ClientMessage { id: string; role: 'user' | 'assistant'; display: ReactNode;
} export async function continueConversation( input: string,
): Promise<ClientMessage> { 'use server'; const history = getMutableAIState(); const result = await streamUI({ model: openai('gpt-3.5-turbo'), messages: [...history.get(), { role: 'user', content: input }], text: ({ content, done }) => { if (done) { history.done((messages: ServerMessage[]) => [ ...messages, { role: 'assistant', content }, ]); } return <div>{content}</div>; }, tools: { deploy: { description: 'Deploy repository to vercel', parameters: z.object({ repositoryName: z .string() .describe('The name of the repository, example: vercel/ai-chatbot'), }), generate: async function* ({ repositoryName }) { yield <div>Cloning repository {repositoryName}...</div>; // [!code highlight:5] await new Promise(resolve => setTimeout(resolve, 3000)); yield <div>Building repository {repositoryName}...</div>; await new Promise(resolve => setTimeout(resolve, 2000)); return <div>{repositoryName} deployed!</div>; }, }, }, onFinish: ({ usage }) => { const { promptTokens, completionTokens, totalTokens } = usage; // your own logic, e.g. for saving the chat history or recording usage console.log('Prompt tokens:', promptTokens); console.log('Completion tokens:', completionTokens); console.log('Total tokens:', totalTokens); }, }); return { id: generateId(), role: 'assistant', display: result.value, };
}
``` ---------------------------------------- TITLE: Streaming Structured Output with streamText in TypeScript
DESCRIPTION: This snippet illustrates how to use `streamText` with the `experimental_output` setting to stream partially generated structured objects. It defines the same output schema as `generateText` for an example person, allowing for real-time processing of structured data as it becomes available.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#_snippet_11 LANGUAGE: TypeScript
CODE:
```
// experimental_partialOutputStream contains generated partial objects:
const { experimental_partialOutputStream } = await streamText({ // ... experimental_output: Output.object({ schema: z.object({ name: z.string(), age: z.number().nullable().describe('Age of the person.'), contact: z.object({ type: z.literal('email'), value: z.string(), }), occupation: z.object({ type: z.literal('employed'), company: z.string(), position: z.string(), }), }), }), prompt: 'Generate an example person for testing.',
});
``` ---------------------------------------- TITLE: Initializing a Provider Registry with Multiple AI Providers (TypeScript)
DESCRIPTION: This snippet demonstrates how to create a central registry for AI providers and their models using `createProviderRegistry`. It shows how to register providers like Anthropic with a default setup and OpenAI with a custom setup, including an API key. This setup allows for easy access to models via a unified interface.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/40-provider-registry.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { anthropic } from '@ai-sdk/anthropic';
import { createOpenAI } from '@ai-sdk/openai';
import { createProviderRegistry } from 'ai'; export const registry = createProviderRegistry({ // register provider with prefix and default setup: anthropic, // register provider with prefix and custom setup: openai: createOpenAI({ apiKey: process.env.OPENAI_API_KEY, }),
});
``` ---------------------------------------- TITLE: Generate Text with OpenAI Provider and AI SDK
DESCRIPTION: An example demonstrating how to use the OpenAI provider with the AI SDK's 'generateText' function to create a text-based response from an OpenAI model.
SOURCE: https://github.com/vercel/ai/blob/main/packages/openai/README.md#_snippet_2 LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai'; const { text } = await generateText({ model: openai('gpt-4-turbo'), prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
``` ---------------------------------------- TITLE: Handling Streamed UI Components in Next.js Client (AI SDK)
DESCRIPTION: This client-side React component simplifies the handling of AI responses by directly rendering streamed React components received from the server. Unlike the text-streaming example, this approach offloads UI rendering logic to the server, allowing the client to simply set the received `React.ReactNode` as its generation state and display it.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/06-loading-state.mdx#_snippet_5 LANGUAGE: tsx
CODE:
```
'use client'; import { useState } from 'react';
import { generateResponse } from './actions';
import { readStreamableValue } from 'ai/rsc'; // Force the page to be dynamic and allow streaming responses up to 30 seconds
export const maxDuration = 30; export default function Home() { const [input, setInput] = useState<string>(''); const [generation, setGeneration] = useState<React.ReactNode>(); return ( <div> <div>{generation}</div> <form onSubmit={async e => { e.preventDefault(); const result = await generateResponse(input); setGeneration(result); setInput(''); }} > <input type="text" value={input} onChange={event => { setInput(event.target.value); }} /> <button>Send Message</button> </form> </div> );
}
``` ---------------------------------------- TITLE: Wrapping Application with Vercel AI Provider (TSX)
DESCRIPTION: This snippet shows how to integrate the Vercel AI SDK provider (`AI` component) into a React layout. By wrapping the `children` prop, all components within this layout will have access to the AI context and functionalities defined by the `AI` provider.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/121-stream-assistant-response-with-tools.mdx#_snippet_5 LANGUAGE: tsx
CODE:
```
import { ReactNode } from 'react';
import { AI } from './ai'; export default function Layout({ children }: { children: ReactNode }) { return <AI>{children}</AI>;
}
``` ---------------------------------------- TITLE: Generating Text with Portkey and AI SDK (JavaScript)
DESCRIPTION: This example illustrates how to use Portkey's chat model with the AI SDK's `generateText` function to get a single text response. It initializes the Portkey provider and then calls `generateText` with the Portkey chat model and a prompt. The generated text is then logged to the console.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/10-portkey.mdx#_snippet_5 LANGUAGE: JavaScript
CODE:
```
import { createPortkey } from '@portkey-ai/vercel-provider';
import { generateText } from 'ai'; const portkey = createPortkey({ apiKey: 'YOUR_PORTKEY_API_KEY', config: portkeyConfig,
}); const { text } = await generateText({ model: portkey.chatModel(''), prompt: 'What is Portkey?',
}); console.log(text);
``` ---------------------------------------- TITLE: Bootstrap Next.js AI Chat App with create-next-app
DESCRIPTION: Commands to quickly set up a new Next.js project based on the AI SDK, Next.js, and OpenAI telemetry example. These commands use `create-next-app` with different package managers to clone and initialize the project.
SOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai-telemetry/README.md#_snippet_0 LANGUAGE: bash
CODE:
```
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry next-openai-telemetry-app
``` LANGUAGE: bash
CODE:
```
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry next-openai-telemetry-app
``` LANGUAGE: bash
CODE:
```
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry next-openai-telemetry-app
``` ---------------------------------------- TITLE: Generating Slogans with Few-Shot Examples - Prompt Engineering
DESCRIPTION: This advanced prompt demonstrates few-shot learning by providing examples of business types and their corresponding slogans. It instructs the LLM to generate three slogans for a 'Coffee shop with live music' by inferring the desired pattern from the provided examples, leading to more accurate and contextually relevant outputs.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/01-prompt-engineering.mdx#_snippet_3 LANGUAGE: Natural Language
CODE:
```
Create three slogans for a business with unique features. Business: Bookstore with cats
Slogans: "Purr-fect Pages", "Books and Whiskers", "Novels and Nuzzles"
Business: Gym with rock climbing
Slogans: "Peak Performance", "Reach New Heights", "Climb Your Way Fit"
Business: Coffee shop with live music
Slogans:
``` ---------------------------------------- TITLE: Sending Custom Data with Data Stream (Node.js)
DESCRIPTION: This Node.js server example shows how to use `pipeDataStreamToResponse` with a custom `execute` function to send initial data before merging the AI SDK's text stream. It also includes an `onError` handler to expose error messages to the client, demonstrating advanced data streaming control.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/10-node-http-server.mdx#_snippet_2 LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { pipeDataStreamToResponse, streamText } from 'ai';
import { createServer } from 'http'; createServer(async (req, res) => { // immediately start streaming the response pipeDataStreamToResponse(res, { execute: async dataStreamWriter => { dataStreamWriter.writeData('initialized call'); const result = streamText({ model: openai('gpt-4o'), prompt: 'Invent a new holiday and describe its traditions.', }); result.mergeIntoDataStream(dataStreamWriter); }, onError: error => { // Error messages are masked by default for security reasons. // If you want to expose the error message to the client, you can do so here: return error instanceof Error ? error.message : String(error); } });
}).listen(8080);
``` ---------------------------------------- TITLE: Creating a Chat Route Handler with Anthropic AI (TypeScript)
DESCRIPTION: This server-side route handler processes incoming chat messages and streams responses using Anthropic's Claude 3.5 Sonnet model. It's designed to handle AI interactions, including potential PDF understanding, by setting a maximum duration for the request and using the `streamText` utility from `@ai-sdk/anthropic`.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/23-chat-with-pdf.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { anthropic } from '@ai-sdk/anthropic';
import { streamText } from 'ai'; export const maxDuration = 30; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: anthropic('claude-3-5-sonnet-latest'), messages, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Testing generateObject with MockLanguageModelV1 and Zod in TypeScript
DESCRIPTION: This snippet shows how to unit test the `generateObject` function, which generates structured JSON, using `MockLanguageModelV1` and Zod for schema validation. The mock model's `doGenerate` method is set to return a predefined JSON string, ensuring predictable output for testing object generation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/55-testing.mdx#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { generateObject } from 'ai';
import { MockLanguageModelV1 } from 'ai/test';
import { z } from 'zod'; const result = await generateObject({ model: new MockLanguageModelV1({ defaultObjectGenerationMode: 'json', doGenerate: async () => ({ rawCall: { rawPrompt: null, rawSettings: {} }, finishReason: 'stop', usage: { promptTokens: 10, completionTokens: 20 }, text: `{"content":"Hello, world!"}`, }), }), schema: z.object({ content: z.string() }), prompt: 'Hello, test!',
});
``` ---------------------------------------- TITLE: Streaming AI Text as Data Stream in Fastify (TypeScript)
DESCRIPTION: This Fastify endpoint streams AI-generated text as a Vercel AI data stream. It uses `streamText` with OpenAI's `gpt-4o` model and pipes the `toDataStream` result directly to the HTTP response, setting appropriate headers for data streaming.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/40-fastify.mdx#_snippet_1 LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import Fastify from 'fastify'; const fastify = Fastify({ logger: true }); fastify.post('/', async function (request, reply) { const result = streamText({ model: openai('gpt-4o'), prompt: 'Invent a new holiday and describe its traditions.', }); // Mark the response as a v1 data stream: reply.header('X-Vercel-AI-Data-Stream', 'v1'); reply.header('Content-Type', 'text/plain; charset=utf-8'); return reply.send(result.toDataStream({ data }));
}); fastify.listen({ port: 8080 });
``` ---------------------------------------- TITLE: Piping AI SDK Data Stream to Nest.js Response (TypeScript)
DESCRIPTION: This snippet demonstrates how to stream AI-generated text as a data stream using the `pipeDataStreamToResponse` method in a Nest.js controller. It sets up a POST endpoint, calls `streamText` with an OpenAI model, and pipes the result directly to the Express response object.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/50-nest.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { Controller, Post, Res } from '@nestjs/common';
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { Response } from 'express'; @Controller()
export class AppController { @Post() async example(@Res() res: Response) { const result = streamText({ model: openai('gpt-4o'), prompt: 'Invent a new holiday and describe its traditions.', }); result.pipeDataStreamToResponse(res); }
}
``` ---------------------------------------- TITLE: Updating TypeScript Server Action with Zod Schema for Structured Output
DESCRIPTION: This updated TypeScript server action, `explainQuery`, now incorporates the `explanationSchema` to ensure the AI model generates structured output. It uses `output: 'array'` to indicate that an array of objects matching the schema is expected, enhancing the reliability and usability of the generated explanations.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#_snippet_13 LANGUAGE: ts
CODE:
```
// other imports
import { explanationSchema } from '@/lib/types'; /* ...rest of the file... */ export const explainQuery = async (input: string, sqlQuery: string) => { 'use server'; try { const result = await generateObject({ model: openai('gpt-4o'), system: `You are a SQL (postgres) expert. ...`, // SYSTEM PROMPT AS ABOVE - OMITTED FOR BREVITY prompt: `Explain the SQL query you generated to retrieve the data the user wanted. Assume the user is not an expert in SQL. Break down the query into steps. Be concise. User Query: ${input} Generated SQL Query: ${sqlQuery}`, schema: explanationSchema, output: 'array', }); return result.object; } catch (e) { console.error(e); throw new Error('Failed to generate query'); }
};
``` ---------------------------------------- TITLE: Bootstrap Next.js AI Chat Example Project
DESCRIPTION: Commands to initialize a new Next.js project based on the AI SDK OpenAI example using different package managers (npx, yarn, or pnpm). This sets up the project structure and dependencies.
SOURCE: https://github.com/vercel/ai/blob/main/examples/next-openai/README.md#_snippet_0 LANGUAGE: bash
CODE:
```
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app
``` LANGUAGE: bash
CODE:
```
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app
``` LANGUAGE: bash
CODE:
```
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app
``` ---------------------------------------- TITLE: Implement Chat UI with useChat Hook in Next.js
DESCRIPTION: Updates the root page of a Next.js application to use the `@ai-sdk/react` `useChat` hook, displaying messages, handling user input, and submitting chat requests to the API endpoint. It also demonstrates rendering text and reasoning parts of messages.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/20-sonnet-3-7.mdx#_snippet_5 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Page() { const { messages, input, handleInputChange, handleSubmit, error } = useChat(); return ( <> {messages.map(message => ( <div key={message.id}> {message.role === 'user' ? 'User: ' : 'AI: '} {message.parts.map((part, index) => { // text parts: if (part.type === 'text') { return <div key={index}>{part.text}</div>; } // reasoning parts: if (part.type === 'reasoning') { return ( <pre key={index}> {part.details.map(detail => detail.type === 'text' ? detail.text : '<redacted>' )} </pre> ); } })} </div> ))} <form onSubmit={handleSubmit}> <input name="prompt" value={input} onChange={handleInputChange} /> <button type="submit">Submit</button> </form> </> );
}
``` ---------------------------------------- TITLE: Defining Server-Side Tool with Execute Function (TypeScript)
DESCRIPTION: This snippet demonstrates how to define a tool with an `execute` function for server-side execution. The `execute` function handles the logic for the tool, such as fetching weather data, and returns the result directly, satisfying the model's expectation for a tool invocation result. It uses `zod` for parameter validation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/09-troubleshooting/05-tool-invocation-missing-result.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
const tools = { weather: tool({ description: 'Get the weather in a location', parameters: z.object({ location: z .string() .describe('The city and state, e.g. "San Francisco, CA"'), }), execute: async ({ location }) => { // Fetch and return weather data return { temperature: 72, conditions: 'sunny', location }; }, }),
};
``` ---------------------------------------- TITLE: Wrapping Application with AI Context Provider in Next.js (TypeScript)
DESCRIPTION: Demonstrates how to integrate the `AI` context provider into the root layout of a Next.js application. By wrapping the `children` prop with `<AI>`, all components within the application tree gain access to the AI and UI state managed by the AI SDK.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-generative-ui-state.mdx#_snippet_2 LANGUAGE: tsx
CODE:
```
import { type ReactNode } from 'react';
import { AI } from './ai'; export default function RootLayout({ children,
}: Readonly<{ children: ReactNode }>) { return ( <AI> <html lang="en"> <body>{children}</body> </html> </AI> );
}
``` ---------------------------------------- TITLE: Update `createResource` with Embeddings Generation
DESCRIPTION: This updated version of the `createResource` Server Action now includes functionality to generate and store embeddings for the newly created resource content. After inserting the resource, it calls `generateEmbeddings` and then saves the resulting embeddings to the `embeddingsTable` in the database, linking them to the resource ID.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#_snippet_13 LANGUAGE: tsx
CODE:
```
'use server'; import { NewResourceParams, insertResourceSchema, resources,
} from '@/lib/db/schema/resources';
import { db } from '../db';
import { generateEmbeddings } from '../ai/embedding';
import { embeddings as embeddingsTable } from '../db/schema/embeddings'; export const createResource = async (input: NewResourceParams) => { try { const { content } = insertResourceSchema.parse(input); const [resource] = await db .insert(resources) .values({ content }) .returning(); const embeddings = await generateEmbeddings(content); await db.insert(embeddingsTable).values( embeddings.map(embedding => ({ resourceId: resource.id, ...embedding, })), ); return 'Resource successfully created and embedded.'; } catch (error) { return error instanceof Error && error.message.length > 0 ? error.message : 'Error, please try again.'; }
};
``` ---------------------------------------- TITLE: Defining Agent Stopping Conditions with `stopWhen` Parameter in TypeScript
DESCRIPTION: The `stopWhen` parameter allows specifying termination conditions for an AI agent's loop. It supports various conditions like reaching a maximum number of steps, detecting a specific tool call, or satisfying custom criteria such as a maximum token count. This ensures agents operate within defined constraints.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/01-announcing-ai-sdk-5-alpha/index.mdx#_snippet_11 LANGUAGE: TypeScript
CODE:
```
const result = generateText({ // ... // stop loop at 5 steps stopWhen: stepCountIs(5),
}); const result = generateText({ // ... // stop loop when weather tool called stopWhen: hasToolCall('weather'),
}); const result = generateText({ // ... // stop loop at your own custom condition stopWhen: maxTotalTokens(20000),
});
``` ---------------------------------------- TITLE: Using System Messages to Guide AI Behavior (TypeScript)
DESCRIPTION: This example illustrates the use of a 'system' role message to provide initial instructions or context to the AI model before user messages. System messages are crucial for guiding the assistant's overall behavior and setting the tone or purpose for the conversation. An alternative `system` property can also be used for this purpose.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#_snippet_19 LANGUAGE: ts
CODE:
```
const result = await generateText({ model: yourModel, messages: [ { role: 'system', content: 'You help planning travel itineraries.' }, { role: 'user', content: 'I am planning a trip to Berlin for 3 days. Please suggest the best tourist activities for me to do.', } ]
});
``` ---------------------------------------- TITLE: Generating Speech Audio with AI SDK and OpenAI
DESCRIPTION: This snippet demonstrates how to use the `generateSpeech` function from the AI SDK to convert text into speech audio. It imports the necessary functions, initializes an OpenAI speech model (`tts-1`), provides the text to be converted, and then logs the resulting audio object. This example requires `@ai-sdk/openai` for the model and `fs/promises` for potential file handling.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/12-generate-speech.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { experimental_generateSpeech as generateSpeech } from 'ai';
import { openai } from '@ai-sdk/openai';
import { readFile } from 'fs/promises'; const { audio } = await generateSpeech({ model: openai.speech('tts-1'), text: 'Hello from the AI SDK!',
}); console.log(audio);
``` ---------------------------------------- TITLE: Reading AI State in Client Component with useAIState (TypeScript)
DESCRIPTION: Shows how to retrieve the current AI state within a client component using the `useAIState` hook from `ai/rsc`. It accesses the `messages` array (representing the AI's conversation history) and renders each message's `content` property.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-generative-ui-state.mdx#_snippet_4 LANGUAGE: tsx
CODE:
```
'use client'; import { useAIState } from 'ai/rsc'; export default function Page() { const [messages, setMessages] = useAIState(); return ( <ul> {messages.map(message => ( <li key={message.id}>{message.content}</li> ))} </ul> );
}
``` ---------------------------------------- TITLE: Configuring OpenAI API Key in .env.local
DESCRIPTION: This snippet shows how to add the OpenAI API key to the `.env.local` file. Replace `xxxxxxxxx` with your actual OpenAI API key. The AI SDK's OpenAI Provider automatically uses this environment variable for authentication with the OpenAI service.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#_snippet_6 LANGUAGE: Env
CODE:
```
OPENAI_API_KEY=xxxxxxxxx
``` ---------------------------------------- TITLE: Configuring Max Steps for AI Chat in React
DESCRIPTION: This snippet demonstrates how to configure the `maxSteps` option within the `useChat` hook from `@ai-sdk/react`. Setting `maxSteps` to 5 allows the AI model to perform up to five sequential steps for a single generation, enabling more complex, multi-turn interactions and information gathering. This is crucial for scenarios where the model needs to process information over several steps, such as using multiple tools.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/03-nextjs-pages-router.mdx#_snippet_12 LANGUAGE: tsx
CODE:
```
import { useChat } from '@ai-sdk/react'; export default function Chat() { const { messages, input, handleInputChange, handleSubmit } = useChat({ maxSteps: 5, }); // ... rest of your component code
}
``` ---------------------------------------- TITLE: Integrating Tools into AI SDK Chat API Route
DESCRIPTION: This snippet updates the existing chat API route to include the newly defined `tools` object. By passing `tools` to `streamText`, the LLM can now decide to call the `displayWeather` tool based on the conversation context, enabling dynamic UI generation based on model responses.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { tools } from '@/ai/tools'; export async function POST(request: Request) { const { messages } = await request.json(); const result = streamText({ model: openai('gpt-4o'), system: 'You are a friendly assistant!', messages, maxSteps: 5, tools, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Defining a JSON Schema with jsonSchema() in TypeScript
DESCRIPTION: This snippet demonstrates how to define a complex JSON schema using the `jsonSchema` helper function from the AI SDK. It specifies an object type for a recipe, including nested properties for name, ingredients (an array of objects), and steps (an array of strings), along with required fields. This schema can be used for structured data generation or tool definitions.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/25-json-schema.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { jsonSchema } from 'ai'; const mySchema = jsonSchema<{ recipe: { name: string; ingredients: { name: string; amount: string }[]; steps: string[]; };
}>({ type: 'object', properties: { recipe: { type: 'object', properties: { name: { type: 'string' }, ingredients: { type: 'array', items: { type: 'object', properties: { name: { type: 'string' }, amount: { type: 'string' } }, required: ['name', 'amount'] } }, steps: { type: 'array', items: { type: 'string' } } }, required: ['name', 'ingredients', 'steps'] } }, required: ['recipe']
});
``` ---------------------------------------- TITLE: Refine AI Chatbot Prompt with System Instructions in Next.js
DESCRIPTION: This snippet shows how to refine the AI model's behavior by adding `system` instructions to the `streamText` function. The system prompt guides the model to use only retrieved information and respond with "Sorry, I don't know" if no relevant data is found, making the chatbot more controlled.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#_snippet_18 LANGUAGE: tsx
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: openai('gpt-4o'), system: `You are a helpful assistant. Check your knowledge base before answering any questions. Only respond to questions using information from tool calls. if no relevant information is found in the tool calls, respond, "Sorry, I don't know."`, messages, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Generating Text with Chat Prompt using AI SDK and OpenAI (TypeScript)
DESCRIPTION: This snippet demonstrates how to use the `generateText` function from the AI SDK to perform a chat completion. It configures the OpenAI `gpt-3.5-turbo` model, sets a maximum token limit, defines a system prompt, and provides a series of user and assistant messages to simulate a conversation. The generated text from the model is then logged to the console.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/11-generate-text-with-chat-prompt.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai'; const result = await generateText({ model: openai('gpt-3.5-turbo'), maxTokens: 1024, system: 'You are a helpful chatbot.', messages: [ { role: 'user', content: 'Hello!', }, { role: 'assistant', content: 'Hello! How can I help you today?', }, { role: 'user', content: 'I need help with my computer.', }, ],
}); console.log(result.text);
``` ---------------------------------------- TITLE: Implementing Human-in-the-Loop Tool Confirmation in a React Chat UI (TSX)
DESCRIPTION: This React component (`Chat`) uses the `@ai-sdk/react`'s `useChat` hook to build a conversational UI. It integrates custom logic to identify AI tool calls that require human confirmation using `getToolsRequiringConfirmation` and `APPROVAL` values. The UI dynamically renders 'Yes'/'No' buttons for user approval, disabling the chat input while a confirmation is pending, and uses `addToolResult` to send the user's decision back to the AI system.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#_snippet_11 LANGUAGE: tsx
CODE:
```
'use client'; import { Message, useChat } from '@ai-sdk/react';
import { APPROVAL, getToolsRequiringConfirmation,
} from '../api/use-chat-human-in-the-loop/utils';
import { tools } from '../api/use-chat-human-in-the-loop/tools'; export default function Chat() { const { messages, input, handleInputChange, handleSubmit, addToolResult } = useChat({ maxSteps: 5, }); const toolsRequiringConfirmation = getToolsRequiringConfirmation(tools); // used to disable input while confirmation is pending const pendingToolCallConfirmation = messages.some((m: Message) => m.parts?.some( part => part.type === 'tool-invocation' && part.toolInvocation.state === 'call' && toolsRequiringConfirmation.includes(part.toolInvocation.toolName), ), ); return ( <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch"> {messages?.map((m: Message) => ( <div key={m.id} className="whitespace-pre-wrap"> <strong>{`${m.role}: `}</strong> {m.parts?.map((part, i) => { switch (part.type) { case 'text': return <div key={i}>{part.text}</div>; case 'tool-invocation': const toolInvocation = part.toolInvocation; const toolCallId = toolInvocation.toolCallId; const dynamicInfoStyles = 'font-mono bg-gray-100 p-1 text-sm'; // render confirmation tool (client-side tool with user interaction) if ( toolsRequiringConfirmation.includes( toolInvocation.toolName, ) && toolInvocation.state === 'call' ) { return ( <div key={toolCallId} className="text-gray-500"> Run{' '} <span className={dynamicInfoStyles}> {toolInvocation.toolName} </span>{' '} with args:{' '} <span className={dynamicInfoStyles}> {JSON.stringify(toolInvocation.args)} </span> <div className="flex gap-2 pt-2"> <button className="px-4 py-2 font-bold text-white bg-blue-500 rounded hover:bg-blue-700" onClick={() => addToolResult({ toolCallId, result: APPROVAL.YES, }) } > Yes </button> <button className="px-4 py-2 font-bold text-white bg-red-500 rounded hover:bg-red-700" onClick={() => addToolResult({ toolCallId, result: APPROVAL.NO, }) } > No </button> </div> </div> ); } } })} <br /> </div> ))} <form onSubmit={handleSubmit}> <input disabled={pendingToolCallConfirmation} className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl" value={input} placeholder="Say something..." onChange={handleInputChange} /> </form> </div> );
}
``` ---------------------------------------- TITLE: Handling Regular Errors with AI SDK Core (TypeScript)
DESCRIPTION: This snippet demonstrates how to handle non-streaming or synchronous errors using a standard `try/catch` block with the `generateText` function. Errors that occur during the text generation process are thrown and can be caught to implement custom error handling logic.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/50-error-handling.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { generateText } from 'ai'; try { const { text } = await generateText({ model: yourModel, prompt: 'Write a vegetarian lasagna recipe for 4 people.', });
} catch (error) { // handle error
}
``` ---------------------------------------- TITLE: Utilizing OpenAI Web Search Preview Tool (TypeScript)
DESCRIPTION: This snippet demonstrates how to integrate and force the use of the webSearchPreview tool with an OpenAI responses model. It shows how to define the tool with optional configuration like searchContextSize and userLocation, and how to explicitly set toolChoice to ensure the web search tool is used for a given prompt. It also shows how to access the sources from the result.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_22 LANGUAGE: TypeScript
CODE:
```
const result = await generateText({ model: openai.responses('gpt-4o-mini'), prompt: 'What happened in San Francisco last week?', tools: { web_search_preview: openai.tools.webSearchPreview({ // optional configuration: searchContextSize: 'high', userLocation: { type: 'approximate', city: 'San Francisco', region: 'California', }, }), }, // Force web search tool: toolChoice: { type: 'tool', toolName: 'web_search_preview' },
}); // URL sources
const sources = result.sources;
``` ---------------------------------------- TITLE: Handle Chat Errors with `useChat` Hook
DESCRIPTION: The `error` state from the `useChat` hook reflects any error object thrown during a fetch request. This example demonstrates how to use the `error` state to display an error message to the user, such as 'An error occurred.', and provide a retry button using the `reload` function to re-attempt the chat request. It's recommended to show generic error messages to avoid leaking server information.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/02-chatbot.mdx#_snippet_3 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Chat() { const { messages, input, handleInputChange, handleSubmit, error, reload } = useChat({}); return ( <div> {messages.map(m => ( <div key={m.id}> {m.role}: {m.content} </div> ))} {error && ( <> <div>An error occurred.</div> <button type="button" onClick={() => reload()}> Retry </button> </> )} <form onSubmit={handleSubmit}> <input value={input} onChange={handleInputChange} disabled={error != null} /> </form> </div> );
}
``` ---------------------------------------- TITLE: Update Embedding Logic for Knowledge Base Search
DESCRIPTION: This code updates the `lib/ai/embedding.ts` file to enable searching a knowledge base for semantic similarities. It introduces `generateEmbedding` to create a single embedding from an input string and `findRelevantContent` to embed a user query, search the database for similar items, and return relevant content. It leverages the `ai` SDK, OpenAI's embedding model, and `drizzle-orm` for database operations.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#_snippet_23 LANGUAGE: tsx
CODE:
```
import { embed, embedMany } from 'ai';
import { openai } from '@ai-sdk/openai';
import { db } from '../db';
import { cosineDistance, desc, gt, sql } from 'drizzle-orm';
import { embeddings } from '../db/schema/embeddings'; const embeddingModel = openai.embedding('text-embedding-ada-002'); const generateChunks = (input: string): string[] => { return input .trim() .split('.') .filter(i => i !== '');
}; export const generateEmbeddings = async ( value: string,
): Promise<Array<{ embedding: number[]; content: string }>> => { const chunks = generateChunks(value); const { embeddings } = await embedMany({ model: embeddingModel, values: chunks, }); return embeddings.map((e, i) => ({ content: chunks[i], embedding: e }));
}; export const generateEmbedding = async (value: string): Promise<number[]> => { const input = value.replaceAll('\n', ' '); const { embedding } = await embed({ model: embeddingModel, value: input, }); return embedding;
}; export const findRelevantContent = async (userQuery: string) => { const userQueryEmbedded = await generateEmbedding(userQuery); const similarity = sql<number>`1 - (${cosineDistance( embeddings.embedding, userQueryEmbedded, )})`; const similarGuides = await db .select({ name: embeddings.content, similarity }) .from(embeddings) .where(gt(similarity, 0.5)) .orderBy(t => desc(t.similarity)) .limit(4); return similarGuides;
};
``` ---------------------------------------- TITLE: Handle Errors with generateText using Try/Catch (TypeScript)
DESCRIPTION: `generateText` throws errors that can be caught using a standard `try`/`catch` block. This example demonstrates how to differentiate and handle specific error types such as `NoSuchToolError`, `InvalidToolArgumentsError`, and `ToolExecutionError`, along with a general catch-all for other errors.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#_snippet_12 LANGUAGE: ts
CODE:
```
try { const result = await generateText({ //... });
} catch (error) { if (NoSuchToolError.isInstance(error)) { // handle the no such tool error } else if (InvalidToolArgumentsError.isInstance(error)) { // handle the invalid tool arguments error } else if (ToolExecutionError.isInstance(error)) { // handle the tool execution error } else { // handle other errors }
}
``` ---------------------------------------- TITLE: Defining a Zod Schema for Recipe Data in TypeScript
DESCRIPTION: This TypeScript code defines a Zod schema named `recipeSchema` for structuring recipe data. It specifies that a recipe object must contain a name (string), an array of ingredients (each with a name and amount), and an array of steps (strings). This schema can be used by LLMs to generate structured output or validate tool calls, ensuring data adheres to a predefined format.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/04-tools.mdx#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import z from 'zod'; const recipeSchema = z.object({ recipe: z.object({ name: z.string(), ingredients: z.array( z.object({ name: z.string(), amount: z.string(), }), ), steps: z.array(z.string()), }),
});
``` ---------------------------------------- TITLE: Streaming AI Responses with Tools using Vercel AI SDK (TypeScript)
DESCRIPTION: This server-side API route (api/chat.ts) processes incoming chat messages, streams text responses from the 'gpt-4-turbo' model using '@ai-sdk/openai', and defines various tools. It includes a server-executed tool ('getWeatherInformation') and client-side tools ('askForConfirmation', 'getLocation') that can be invoked by the AI model to extend its capabilities.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/90-render-visual-interface-in-chat.mdx#_snippet_3 LANGUAGE: tsx
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { z } from 'zod'; export default async function POST(request: Request) { const { messages } = await request.json(); const result = streamText({ model: openai('gpt-4-turbo'), messages, tools: { // server-side tool with execute function: getWeatherInformation: { description: 'show the weather in a given city to the user', parameters: z.object({ city: z.string() }), execute: async ({}: { city: string }) => { return { value: 24, unit: 'celsius', weeklyForecast: [ { day: 'Monday', value: 24 }, { day: 'Tuesday', value: 25 }, { day: 'Wednesday', value: 26 }, { day: 'Thursday', value: 27 }, { day: 'Friday', value: 28 }, { day: 'Saturday', value: 29 }, { day: 'Sunday', value: 30 } ] }; } }, // client-side tool that starts user interaction: askForConfirmation: { description: 'Ask the user for confirmation.', parameters: z.object({ message: z.string().describe('The message to ask for confirmation.') }) }, // client-side tool that is automatically executed on the client: getLocation: { description: 'Get the user location. Always ask for confirmation before using this tool.', parameters: z.object({}) } } }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Configure maxSteps for Multi-Step Tool Calls in AI SDK
DESCRIPTION: This configuration enables the AI model to automatically send tool call results back to itself for further processing, allowing for follow-up responses after a tool call. It is added to the `useChat` hook's configuration object in a Next.js `app/page.tsx` file to control the number of steps the model can take after a tool call.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#_snippet_22 LANGUAGE: tsx
CODE:
```
// ... Rest of your code const { messages, input, handleInputChange, handleSubmit } = useChat({ maxSteps: 3,
}); // ... Rest of your code
``` ---------------------------------------- TITLE: Generate Text with AI SDK Core and OpenAI
DESCRIPTION: Demonstrates using `generateText` from AI SDK Core with the OpenAI provider in Node.js. It sends a prompt to 'gpt-4o' and logs the response, requiring the `OPENAI_API_KEY` environment variable.
SOURCE: https://github.com/vercel/ai/blob/main/packages/ai/README.md#_snippet_2 LANGUAGE: typescript
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai'; // Ensure OPENAI_API_KEY environment variable is set const { text } = await generateText({ model: openai('gpt-4o'), system: 'You are a friendly assistant!', prompt: 'Why is the sky blue?',
}); console.log(text);
``` ---------------------------------------- TITLE: Defining AI Tools with 'ai' and 'zod' in TypeScript
DESCRIPTION: This snippet defines two AI tools, 'getWeatherInformation' and 'getLocalTime', using the 'ai' library's 'tool' function and 'zod' for parameter validation. 'getWeatherInformation' is designed for human-in-the-loop confirmation (lacks an 'execute' function), while 'getLocalTime' includes an 'execute' function for direct execution without confirmation.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#_snippet_9 LANGUAGE: TypeScript
CODE:
```
import { tool } from 'ai';
import { z } from 'zod'; const getWeatherInformation = tool({ description: 'show the weather in a given city to the user', parameters: z.object({ city: z.string() }), // no execute function, we want human in the loop
}); const getLocalTime = tool({ description: 'get the local time for a specified location', parameters: z.object({ location: z.string() }), // including execute function -> no confirmation required execute: async ({ location }) => { console.log(`Getting local time for ${location}`); return '10am'; },
}); export const tools = { getWeatherInformation, getLocalTime,
};
``` ---------------------------------------- TITLE: AI SDK `useChat` Hook Overview
DESCRIPTION: Provides an overview of the `useChat` hook from the AI SDK, detailing its purpose for creating conversational UIs, its state management capabilities, and its default API interaction.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/01-rag-chatbot.mdx#_snippet_15 LANGUAGE: APIDOC
CODE:
```
useChat Hook: Purpose: Easily create a conversational user interface for chatbot applications. Functionality: - Enables streaming of chat messages from AI provider (e.g., OpenAI). - Manages state for chat input. - Automatically updates UI as new messages are received. Default Behavior: - Sends POST request to `/api/chat` endpoint. - Request body contains `messages`. Customization: - Endpoint can be customized via configuration object. Returned Values (from example usage): - messages: Array of chat messages. - input: Current chat input string. - handleInputChange: Function to handle changes to the input. - handleSubmit: Function to handle form submission.
``` ---------------------------------------- TITLE: Implementing Chat UI with useChat Hook in React
DESCRIPTION: This React component utilizes the `@ai-sdk/react`'s `useChat` hook to manage chat state and interact with a backend API. It captures user input, sends messages to the `/api/chat` endpoint, and displays streamed responses, providing a real-time chat experience.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/21-stream-text-with-chat-prompt.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Page() { const { messages, input, setInput, append } = useChat(); return ( <div> <input value={input} onChange={event => { setInput(event.target.value); }} onKeyDown={async event => { if (event.key === 'Enter') { append({ content: input, role: 'user' }); } }} /> {messages.map((message, index) => ( <div key={index}>{message.content}</div> ))} </div> );
}
``` ---------------------------------------- TITLE: Sending Messages with useChat Hook and Shared ID (AI SDK React - After)
DESCRIPTION: This snippet demonstrates the updated method for sending messages from a client component using the `useChat` hook from `@ai-sdk/react`. It highlights the importance of initializing `useChat` with a shared `chatId` to synchronize messages with the parent component and uses `append` to send user content.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#_snippet_8 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export function ListFlights({ chatId, flights }) { const { append } = useChat({ id: chatId, body: { id: chatId }, maxSteps: 5, }); return ( <div> {flights.map(flight => ( <div key={flight.id} onClick={async () => { await append({ role: 'user', content: `I would like to choose flight ${flight.id}!`, }); }} > {flight.name} </div> ))} </div> );
}
``` ---------------------------------------- TITLE: Using Computer Tools with AI SDK's generateText
DESCRIPTION: This example demonstrates how to use the previously defined `computerTool` with the AI SDK's `generateText` function for a single-turn text generation. The `tools` option links the `computerTool` to the model, allowing the AI to invoke computer actions based on the prompt.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/05-computer-use.mdx#_snippet_2 LANGUAGE: TypeScript
CODE:
```
const result = await generateText({ model: anthropic('claude-3-5-sonnet-20241022'), prompt: 'Move the cursor to the center of the screen and take a screenshot', tools: { computer: computerTool }
}); console.log(response.text);
``` ---------------------------------------- TITLE: Processing Stream Chunks with `onChunk` Callback in AI SDK Core `streamText` (TypeScript)
DESCRIPTION: This snippet illustrates the use of the `onChunk` callback in `streamText` to process individual data chunks as they arrive. The callback is triggered for various chunk types like `text-delta`, `reasoning`, `source`, `tool-call`, and `tool-result`, enabling real-time processing or UI updates based on the stream's content.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#_snippet_5 LANGUAGE: TypeScript
CODE:
```
import { streamText } from 'ai'; const result = streamText({ model: yourModel, prompt: 'Invent a new holiday and describe its traditions.', onChunk({ chunk }) { // implement your own logic here, e.g.: if (chunk.type === 'text-delta') { console.log(chunk.text); } },
});
``` ---------------------------------------- TITLE: Restoring UI State with onGetUIState Callback in TypeScript
DESCRIPTION: This snippet demonstrates restoring the UI state by leveraging the `onGetUIState` callback, which is invoked during Server-Side Rendering (SSR). It compares the chat history from the database with the current AI state in the app. If they differ, it reconstructs and returns the UI state based on the database history, effectively synchronizing the UI with the persisted AI state.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-saving-and-restoring-states.mdx#_snippet_2 LANGUAGE: TypeScript
CODE:
```
export const AI = createAI<ServerMessage[], ClientMessage[]> ({ actions: { continueConversation, }, onGetUIState: async () => { 'use server'; const historyFromDB: ServerMessage[] = await loadChatFromDB(); const historyFromApp: ServerMessage[] = getAIState(); // If the history from the database is different from the // history in the app, they're not in sync so return the UIState // based on the history from the database if (historyFromDB.length !== historyFromApp.length) { return historyFromDB.map(({ role, content }) => ({ id: generateId(), role, display: role === 'function' ? ( <Component {...JSON.parse(content)} /> ) : ( content ), })); } },
});
``` ---------------------------------------- TITLE: Implementing Client Chat Interface with AI SDK and React
DESCRIPTION: This React client component creates a chat interface allowing users to send messages and receive streamed responses from an AI assistant. It manages input state, displays messages, and uses `ai/rsc`'s `useActions` hook to submit messages and update the UI with the assistant's responses.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/120-stream-assistant-response.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
'use client'; import { useState } from 'react';
import { ClientMessage } from './actions';
import { useActions } from 'ai/rsc'; export default function Home() { const [input, setInput] = useState(''); const [messages, setMessages] = useState<ClientMessage[]>([]); const { submitMessage } = useActions(); const handleSubmission = async () => { setMessages(currentMessages => [ ...currentMessages, { id: '123', status: 'user.message.created', text: input, gui: null, }, ]); const response = await submitMessage(input); setMessages(currentMessages => [...currentMessages, response]); setInput(''); }; return ( <div className="flex flex-col-reverse"> <div className="flex flex-row gap-2 p-2 bg-zinc-100 w-full"> <input className="bg-zinc-100 w-full p-2 outline-none" value={input} onChange={event => setInput(event.target.value)} placeholder="Ask a question" onKeyDown={event => { if (event.key === 'Enter') { handleSubmission(); } }} /> <button className="p-2 bg-zinc-900 text-zinc-100 rounded-md" onClick={handleSubmission} > Send </button> </div> <div className="flex flex-col h-[calc(100dvh-56px)] overflow-y-scroll"> <div> {messages.map(message => ( <div key={message.id} className="flex flex-col gap-1 border-b p-2"> <div className="flex flex-row justify-between"> <div className="text-sm text-zinc-500">{message.status}</div> </div> <div>{message.text}</div> </div> ))} </div> </div> </div> );
}
``` ---------------------------------------- TITLE: Implementing Chat UI with AI SDK's useChat Hook in Next.js
DESCRIPTION: This snippet demonstrates how to set up a basic chat interface in a Next.js application using the `@ai-sdk/react`'s `useChat` hook. It shows how to display chat messages, handle user input, and submit messages to a backend API, leveraging the `messages`, `input`, `handleInputChange`, and `handleSubmit` properties provided by the hook.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/02-nextjs-app-router.mdx#_snippet_8 LANGUAGE: tsx
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Chat() { const { messages, input, handleInputChange, handleSubmit } = useChat(); return ( <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch"> {messages.map(message => ( <div key={message.id} className="whitespace-pre-wrap"> {message.role === 'user' ? 'User: ' : 'AI: '} {message.parts.map((part, i) => { switch (part.type) { case 'text': return <div key={`${message.id}-${i}`}>{part.text}</div>; } })} </div> ))} <form onSubmit={handleSubmit}> <input className="fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl" value={input} placeholder="Say something..." onChange={handleInputChange} /> </form> </div> );
}
``` ---------------------------------------- TITLE: Wrapping Application with AI Provider (React, TSX)
DESCRIPTION: This layout component wraps the application's children with the `AI` component, making the AI context and actions available to all nested components. This is a crucial step for integrating the Vercel AI SDK into a React application.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/120-stream-assistant-response.mdx#_snippet_4 LANGUAGE: TSX
CODE:
```
import { ReactNode } from 'react';
import { AI } from './ai'; export default function Layout({ children }: { children: ReactNode }) { return <AI>{children}</AI>;
}
``` ---------------------------------------- TITLE: Consuming Streamable Value in React Component (TypeScript)
DESCRIPTION: This example shows how to use the `useStreamableValue` hook within a React functional component to consume a streamable value passed via props. It effectively manages and displays the loading, error, and final data states of the asynchronous stream, providing a clear user interface based on the stream's progress.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/03-ai-sdk-rsc/11-use-streamable-value.mdx#_snippet_1 LANGUAGE: TypeScript
CODE:
```
function MyComponent({ streamableValue }) { const [data, error, pending] = useStreamableValue(streamableValue); if (pending) return <div>Loading...</div>; if (error) return <div>Error: {error.message}</div>; return <div>Data: {data}</div>;
}
``` ---------------------------------------- TITLE: Notifying on Each Completed Step in AI SDK
DESCRIPTION: This snippet illustrates the use of the 'onStepFinish' callback within 'generateText'. This callback is triggered once a step is fully completed, providing access to the step's text, tool calls, tool results, finish reason, and usage, enabling custom logic like chat history logging or usage tracking.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#_snippet_8 LANGUAGE: TypeScript
CODE:
```
import { generateText } from 'ai'; const result = await generateText({ model: yourModel, maxSteps: 10, onStepFinish({ text, toolCalls, toolResults, finishReason, usage }) { // your own logic, e.g. for saving the chat history or recording usage }, // ...
});
``` ---------------------------------------- TITLE: Initializing AI SDK Chat with Max Steps (Svelte)
DESCRIPTION: This Svelte snippet initializes the @ai-sdk/svelte Chat class, setting the maxSteps option to 5. This configuration allows the AI model to perform up to 5 sequential steps for a single generation, enabling more complex, multi-turn interactions and information gathering. It's a crucial setting for scenarios requiring chained tool calls.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#_snippet_11 LANGUAGE: svelte
CODE:
```
<script> import { Chat } from '@ai-sdk/svelte'; const chat = new Chat({ maxSteps: 5 });
</script> <!-- ... rest of your component code -->
``` ---------------------------------------- TITLE: Defining a Weather Tool with AI SDK and Zod
DESCRIPTION: This snippet creates a `weatherTool` using `ai`'s `createTool` function. It defines parameters for location using `zod` and simulates fetching weather data after a delay. This tool is designed for the LLM to call when specific weather information is requested, demonstrating how to create custom actions for generative UI.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { tool as createTool } from 'ai';
import { z } from 'zod'; export const weatherTool = createTool({ description: 'Display the weather for a location', parameters: z.object({ location: z.string().describe('The location to get the weather for'), }), execute: async function ({ location }) { await new Promise(resolve => setTimeout(resolve, 2000)); return { weather: 'Sunny', temperature: 75, location }; },
}); export const tools = { displayWeather: weatherTool,
};
``` ---------------------------------------- TITLE: Streaming React UI Components with AI SDK RSC in TypeScript
DESCRIPTION: This server action illustrates how to stream React components from the server to the client using `createStreamableUI`. It initializes a stream with a loading state, then updates it with the final UI component after a simulated delay, allowing for dynamic and progressive rendering of UI elements.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/05-streaming-values.mdx#_snippet_2 LANGUAGE: TypeScript
CODE:
```
'use server'; import { createStreamableUI } from 'ai/rsc'; export async function getWeather() { const weatherUI = createStreamableUI(); weatherUI.update(<div style={{ color: 'gray' }}>Loading...</div>); setTimeout(() => { weatherUI.done(<div>It&apos;s a sunny day!</div>); }, 1000); return weatherUI.value;
}
``` ---------------------------------------- TITLE: Displaying Flights and Triggering Actions in Vercel AI UI (TSX)
DESCRIPTION: This client component displays a list of flights. When a flight number is clicked, it uses the `useActions` hook to call `submitUserMessage` with a `lookupFlight` command, then updates the UI state via `setMessages` with the returned React component. It depends on `ai/rsc` hooks and `ReactNode`.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/04-multistep-interfaces.mdx#_snippet_4 LANGUAGE: TSX
CODE:
```
'use client'; import { useActions, useUIState } from 'ai/rsc';
import { ReactNode } from 'react'; interface FlightsProps { flights: { id: string; flightNumber: string }[];
} export const Flights = ({ flights }: FlightsProps) => { const { submitUserMessage } = useActions(); const [_, setMessages] = useUIState(); return ( <div> {flights.map(result => ( <div onClick={async () => { const display = await submitUserMessage( `lookupFlight ${result.flightNumber}`, ); setMessages((messages: ReactNode[]) => [...messages, display]); }} > {result.flightNumber} </div> </div> ))} </div> );
};
``` ---------------------------------------- TITLE: Implementing Client-Side Chat Interface with AI SDK (TypeScript/React)
DESCRIPTION: This React client component manages the chat conversation state and handles user input. It uses `useUIState` and `useActions` from `ai/rsc` to interact with server actions, displaying messages and sending user input to the AI model. It also sets `maxDuration` for streaming responses, allowing up to 30 seconds.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/90-render-visual-interface-in-chat.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
'use client'; import { useState } from 'react';
import { ClientMessage } from './actions';
import { useActions, useUIState } from 'ai/rsc';
import { generateId } from 'ai'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export default function Home() { const [input, setInput] = useState<string>(''); const [conversation, setConversation] = useUIState(); const { continueConversation } = useActions(); return ( <div> <div> {conversation.map((message: ClientMessage) => ( <div key={message.id}> {message.role}: {message.display} </div> ))} </div> <div> <input type="text" value={input} onChange={event => { setInput(event.target.value); }} /> <button onClick={async () => { setConversation((currentConversation: ClientMessage[]) => [ ...currentConversation, { id: generateId(), role: 'user', display: input }, ]); const message = await continueConversation(input); setConversation((currentConversation: ClientMessage[]) => [ ...currentConversation, message, ]); }} > Send Message </button> </div> </div> );
}
``` ---------------------------------------- TITLE: Generate Text Using Google Vertex AI Model
DESCRIPTION: Shows how to use a Google Vertex AI language model with the `generateText` function from the AI SDK. This example demonstrates importing necessary modules, instantiating the model, and generating a text response based on a given prompt.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#_snippet_12 LANGUAGE: ts
CODE:
```
import { vertex } from '@ai-sdk/google-vertex';
import { generateText } from 'ai'; const { text } = await generateText({ model: vertex('gemini-1.5-pro'), prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
``` ---------------------------------------- TITLE: Client-Side Text Streaming with useCompletion Hook (React/Next.js)
DESCRIPTION: This React component demonstrates how to use the `useCompletion` hook from `@ai-sdk/react` to initiate text generation and display streamed results. When the 'Generate' div is clicked, it calls the `/api/completion` endpoint with a predefined prompt, and the `completion` state variable updates in real-time as text streams from the server.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/20-stream-text.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
'use client'; import { useCompletion } from '@ai-sdk/react'; export default function Page() { const { completion, complete } = useCompletion({ api: '/api/completion', }); return ( <div> <div onClick={async () => { await complete('Why is the sky blue?'); }} > Generate </div> {completion} </div> );
}
``` ---------------------------------------- TITLE: Implementing Basic Chat Interface with AI SDK React
DESCRIPTION: This snippet demonstrates a basic client-side chat interface using the `useChat` hook from `@ai-sdk/react`. It displays messages from both user and AI and provides an input field for sending new messages, forming the foundation for a generative UI chat application.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#_snippet_0 LANGUAGE: TSX
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react'; export default function Page() { const { messages, input, handleInputChange, handleSubmit } = useChat(); return ( <div> {messages.map(message => ( <div key={message.id}> <div>{message.role === 'user' ? 'User: ' : 'AI: '}</div> <div>{message.content}</div> </div> ))} <form onSubmit={handleSubmit}> <input value={input} onChange={handleInputChange} placeholder="Type a message..." /> <button type="submit">Send</button> </form> </div> );
}
``` ---------------------------------------- TITLE: Managing Conversation State and User Input in Next.js Client Component (TSX)
DESCRIPTION: This client-side component handles user input and displays the ongoing conversation. It leverages `useState` for managing the input field, `useUIState` from `ai/rsc` to display and update the conversation, and `useActions` to invoke server actions for continuing the AI conversation.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/60-save-messages-to-database.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
'use client'; import { useState } from 'react';
import { ClientMessage } from './actions';
import { useActions, useUIState } from 'ai/rsc';
import { generateId } from 'ai'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export default function Home() { const [input, setInput] = useState<string>(''); const [conversation, setConversation] = useUIState(); const { continueConversation } = useActions(); return ( <div> <div> {conversation.map((message: ClientMessage) => ( <div key={message.id}> {message.role}: {message.display} </div> ))} </div> <div> <input type="text" value={input} onChange={event => { setInput(event.target.value); }} /> <button onClick={async () => { setConversation((currentConversation: ClientMessage[]) => [ ...currentConversation, { id: generateId(), role: 'user', display: input }, ]); const message = await continueConversation(input); setConversation((currentConversation: ClientMessage[]) => [ ...currentConversation, message, ]); }} > Send Message </button> </div> </div> );
}
``` ---------------------------------------- TITLE: Next.js Server Action for Streaming AI Responses
DESCRIPTION: This server-side function (`continueConversation`) is responsible for processing chat history and streaming AI model responses. It utilizes `createStreamableValue` from `ai/rsc` to create a stream, then uses `streamText` with the OpenAI model to generate text, updating the streamable value incrementally as new text becomes available. This allows the client to receive and display the AI's response in real-time.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/21-stream-text-with-chat-prompt.mdx#_snippet_1 LANGUAGE: typescript
CODE:
```
'use server'; import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { createStreamableValue } from 'ai/rsc'; export interface Message { role: 'user' | 'assistant'; content: string;
} export async function continueConversation(history: Message[]) { 'use server'; const stream = createStreamableValue(); (async () => { const { textStream } = streamText({ model: openai('gpt-3.5-turbo'), system: "You are a dude that doesn't drop character until the DVD commentary.", messages: history, }); for await (const text of textStream) { stream.update(text); } stream.done(); })(); return { messages: history, newMessage: stream.value, };
}
``` ---------------------------------------- TITLE: Implementing Server-Side Multi-Step Tool Calls with AI SDK
DESCRIPTION: This server-side API route demonstrates how to define and execute multi-step tool calls using `streamText`. It includes a `getWeatherInformation` tool with a `zod` schema for parameters and an `execute` function, allowing the AI model to perform actions on the server. `maxSteps` limits the number of tool execution steps.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-tool-usage.mdx#_snippet_5 LANGUAGE: tsx
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { z } from 'zod'; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: openai('gpt-4o'), messages, tools: { getWeatherInformation: { description: 'show the weather in a given city to the user', parameters: z.object({ city: z.string() }), // tool has execute function: execute: async ({}: { city: string }) => { const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy']; return weatherOptions[ Math.floor(Math.random() * weatherOptions.length) ]; }, }, }, maxSteps: 5, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Implementing AI Model Caching Middleware with Upstash Redis (TypeScript)
DESCRIPTION: This TypeScript snippet defines `cacheMiddleware`, a `LanguageModelV1Middleware` that caches AI model responses using Upstash Redis. It implements `wrapGenerate` for direct response caching and `wrapStream` for caching stream parts, which are then replayed using `simulateReadableStream` to mimic live streaming from cache.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/122-caching-middleware.mdx#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { Redis } from '@upstash/redis';
import { type LanguageModelV1, type LanguageModelV1Middleware, type LanguageModelV1StreamPart, simulateReadableStream,
} from 'ai'; const redis = new Redis({ url: process.env.KV_URL, token: process.env.KV_TOKEN,
}); export const cacheMiddleware: LanguageModelV1Middleware = { wrapGenerate: async ({ doGenerate, params }) => { const cacheKey = JSON.stringify(params); const cached = (await redis.get(cacheKey)) as Awaited< ReturnType<LanguageModelV1['doGenerate']> > | null; if (cached !== null) { return { ...cached, response: { ...cached.response, timestamp: cached?.response?.timestamp ? new Date(cached?.response?.timestamp) : undefined, }, }; } const result = await doGenerate(); redis.set(cacheKey, result); return result; }, wrapStream: async ({ doStream, params }) => { const cacheKey = JSON.stringify(params); // Check if the result is in the cache const cached = await redis.get(cacheKey); // If cached, return a simulated ReadableStream that yields the cached result if (cached !== null) { // Format the timestamps in the cached response const formattedChunks = (cached as LanguageModelV1StreamPart[]).map(p => { if (p.type === 'response-metadata' && p.timestamp) { return { ...p, timestamp: new Date(p.timestamp) }; } else return p; }); return { stream: simulateReadableStream({ initialDelayInMs: 0, chunkDelayInMs: 10, chunks: formattedChunks, }), rawCall: { rawPrompt: null, rawSettings: {} }, }; } // If not cached, proceed with streaming const { stream, ...rest } = await doStream(); const fullResponse: LanguageModelV1StreamPart[] = []; const transformStream = new TransformStream< LanguageModelV1StreamPart, LanguageModelV1StreamPart >({ transform(chunk, controller) { fullResponse.push(chunk); controller.enqueue(chunk); }, flush() { // Store the full response in the cache after streaming is complete redis.set(cacheKey, fullResponse); }, }); return { stream: stream.pipeThrough(transformStream), ...rest, }; }
}
``` ---------------------------------------- TITLE: Implement Tool Calling with AI SDK for External Interactions
DESCRIPTION: This example illustrates how to integrate tool calling into AI SDK applications using `generateText` and the `tool` function. It defines a `getWeather` tool with parameters and an execution logic, enabling the AI model to interact with external systems (simulated here) to fetch dynamic information.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/24-o3.mdx#_snippet_3 LANGUAGE: tsx
CODE:
```
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai'; const { text } = await generateText({ model: openai('o3-mini'), prompt: 'What is the weather like today in San Francisco?', tools: { getWeather: tool({ description: 'Get the weather in a location', parameters: z.object({ location: z.string().describe('The location to get the weather for') }), execute: async ({ location }) => ({ location, temperature: 72 + Math.floor(Math.random() * 21) - 10 }) }) }
});
``` ---------------------------------------- TITLE: Implementing Server-Side Tool Calling with AI SDK and OpenAI
DESCRIPTION: This server action defines the `continueConversation` function, which uses the AI SDK's `generateText` to interact with an OpenAI model. It includes a custom tool, `celsiusToFahrenheit`, with Zod-defined parameters, enabling the model to perform specific conversions and extend its capabilities beyond basic text generation.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/50-call-tools.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
'use server'; import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod'; export interface Message { role: 'user' | 'assistant'; content: string;
} export async function continueConversation(history: Message[]) { 'use server'; const { text, toolResults } = await generateText({ model: openai('gpt-3.5-turbo'), system: 'You are a friendly assistant!', messages: history, tools: { celsiusToFahrenheit: { description: 'Converts celsius to fahrenheit', parameters: z.object({ value: z.string().describe('The value in celsius') }), execute: async ({ value }) => { const celsius = parseFloat(value); const fahrenheit = celsius * (9 / 5) + 32; return `${celsius}°C is ${fahrenheit.toFixed(2)}°F`; } } } }); return { messages: [ ...history, { role: 'assistant' as const, content: text || toolResults.map(toolResult => toolResult.result).join('\n') } ] };
}
``` ---------------------------------------- TITLE: Initializing Vercel AI SDK with Actions (TypeScript)
DESCRIPTION: This snippet initializes the Vercel AI SDK using 'createAI'. It configures the AI instance by linking the 'continueConversation' action defined in 'app/actions.tsx' and setting up initial empty states for both AI and UI. This setup is crucial for enabling the conversational flow.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/90-render-visual-interface-in-chat.mdx#_snippet_4 LANGUAGE: TypeScript
CODE:
```
import { createAI } from 'ai/rsc';
import { ServerMessage, ClientMessage, continueConversation } from './actions'; export const AI = createAI<ServerMessage[], ClientMessage[]> ({ actions: { continueConversation, }, initialAIState: [], initialUIState: [],
});
``` ---------------------------------------- TITLE: Configure Custom Google Vertex Provider for Edge Runtime
DESCRIPTION: This example shows how to create a custom Google Vertex provider instance tailored for Edge runtimes. It uses the `googleCredentials` option for authentication, which takes precedence over environment variables, enabling secure credential management in serverless environments.
SOURCE: https://github.com/vercel/ai/blob/main/packages/google-vertex/README.md#_snippet_7 LANGUAGE: ts
CODE:
```
import { createVertex } from '@ai-sdk/google-vertex/edge';
import { generateText } from 'ai'; const customProvider = createVertex({ project: 'your-project-id', location: 'us-central1', googleCredentials: { clientEmail: 'your-client-email', privateKey: 'your-private-key', },
}); const { text } = await generateText({ model: customProvider('gemini-1.5-flash'), prompt: 'Write a vegetarian lasagna recipe.',
});
``` ---------------------------------------- TITLE: Initialize and Implement Vercel AI Computer Tool Logic (TypeScript)
DESCRIPTION: This TypeScript code demonstrates how to initialize the `computer_20241022` tool from `vertexAnthropic.tools`. It shows the configuration parameters like display dimensions and an `execute` callback function where custom computer control logic can be implemented, including handling actions like 'screenshot' and returning results.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#_snippet_47 LANGUAGE: ts
CODE:
```
const computerTool = vertexAnthropic.tools.computer_20241022({ displayWidthPx: 1920, displayHeightPx: 1080, displayNumber: 0, // Optional, for X11 environments execute: async ({ action, coordinate, text }) => { // Implement your computer control logic here // Return the result of the action // Example code: switch (action) { case 'screenshot': { // multipart result: return { type: 'image', data: fs .readFileSync('./data/screenshot-editor.png') .toString('base64'), }; } default: { console.log('Action:', action); console.log('Coordinate:', coordinate); console.log('Text:', text); return `executed ${action}`; } } }, // map to tool result content for LLM consumption: experimental_toToolResultContent(result) { return typeof result === 'string' ? [{ type: 'text', text: result }] : [{ type: 'image', data: result.data, mimeType: 'image/png' }]; }
});
``` ---------------------------------------- TITLE: Enable Prompt Caching with Google Vertex Anthropic
DESCRIPTION: This example demonstrates how to enable prompt caching for Google Vertex Anthropic models using the `cacheControl` property. It shows both global caching for the model and ephemeral caching for specific message parts, improving performance for repeated prompts.
SOURCE: https://github.com/vercel/ai/blob/main/packages/google-vertex/README.md#_snippet_5 LANGUAGE: ts
CODE:
```
import { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic';
import { generateText } from 'ai';
import fs from 'node:fs'; const errorMessage = fs.readFileSync('data/error-message.txt', 'utf8'); async function main() { const result = await generateText({ model: vertexAnthropic('claude-3-5-sonnet-v2@20241022', { cacheControl: true, }), messages: [ { role: 'user', content: [ { type: 'text', text: 'You are a JavaScript expert.', }, { type: 'text', text: `Error message: ${errorMessage}`, providerOptions: { anthropic: { cacheControl: { type: 'ephemeral' }, }, }, }, { type: 'text', text: 'Explain the error message.', }, ], }, ], }); console.log(result.text); console.log(result.experimental_providerMetadata?.anthropic); // e.g. { cacheCreationInputTokens: 2118, cacheReadInputTokens: 0 }
} main().catch(console.error);
``` ---------------------------------------- TITLE: Initializing OpenAI Embedding Model with Options (TypeScript)
DESCRIPTION: This example shows how to initialize an OpenAI embedding model, 'text-embedding-3-large', with optional configuration parameters. It includes `dimensions` to specify the output embedding size and `user` for identifying the end-user, enhancing abuse monitoring and detection by OpenAI.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_31 LANGUAGE: TypeScript
CODE:
```
const model = openai.embedding('text-embedding-3-large', { dimensions: 512 // optional, number of dimensions for the embedding user: 'test-user' // optional unique user identifier
})
``` ---------------------------------------- TITLE: Importing Default OpenAI Provider Instance
DESCRIPTION: This snippet imports the default `openai` provider instance from the `@ai-sdk/openai` module. This instance is pre-configured and ready for immediate use with OpenAI APIs.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_3 LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
``` ---------------------------------------- TITLE: Initializing and Using MCP Clients with AI SDK in TypeScript
DESCRIPTION: This snippet demonstrates how to initialize Model Context Protocol (MCP) clients using `experimental_createMCPClient` with different transport types (stdio, SSE, and custom). It shows how to retrieve tool sets from these clients, combine them, and then use the combined tools with the `generateText` function from the AI SDK and an OpenAI model. The example also includes error handling and ensures clients are closed in the `finally` block to release resources.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/54-mcp-tools.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { experimental_createMCPClient, generateText } from 'ai';
import { Experimental_StdioMCPTransport } from 'ai/mcp-stdio';
import { openai } from '@ai-sdk/openai'; let clientOne;
let clientTwo;
let clientThree; try { // Initialize an MCP client to connect to a `stdio` MCP server: const transport = new Experimental_StdioMCPTransport({ command: 'node', args: ['src/stdio/dist/server.js'] }); clientOne = await experimental_createMCPClient({ transport, }); // Alternatively, you can connect to a Server-Sent Events (SSE) MCP server: clientTwo = await experimental_createMCPClient({ transport: { type: 'sse', url: 'http://localhost:3000/sse', }, }); // Similarly to the stdio example, you can pass in your own custom transport as long as it implements the `MCPTransport` interface: const transport = new MyCustomTransport({ // ... }); clientThree = await experimental_createMCPClient({ transport, }); const toolSetOne = await clientOne.tools(); const toolSetTwo = await clientTwo.tools(); const toolSetThree = await clientThree.tools(); const tools = { ...toolSetOne, ...toolSetTwo, ...toolSetThree, // note: this approach causes subsequent tool sets to override tools with the same name }; const response = await generateText({ model: openai('gpt-4o'), tools, messages: [ { role: 'user', content: 'Find products under $100', }, ], }); console.log(response.text);
} catch (error) { console.error(error);
} finally { await Promise.all([clientOne.close(), clientTwo.close()]);
}
``` ---------------------------------------- TITLE: Basic streamUI Call for Text Streaming
DESCRIPTION: This example demonstrates a basic usage of the `streamUI` function from the AI SDK RSC API. It calls OpenAI's `gpt-4o` model with a specific prompt. The `text` property defines how the model's plain text response should be rendered as a React component, in this case, wrapped in a `div`. Although no tools are defined, the response will still be streamed as a React component.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/02-streaming-react-components.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
const result = await streamUI({ model: openai('gpt-4o'), prompt: 'Get the weather for San Francisco', text: ({ content }) => <div>{content}</div>, tools: {},
});
``` ---------------------------------------- TITLE: Client-side Object Generation with useObject (TSX)
DESCRIPTION: This client-side code demonstrates the use of the `experimental_useObject` hook from `@ai-sdk/react` to initiate and stream structured JSON object generation. It connects to the `/api/notifications` endpoint, uses the predefined `notificationSchema`, and renders partial results as they are received, handling potentially `undefined` values during streaming.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/08-object-generation.mdx#_snippet_1 LANGUAGE: TSX
CODE:
```
'use client'; import { experimental_useObject as useObject } from '@ai-sdk/react';
import { notificationSchema } from './api/notifications/schema'; export default function Page() { const { object, submit } = useObject({ api: '/api/notifications', schema: notificationSchema, }); return ( <> <button onClick={() => submit('Messages during finals week.')}> Generate notifications </button> {object?.notifications?.map((notification, index) => ( <div key={index}> <p>{notification?.name}</p> <p>{notification?.message}</p> </div> ))} </> );
}
``` ---------------------------------------- TITLE: Rendering Streamable UI with React and AI SDK (Client-side)
DESCRIPTION: This client-side React component demonstrates how to call a server action (`getWeather`) and render its streamable UI output. It uses `useState` to manage the UI content and imports `readStreamableValue` (though not directly invoked in this snippet, it's part of the `ai/rsc` context). The UI updates dynamically when the button is clicked, showing a loading message followed by the actual weather information.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/05-streaming-values.mdx#_snippet_3 LANGUAGE: tsx
CODE:
```
'use client'; import { useState } from 'react';
import { readStreamableValue } from 'ai/rsc';
import { getWeather } from '@/actions'; export default function Page() { const [weather, setWeather] = useState<React.ReactNode | null>(null); return ( <div> <button onClick={async () => { const weatherUI = await getWeather(); setWeather(weatherUI); }} > What&apos;s the weather? </button> {weather} </div> );
}
``` ---------------------------------------- TITLE: Updating AI State in Server Action with getMutableAIState (TypeScript)
DESCRIPTION: Illustrates how to modify the AI state from within a server action using `getMutableAIState`. This function provides methods (`.update()` and `.done()`) to append new messages (user input and AI responses) to the conversation history, ensuring state synchronization.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-generative-ui-state.mdx#_snippet_6 LANGUAGE: ts
CODE:
```
import { getMutableAIState } from 'ai/rsc'; export async function sendMessage(message: string) { 'use server'; const history = getMutableAIState(); // Update the AI state with the new user message. history.update([...history.get(), { role: 'user', content: message }]); const response = await generateText({ model: openai('gpt-3.5-turbo'), messages: history.get(), }); // Update the AI state again with the response from the model. history.done([...history.get(), { role: 'assistant', content: response }]); return response;
}
``` ---------------------------------------- TITLE: Generating Structured Output with generateText in TypeScript
DESCRIPTION: This snippet demonstrates how to use `generateText` with the `experimental_output` setting to define a structured output schema using `z.object`. It specifies the expected data shape for generating an example person, including nested objects and nullable fields, for a single, complete output.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/10-generating-structured-data.mdx#_snippet_10 LANGUAGE: TypeScript
CODE:
```
// experimental_output is a structured object that matches the schema:
const { experimental_output } = await generateText({ // ... experimental_output: Output.object({ schema: z.object({ name: z.string(), age: z.number().nullable().describe('Age of the person.'), contact: z.object({ type: z.literal('email'), value: z.string(), }), occupation: z.object({ type: z.literal('employed'), company: z.string(), position: z.string(), }), }), }), prompt: 'Generate an example person for testing.',
});
``` ---------------------------------------- TITLE: Combining Multiple Anthropic Computer Use Tools in AI SDK (TypeScript)
DESCRIPTION: This snippet demonstrates how to initialize and combine multiple Anthropic Computer Use tools (computer, bash, text editor) within a single `generateText` request using the AI SDK. It shows how to define custom execution logic for tools like `bashTool` and `textEditorTool` and then pass them to the `tools` parameter of the `generateText` function, enabling complex multi-step operations.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/05-computer-use.mdx#_snippet_5 LANGUAGE: TypeScript
CODE:
```
const computerTool = anthropic.tools.computer_20241022({ ...
}); const bashTool = anthropic.tools.bash_20241022({ execute: async ({ command, restart }) => execSync(command).toString()
}); const textEditorTool = anthropic.tools.textEditor_20241022({ execute: async ({ command, path, file_text, insert_line, new_str, old_str, view_range }) => { // Handle file operations based on command switch(command) { return executeTextEditorFunction({ command, path, fileText: file_text, insertLine: insert_line, newStr: new_str, oldStr: old_str, viewRange: view_range }); } }
}); const response = await generateText({ model: anthropic("claude-3-5-sonnet-20241022"), prompt: "Create a new file called example.txt, write 'Hello World' to it, and run 'cat example.txt' in the terminal", tools: { computer: computerTool, bash: bashTool, str_replace_editor: textEditorTool }
});
``` ---------------------------------------- TITLE: Testing generateText with MockLanguageModelV1 in TypeScript
DESCRIPTION: This snippet demonstrates how to unit test the `generateText` function from the AI SDK using `MockLanguageModelV1`. It configures the mock model's `doGenerate` method to return a predefined text output, allowing for deterministic testing of text generation without making actual API calls.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/55-testing.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { generateText } from 'ai';
import { MockLanguageModelV1 } from 'ai/test'; const result = await generateText({ model: new MockLanguageModelV1({ doGenerate: async () => ({ rawCall: { rawPrompt: null, rawSettings: {} }, finishReason: 'stop', usage: { promptTokens: 10, completionTokens: 20 }, text: `Hello, world!`, }), }), prompt: 'Hello, test!',
});
``` ---------------------------------------- TITLE: Defining useAutoResume Hook for Resilient Stream Handling
DESCRIPTION: This custom React hook, `useAutoResume`, enhances stream resumption by handling race conditions and processing server-sent `append-message` data parts. It conditionally calls `experimental_resume` if the last message was from a user and updates the chat messages based on incoming stream data, ensuring a robust and consistent chat experience.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-message-persistence.mdx#_snippet_13 LANGUAGE: tsx
CODE:
```
'use client'; import { useEffect } from 'react';
import type { UIMessage } from 'ai';
import type { UseChatHelpers } from '@ai-sdk/react'; export type DataPart = { type: 'append-message'; message: string }; export interface Props { autoResume: boolean; initialMessages: UIMessage[]; experimental_resume: UseChatHelpers['experimental_resume']; data: UseChatHelpers['data']; setMessages: UseChatHelpers['setMessages'];
} export function useAutoResume({ autoResume, initialMessages, experimental_resume, data, setMessages,
}: Props) { useEffect(() => { if (!autoResume) return; const mostRecentMessage = initialMessages.at(-1); if (mostRecentMessage?.role === 'user') { experimental_resume(); } // we intentionally run this once // eslint-disable-next-line react-hooks/exhaustive-deps }, []); useEffect(() => { if (!data || data.length === 0) return; const dataPart = data[0] as DataPart; if (dataPart.type === 'append-message') { const message = JSON.parse(dataPart.message) as UIMessage; setMessages([...initialMessages, message]); } }, [data, initialMessages, setMessages]);
}
``` ---------------------------------------- TITLE: Update Query Viewer to Fetch AI Explanations in TypeScript
DESCRIPTION: This snippet updates the `handleExplainQuery` function in `query-viewer.tsx` to asynchronously fetch AI-generated explanations for SQL queries. It utilizes the `explainQuery` Server Action, manages loading states, and updates the component's state with the retrieved explanations for UI rendering.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/04-natural-language-postgres.mdx#_snippet_14 LANGUAGE: TypeScript
CODE:
```
/* ...other imports... */
import { explainQuery } from '@/app/actions'; /* ...rest of the component... */ const handleExplainQuery = async () => { setQueryExpanded(true); setLoadingExplanation(true); const explanations = await explainQuery(inputValue, activeQuery); setQueryExplanations(explanations); setLoadingExplanation(false);
}; /* ...rest of the component... */
``` ---------------------------------------- TITLE: Initializing LangWatch and Starting a Trace (TypeScript)
DESCRIPTION: This snippet initializes the LangWatch client and starts a new trace. The trace can include optional metadata like `threadId` and `userId` to provide context for the captured interactions, allowing manual tracing when the AI SDK OpenTelemetry integration is not used.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langwatch.mdx#_snippet_8 LANGUAGE: typescript
CODE:
```
import { LangWatch } from 'langwatch'; const langwatch = new LangWatch(); const trace = langwatch.getTrace({ metadata: { threadId: 'mythread-123', userId: 'myuser-123' },
});
``` ---------------------------------------- TITLE: Generating Text Embeddings with AI SDK and OpenAI (TypeScript)
DESCRIPTION: This snippet demonstrates how to generate text embeddings using the `@ai-sdk/openai` and `ai` libraries in a Node.js environment. It initializes an OpenAI embedding model, processes a given text value ('sunny day at the beach'), and outputs the resulting high-dimensional vector embedding along with the token usage. This functionality is crucial for tasks like semantic search and document similarity.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/60-embed-text.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { embed } from 'ai';
import 'dotenv/config'; async function main() { const { embedding, usage } = await embed({ model: openai.embedding('text-embedding-3-small'), value: 'sunny day at the beach', }); console.log(embedding); console.log(usage);
} main().catch(console.error);
``` ---------------------------------------- TITLE: Implementing Chat UI with useChat Hook in React Native
DESCRIPTION: This snippet demonstrates how to set up a basic chat interface in a React Native Expo application using the `@ai-sdk/react`'s `useChat` hook. It displays chat messages, handles user input, and submits messages to the API. It explicitly uses `expo/fetch` for streaming responses and integrates with a custom `generateAPIUrl` utility.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/07-expo.mdx#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { generateAPIUrl } from '@/utils';
import { useChat } from '@ai-sdk/react';
import { fetch as expoFetch } from 'expo/fetch';
import { View, TextInput, ScrollView, Text, SafeAreaView } from 'react-native'; export default function App() { const { messages, error, handleInputChange, input, handleSubmit } = useChat({ fetch: expoFetch as unknown as typeof globalThis.fetch, api: generateAPIUrl('/api/chat'), onError: error => console.error(error, 'ERROR'), }); if (error) return <Text>{error.message}</Text>; return ( <SafeAreaView style={{ height: '100%' }}> <View style={{ height: '95%', display: 'flex', flexDirection: 'column', paddingHorizontal: 8, }} > <ScrollView style={{ flex: 1 }}> {messages.map(m => ( <View key={m.id} style={{ marginVertical: 8 }}> <View> <Text style={{ fontWeight: 700 }}>{m.role}</Text> <Text>{m.content}</Text> </View> </View> ))} </ScrollView> <View style={{ marginTop: 8 }}> <TextInput style={{ backgroundColor: 'white', padding: 8 }} placeholder="Say something..." value={input} onChange={e => handleInputChange({ ...e, target: { ...e.target, value: e.nativeEvent.text, }, } as unknown as React.ChangeEvent<HTMLInputElement>) } onSubmitEditing={e => { handleSubmit(e); e.preventDefault(); }} autoFocus={true} /> </View> </View> </SafeAreaView> );
}
``` ---------------------------------------- TITLE: Enable Extended Thinking for Claude 3.7 Sonnet
DESCRIPTION: This snippet demonstrates how to activate Claude 3.7 Sonnet's extended thinking feature, which allows for step-by-step reasoning. It involves setting the `thinking` option within `providerOptions.anthropic` and specifying a `budgetTokens` for the reasoning process.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/20-sonnet-3-7.mdx#_snippet_2 LANGUAGE: ts
CODE:
```
import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';
import { generateText } from 'ai'; const { text, reasoning, reasoningDetails } = await generateText({ model: anthropic('claude-3-7-sonnet-20250219'), prompt: 'How many people will live in the world in 2040?', providerOptions: { anthropic: { thinking: { type: 'enabled', budgetTokens: 12000 }, } satisfies AnthropicProviderOptions, },
}); console.log(reasoning); // reasoning text
console.log(reasoningDetails); // reasoning details including redacted reasoning
console.log(text); // text response
``` ---------------------------------------- TITLE: Using Message Prompts for Chat Interfaces in AI SDK (TypeScript)
DESCRIPTION: This example shows how to use an array of `messages` to create a conversational prompt for the `streamUI` function, suitable for chat interfaces. Each message object includes a `role` (e.g., 'user', 'assistant') and `content`. This allows for multi-turn conversations and more complex prompt structures. `yourModel` is the required language model.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#_snippet_3 LANGUAGE: TypeScript
CODE:
```
const result = await streamUI({ model: yourModel, messages: [ { role: 'user', content: 'Hi!' }, { role: 'assistant', content: 'Hello, how can I help?' }, { role: 'user', content: 'Where can I buy the best Currywurst in Berlin?' }, ],
});
``` ---------------------------------------- TITLE: Customize Individual Steps with `experimental_prepareStep` Callback (TypeScript/React)
DESCRIPTION: The `experimental_prepareStep` callback, available in `generateText`, is invoked before a step begins. It allows dynamic modification of settings for that specific step, such as using a different model, forcing a tool choice, or limiting available tools based on step number or prior steps.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#_snippet_4 LANGUAGE: tsx
CODE:
```
import { generateText } from 'ai'; const result = await generateText({ // ... experimental_prepareStep: async ({ model, stepNumber, maxSteps, steps }) => { if (stepNumber === 0) { return { // use a different model for this step: model: modelForThisParticularStep, // force a tool choice for this step: toolChoice: { type: 'tool', toolName: 'tool1' }, // limit the tools that are available for this step: experimental_activeTools: ['tool1'] }; } // when nothing is returned, the default settings are used }
});
``` ---------------------------------------- TITLE: Configuring Vercel AI SDK Provider (TypeScript)
DESCRIPTION: This code defines the `AI` provider using `createAI` from the Vercel AI SDK. It registers `submitMessage` as an available action for the AI model and initializes the AI and UI states, setting up the core AI interaction logic for the application.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/121-stream-assistant-response-with-tools.mdx#_snippet_4 LANGUAGE: typescript
CODE:
```
import { createAI } from 'ai/rsc';
import { submitMessage } from './actions'; export const AI = createAI({ actions: { submitMessage, }, initialAIState: [], initialUIState: [],
});
``` ---------------------------------------- TITLE: Generating Text with Vercel v0-1.0-md Model (TypeScript)
DESCRIPTION: This snippet illustrates how to use the imported `vercel` provider instance with the AI SDK's `generateText` function. It specifies the `v0-1.0-md` model and a prompt to generate AI-powered text, demonstrating basic text generation capabilities.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/02-vercel.mdx#_snippet_5 LANGUAGE: typescript
CODE:
```
import { vercel } from '@ai-sdk/vercel';
import { generateText } from 'ai'; const { text } = await generateText({ model: vercel('v0-1.0-md'), prompt: 'Create a Next.js AI chatbot',
});
``` ---------------------------------------- TITLE: Install AI SDK React UI Package
DESCRIPTION: Installs the `@ai-sdk/react` package, providing framework-agnostic hooks for building chatbots and generative UIs in React applications. This package is essential for developing interactive AI user interfaces.
SOURCE: https://github.com/vercel/ai/blob/main/packages/ai/README.md#_snippet_3 LANGUAGE: shell
CODE:
```
npm install @ai-sdk/react
``` ---------------------------------------- TITLE: Sending Image URL Parts in AI SDK User Messages (TypeScript)
DESCRIPTION: This snippet shows how to include an image by providing its URL directly in the `image` property of an image message part. This is useful for models that can fetch images from external URLs.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#_snippet_10 LANGUAGE: ts
CODE:
```
const result = await generateText({ model: yourModel, messages: [ { role: 'user', content: [ { type: 'text', text: 'Describe the image in detail.' }, { type: 'image', image: 'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true' } ] } ]
});
``` ---------------------------------------- TITLE: Generating Text with System Prompt using AI SDK's generateText (TypeScript)
DESCRIPTION: This example illustrates how to use a 'system' prompt with `generateText` to provide the LLM with specific instructions or a persona. This allows for more controlled and nuanced text generation, ensuring the output adheres to desired styles or roles. The prompt can also include dynamic content.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/05-generating-text.mdx#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { generateText } from 'ai'; const { text } = await generateText({ model: yourModel, system: 'You are a professional writer. ' + 'You write simple, clear, and concise content.', prompt: `Summarize the following article in 3-5 sentences: ${article}`,
});
``` ---------------------------------------- TITLE: Integrating OpenAI Assistant with React UI
DESCRIPTION: This client-side React component demonstrates how to use the `useAssistant` hook to manage the UI state for an OpenAI assistant. It displays messages, handles user input, and updates the UI automatically as the assistant streams responses. Key properties like `status`, `messages`, `input`, `submitMessage`, and `handleInputChange` are utilized for a dynamic chat interface.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/10-openai-assistants.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
'use client'; import { Message, useAssistant } from '@ai-sdk/react'; export default function Chat() { const { status, messages, input, submitMessage, handleInputChange } = useAssistant({ api: '/api/assistant' }); return ( <div> {messages.map((m: Message) => ( <div key={m.id}> <strong>{`${m.role}: `}</strong> {m.role !== 'data' && m.content} {m.role === 'data' && ( <> {(m.data as any).description} <br /> <pre className={'bg-gray-200'}> {JSON.stringify(m.data, null, 2)} </pre> </> )} </div> ))} {status === 'in_progress' && <div />} <form onSubmit={submitMessage}> <input disabled={status !== 'awaiting_message'} value={input} placeholder="What is the temperature in the living room?" onChange={handleInputChange} /> </form> </div> );
}
``` ---------------------------------------- TITLE: Call Server Action to Stream UI Components on Client in Next.js
DESCRIPTION: This client-side React component demonstrates how to invoke a Next.js Server Action (`streamComponent`) to fetch and display dynamically streamed React components. Upon form submission, it calls the server action and updates its state with the returned component, enabling interactive and AI-driven UI experiences.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#_snippet_8 LANGUAGE: tsx
CODE:
```
'use client'; import { useState } from 'react';
import { streamComponent } from './actions'; export default function Page() { const [component, setComponent] = useState<React.ReactNode>(); return ( <div> <form onSubmit={async e => { e.preventDefault(); setComponent(await streamComponent()); }} > <button>Stream Component</button> </form> <div>{component}</div> </div> );
}
``` ---------------------------------------- TITLE: Bootstrap Next.js AI Chat Application
DESCRIPTION: Commands to initialize a new Next.js project using the `create-next-app` utility, specifically cloning the `next-langchain` example from the Vercel AI repository. This sets up the foundational structure for the AI chat application and can be executed with npm, Yarn, or pnpm.
SOURCE: https://github.com/vercel/ai/blob/main/examples/next-langchain/README.md#_snippet_0 LANGUAGE: bash
CODE:
```
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-langchain next-langchain-app
``` LANGUAGE: bash
CODE:
```
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-langchain next-langchain-app
``` LANGUAGE: bash
CODE:
```
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-langchain next-langchain-app
``` ---------------------------------------- TITLE: Using Web Search Tool for Grounding Responses (TypeScript)
DESCRIPTION: Demonstrates how to use the `webSearch` tool with the AI SDK's Responses API to enable the model to access the internet for relevant information. This snippet shows a basic implementation for generating text with web search capabilities.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/19-openai-responses.mdx#_snippet_3 LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai'; const result = await generateText({ model: openai.responses('gpt-4o-mini'), prompt: 'What happened in San Francisco last week?', tools: { web_search_preview: openai.tools.webSearchPreview(), },
}); console.log(result.text);
console.log(result.sources);
``` ---------------------------------------- TITLE: Process File Inputs with Google Vertex AI Provider
DESCRIPTION: This example illustrates how to send file inputs, such as PDF documents, to Google Vertex AI models using the AI SDK. Files are included in the `messages` array with `type: 'file'`, `data` (e.g., from `fs.readFileSync`), and `mimeType`. The SDK can automatically download URLs, except for `gs://` URLs.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/16-google-vertex.mdx#_snippet_14 LANGUAGE: ts
CODE:
```
import { vertex } from '@ai-sdk/google-vertex';
import { generateText } from 'ai'; const { text } = await generateText({ model: vertex('gemini-1.5-pro'), messages: [ { role: 'user', content: [ { type: 'text', text: 'What is an embedding model according to this document?', }, { type: 'file', data: fs.readFileSync('./data/ai.pdf'), mimeType: 'application/pdf', }, ], }, ],
});
``` ---------------------------------------- TITLE: Implementing an Evaluator-Optimizer for Translation in TypeScript
DESCRIPTION: This snippet demonstrates an evaluator-optimizer pattern for improving text translations. It iteratively translates text, evaluates the translation quality using a larger model, and then refines the translation based on feedback until a quality threshold is met or maximum iterations are reached. It uses `@ai-sdk/openai` and `zod` for schema validation.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#_snippet_4 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateText, generateObject } from 'ai';
import { z } from 'zod'; async function translateWithFeedback(text: string, targetLanguage: string) { let currentTranslation = ''; let iterations = 0; const MAX_ITERATIONS = 3; // Initial translation const { text: translation } = await generateText({ model: openai('gpt-4o-mini'), // use small model for first attempt system: 'You are an expert literary translator.', prompt: `Translate this text to ${targetLanguage}, preserving tone and cultural nuances: ${text}`, }); currentTranslation = translation; // Evaluation-optimization loop while (iterations < MAX_ITERATIONS) { // Evaluate current translation const { object: evaluation } = await generateObject({ model: openai('gpt-4o'), // use a larger model to evaluate schema: z.object({ qualityScore: z.number().min(1).max(10), preservesTone: z.boolean(), preservesNuance: z.boolean(), culturallyAccurate: z.boolean(), specificIssues: z.array(z.string()), improvementSuggestions: z.array(z.string()), }), system: 'You are an expert in evaluating literary translations.', prompt: `Evaluate this translation: Original: ${text} Translation: ${currentTranslation} Consider: 1. Overall quality 2. Preservation of tone 3. Preservation of nuance 4. Cultural accuracy`, }); // Check if quality meets threshold if ( evaluation.qualityScore >= 8 && evaluation.preservesTone && evaluation.preservesNuance && evaluation.culturallyAccurate ) { break; } // Generate improved translation based on feedback const { text: improvedTranslation } = await generateText({ model: openai('gpt-4o'), // use a larger model system: 'You are an expert literary translator.', prompt: `Improve this translation based on the following feedback: ${evaluation.specificIssues.join('\n')} ${evaluation.improvementSuggestions.join('\n')} Original: ${text} Current Translation: ${currentTranslation}`, }); currentTranslation = improvedTranslation; iterations++; } return { finalTranslation: currentTranslation, iterationsRequired: iterations, };
}
``` ---------------------------------------- TITLE: Server-side Object Streaming with streamObject (TypeScript)
DESCRIPTION: This server-side API route handles the object generation process using `streamObject` from the `ai` library. It configures an OpenAI model (`gpt-4-turbo`), applies the `notificationSchema` for structured output, and generates notifications based on the provided `context` from the client request, streaming the result back as a text stream.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/08-object-generation.mdx#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';
import { notificationSchema } from './schema'; // Allow streaming responses up to 30 seconds
export const maxDuration = 30; export async function POST(req: Request) { const context = await req.json(); const result = streamObject({ model: openai('gpt-4-turbo'), schema: notificationSchema, prompt: `Generate 3 notifications for a messages app in this context:` + context, }); return result.toTextStreamResponse();
}
``` ---------------------------------------- TITLE: Reading UI State in Client Component with useUIState (TypeScript)
DESCRIPTION: Illustrates how to access and display the current UI state within a client component using the `useUIState` hook from `ai/rsc`. It retrieves the `messages` array and renders each message's `display` property in an unordered list.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/03-generative-ui-state.mdx#_snippet_3 LANGUAGE: tsx
CODE:
```
'use client'; import { useUIState } from 'ai/rsc'; export default function Page() { const [messages, setMessages] = useUIState(); return ( <ul> {messages.map(message => ( <li key={message.id}>{message.display}</li> ))} </ul> );
}
``` ---------------------------------------- TITLE: Enabling Reasoning for Anthropic Models (TypeScript)
DESCRIPTION: This snippet demonstrates how to enable and utilize the reasoning feature for compatible Anthropic models (e.g., `claude-4-opus-20250514`) within the `generateText` function. It configures a thinking budget and captures the generated text, reasoning, and detailed reasoning output.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/05-anthropic.mdx#_snippet_5 LANGUAGE: TypeScript
CODE:
```
import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';
import { generateText } from 'ai'; const { text, reasoning, reasoningDetails } = await generateText({ model: anthropic('claude-4-opus-20250514'), prompt: 'How many people will live in the world in 2040?', providerOptions: { anthropic: { thinking: { type: 'enabled', budgetTokens: 12000 } } satisfies AnthropicProviderOptions }
}); console.log(reasoning); // reasoning text
console.log(reasoningDetails); // reasoning details including redacted reasoning
console.log(text); // text response
``` ---------------------------------------- TITLE: Leveraging Implicit Caching for Google Generative AI Models
DESCRIPTION: This example illustrates how Gemini 2.5 models automatically provide cache cost savings for repetitive content. It shows how to structure prompts with consistent content at the beginning to maximize cache hits and reduce token costs, and how to inspect cached token counts in the response's usage metadata.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/15-google-generative-ai.mdx#_snippet_12 LANGUAGE: TypeScript
CODE:
```
import { google } from '@ai-sdk/google';
import { generateText } from 'ai'; // Structure prompts with consistent content at the beginning
const baseContext = 'You are a cooking assistant with expertise in Italian cuisine. Here are 1000 lasagna recipes for reference...'; const { text: veggieLasagna } = await generateText({ model: google('gemini-2.5-pro'), prompt: `${baseContext}\n\nWrite a vegetarian lasagna recipe for 4 people.`,
}); // Second request with same prefix - eligible for cache hit
const { text: meatLasagna, response } = await generateText({ model: google('gemini-2.5-pro'), prompt: `${baseContext}\n\nWrite a meat lasagna recipe for 12 people.`,
}); // Check cached token count in usage metadata
console.log('Cached tokens:', response.body.usageMetadata);
``` ---------------------------------------- TITLE: Forwarding Abort Signals to Tools in Vercel AI SDK
DESCRIPTION: This snippet illustrates how `abortSignal` from `generateText` or `streamText` is automatically forwarded to the tool's `execute` function. This allows long-running tool operations, such as `fetch` requests, to be cancelled gracefully when the main generation process is aborted, improving responsiveness and resource management.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#_snippet_9 LANGUAGE: ts
CODE:
```
import { z } from 'zod';
import { generateText, tool } from 'ai'; const result = await generateText({ model: yourModel, abortSignal: myAbortSignal, // signal that will be forwarded to tools tools: { weather: tool({ description: 'Get the weather in a location', parameters: z.object({ location: z.string() }), execute: async ({ location }, { abortSignal }) => { return fetch( `https://api.weatherapi.com/v1/current.json?q=${location}`, { signal: abortSignal }, // forward the abort signal to fetch ); }, }), }, prompt: 'What is the weather in San Francisco?',
});
``` ---------------------------------------- TITLE: Migrate AI Models: Generate Text with OpenAI and DeepInfra Llama
DESCRIPTION: This section demonstrates the AI SDK's unified API by showing how to generate text using both OpenAI's GPT-4-Turbo and Meta's Llama 3.1 on DeepInfra. It highlights the minimal code changes required to switch between different AI model providers, primarily by changing the client import and model name.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/21-llama-3_1.mdx#_snippet_9 LANGUAGE: tsx
CODE:
```
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai'; const { text } = await generateText({ model: openai('gpt-4-turbo'), prompt: 'What is love?',
});
``` LANGUAGE: tsx
CODE:
```
import { generateText } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra'; const { text } = await generateText({ model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'), prompt: 'What is love?',
});
``` ---------------------------------------- TITLE: Caching Language Model Generated Text
DESCRIPTION: This example illustrates how to implement a simple in-memory cache for language model generated text using the `wrapGenerate` middleware. It checks the cache before calling the language model and stores the result if not found, improving performance for repeated requests with the same parameters.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/40-middleware.mdx#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import type { LanguageModelV1Middleware } from 'ai'; const cache = new Map<string, any>(); export const yourCacheMiddleware: LanguageModelV1Middleware = { wrapGenerate: async ({ doGenerate, params }) => { const cacheKey = JSON.stringify(params); if (cache.has(cacheKey)) { return cache.get(cacheKey); } const result = await doGenerate(); cache.set(cacheKey, result); return result; }, // here you would implement the caching logic for streaming
};
``` ---------------------------------------- TITLE: Defining Stock Tool and Updating Tools Object in TypeScript
DESCRIPTION: This snippet defines a new AI tool, 'stockTool', using 'createTool' from '@ai-sdk/react'. It specifies a description, parameters (stock symbol), and an asynchronous 'execute' function that simulates fetching a stock price. It then updates the 'tools' object to include this new tool alongside an existing 'weatherTool', making it available for the AI model.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/04-generative-user-interfaces.mdx#_snippet_6 LANGUAGE: TypeScript
CODE:
```
// Add a new stock tool
export const stockTool = createTool({ description: 'Get price for a stock', parameters: z.object({ symbol: z.string().describe('The stock symbol to get the price for'), }), execute: async function ({ symbol }) { // Simulated API call await new Promise(resolve => setTimeout(resolve, 2000)); return { symbol, price: 100 }; },
}); // Update the tools object
export const tools = { displayWeather: weatherTool, getStockPrice: stockTool,
};
``` ---------------------------------------- TITLE: Updating API Route with Weather Tool in TypeScript
DESCRIPTION: This snippet modifies the `app/api/chat+api.ts` file to integrate a `weather` tool into an AI chatbot's API route. It uses `@ai-sdk/openai` for the model and `ai` for tool definition, leveraging `zod` for parameter validation. The `weather` tool is configured with a description, a `location` parameter, and an `execute` function that simulates fetching temperature data, allowing the model to invoke external capabilities.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/07-expo.mdx#_snippet_8 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText, tool } from 'ai';
import { z } from 'zod'; export async function POST(req: Request) { const { messages } = await req.json(); const result = streamText({ model: openai('gpt-4o'), messages, tools: { weather: tool({ description: 'Get the weather in a location (fahrenheit)', parameters: z.object({ location: z.string().describe('The location to get the weather for'), }), execute: async ({ location }) => { const temperature = Math.round(Math.random() * (90 - 32) + 32); return { location, temperature, }; }, }), }, }); return result.toDataStreamResponse({ headers: { 'Content-Type': 'application/octet-stream', 'Content-Encoding': 'none', }, });
}
``` ---------------------------------------- TITLE: Define Language Models with AI Gateway Provider Fallback
DESCRIPTION: Create a model instance by passing an array of AI models to the `aigateway` provider. The provider will attempt to use the models in the specified order, falling back to the next if a primary model fails, ensuring resilience and high availability.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/12-cloudflare-ai-gateway.mdx#_snippet_4 LANGUAGE: typescript
CODE:
```
import { createAiGateway } from 'ai-gateway-provider';
import { createOpenAI } from '@ai-sdk/openai';
import { createAnthropic } from '@ai-sdk/anthropic'; const aigateway = createAiGateway({ accountId: 'your-cloudflare-account-id', gateway: 'your-gateway-name', apiKey: 'your-cloudflare-api-key',
}); const openai = createOpenAI({ apiKey: 'openai-api-key' });
const anthropic = createAnthropic({ apiKey: 'anthropic-api-key' }); const model = aigateway([ anthropic('claude-3-5-haiku-20241022'), // Primary model openai('gpt-4o-mini'), // Fallback model
]);
``` ---------------------------------------- TITLE: Accessing All Steps from AI SDK generateText Response
DESCRIPTION: This code demonstrates how to retrieve and process all intermediate steps generated by the 'generateText' function when 'maxSteps' is configured. The 'steps' property of the response provides a detailed history of the LLM's interactions, including tool calls.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { generateText } from 'ai'; const { steps } = await generateText({ model: openai('gpt-4o'), maxSteps: 10, // ...
}); // extract all tool calls from the steps:
const allToolCalls = steps.flatMap(step => step.toolCalls);
``` ---------------------------------------- TITLE: Enforcing Structured Output via experimental_output with generateText (TypeScript)
DESCRIPTION: This snippet shows how to achieve structured outputs when using `generateText` by leveraging the `experimental_output` option. It passes a Zod schema to `Output.object`, guiding the model to produce text that can be parsed into the defined JSON structure for ingredients and steps.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/03-openai.mdx#_snippet_27 LANGUAGE: TypeScript
CODE:
```
// Using generateText
const result = await generateText({ model: openai.responses('gpt-4.1'), prompt: 'How do I make a pizza?', experimental_output: Output.object({ schema: z.object({ ingredients: z.array(z.string()), steps: z.array(z.string()), }), }),
});
``` ---------------------------------------- TITLE: Implementing Client-Side Text Completion with useCompletion Hook (React)
DESCRIPTION: This React component demonstrates how to use the `useCompletion` hook from `@ai-sdk/react` to create a user interface for text completions. It manages input, handles form submission, and displays the streamed completion from a `/api/completion` endpoint, providing a seamless real-time experience.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/05-completion.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
'use client'; import { useCompletion } from '@ai-sdk/react'; export default function Page() { const { completion, input, handleInputChange, handleSubmit } = useCompletion({ api: '/api/completion', }); return ( <form onSubmit={handleSubmit}> <input name="prompt" value={input} onChange={handleInputChange} id="input" /> <button type="submit">Submit</button> <div>{completion}</div> </form> );
}
``` ---------------------------------------- TITLE: Identifying Tools Requiring Confirmation in TypeScript
DESCRIPTION: This function takes a `ToolSet` object and returns an array of strings, where each string is the name of a tool that does not have an `execute` function defined. These are typically tools that require external human confirmation before their action can proceed, simplifying frontend logic by avoiding manual checks.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/01-next/75-human-in-the-loop.mdx#_snippet_8 LANGUAGE: TypeScript
CODE:
```
export function getToolsRequiringConfirmation<T extends ToolSet>( tools: T,
): string[] { return (Object.keys(tools) as (keyof T)[]).filter(key => { const maybeTool = tools[key]; return typeof maybeTool.execute !== 'function'; }) as string[];
}
``` ---------------------------------------- TITLE: Defining Weather Tool in AI API Route (TypeScript)
DESCRIPTION: This snippet modifies the server-side API route to include a new 'weather' tool. It defines the tool's description, parameters (using Zod for 'location' string validation), and an asynchronous 'execute' function that simulates fetching weather data. This allows the AI model to call this function when a user query requires weather information.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/05-nuxt.mdx#_snippet_10 LANGUAGE: typescript
CODE:
```
import { streamText, tool } from 'ai';
import { createOpenAI } from '@ai-sdk/openai';
import { z } from 'zod'; export default defineLazyEventHandler(async () => { const apiKey = useRuntimeConfig().openaiApiKey; if (!apiKey) throw new Error('Missing OpenAI API key'); const openai = createOpenAI({ apiKey: apiKey, }); return defineEventHandler(async (event: any) => { const { messages } = await readBody(event); const result = streamText({ model: openai('gpt-4o'), messages, tools: { weather: tool({ description: 'Get the weather in a location (fahrenheit)', parameters: z.object({ location: z .string() .describe('The location to get the weather for'), }), execute: async ({ location }) => { const temperature = Math.round(Math.random() * (90 - 32) + 32); return { location, temperature, }; }, }), }, }); return result.toDataStreamResponse(); });
});
``` ---------------------------------------- TITLE: Using `consumeStream` to Persist AI SDK Chat on Client Disconnect
DESCRIPTION: This TypeScript/TSX example demonstrates how to use `result.consumeStream()` with the AI SDK's `streamText` function. Calling `consumeStream` removes backpressure, ensuring the LLM stream completes and `onFinish` is triggered on the backend, even if the client disconnects. This allows the full conversation to be saved, preventing data loss and enabling restoration.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/03-chatbot-message-persistence.mdx#_snippet_11 LANGUAGE: tsx
CODE:
```
import { appendResponseMessages, streamText } from 'ai';
import { saveChat } from '@tools/chat-store'; export async function POST(req: Request) { const { messages, id } = await req.json(); const result = streamText({ model, messages, async onFinish({ response }) { await saveChat({ id, messages: appendResponseMessages({ messages, responseMessages: response.messages, }), }); }, }); // consume the stream to ensure it runs to completion & triggers onFinish // even when the client response is aborted: result.consumeStream(); // no await return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Using System Prompts for Constraining Model Behavior in AI SDK (TypeScript)
DESCRIPTION: This snippet demonstrates how to use a `system` prompt alongside a regular `prompt` in the `generateText` function. The system prompt provides initial instructions to guide the model's overall behavior, ensuring it acts as a travel itinerary planner and responds with a list of stops. `yourModel` is the required language model, and `destination` and `lengthOfStay` are dynamic variables for the user prompt.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/03-prompts.mdx#_snippet_2 LANGUAGE: TypeScript
CODE:
```
const result = await generateText({ model: yourModel, system: `You help planning travel itineraries. ` + `Respond to the users' request with a list ` + `of the best stops to make in their destination.`, prompt: `I am planning a trip to ${destination} for ${lengthOfStay} days. ` + `Please suggest the best tourist activities for me to do.`,
});
``` ---------------------------------------- TITLE: Implementing Image Upload in Next.js Chatbot with AI SDK
DESCRIPTION: This snippet updates the `app/page.tsx` file to enable image uploads in a Next.js chatbot. It integrates `useState` and `useRef` to manage file selection, displays uploaded images using `next/image`, and modifies the `handleSubmit` function to send `FileList` attachments to the AI model. It also includes a file input field and clears the selected files after submission.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-guides/02-multi-modal-chatbot.mdx#_snippet_7 LANGUAGE: TypeScript
CODE:
```
'use client'; import { useChat } from '@ai-sdk/react';
import { useRef, useState } from 'react';
import Image from 'next/image'; export default function Chat() { const { messages, input, handleInputChange, handleSubmit } = useChat(); const [files, setFiles] = useState<FileList | undefined>(undefined); const fileInputRef = useRef<HTMLInputElement>(null); return ( <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch"> {messages.map(m => ( <div key={m.id} className="whitespace-pre-wrap"> {m.role === 'user' ? 'User: ' : 'AI: '} {m.content} <div> {m?.experimental_attachments ?.filter(attachment => attachment?.contentType?.startsWith('image/'), ) .map((attachment, index) => ( <Image key={`${m.id}-${index}`} src={attachment.url} width={500} height={500} alt={attachment.name ?? `attachment-${index}`} /> ))} </div> </div> ))} <form className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl space-y-2" onSubmit={event => { handleSubmit(event, { experimental_attachments: files, }); setFiles(undefined); if (fileInputRef.current) { fileInputRef.current.value = ''; } }} > <input type="file" className="" onChange={event => { if (event.target.files) { setFiles(event.target.files); } }} multiple ref={fileInputRef} /> <input className="w-full p-2" value={input} placeholder="Say something..." onChange={handleInputChange} /> </form> </div> );
}
``` ---------------------------------------- TITLE: Streaming Tool Data via Route Handler with streamText (After)
DESCRIPTION: This snippet demonstrates the updated approach for generative UIs using a Next.js API route handler. It leverages `streamText` to stream only the data (props) from tool executions to the client, where `useChat` can then decode and render the UI. Dependencies include `ai`, `@ai-sdk/openai`, `zod`, and custom query utilities.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/10-migrating-to-ui.mdx#_snippet_5 LANGUAGE: ts
CODE:
```
import { z } from 'zod';
import { openai } from '@ai-sdk/openai';
import { getWeather } from '@/utils/queries';
import { streamText } from 'ai'; export async function POST(request) { const { messages } = await request.json(); const result = streamText({ model: openai('gpt-4o'), system: 'you are a friendly assistant!', messages, tools: { displayWeather: { description: 'Display the weather for a location', parameters: z.object({ latitude: z.number(), longitude: z.number() }), execute: async function ({ latitude, longitude }) { const props = await getWeather({ latitude, longitude }); return props; } } } }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Handling Multiple Streamable UIs with AI SDK RSC
DESCRIPTION: This function demonstrates how to create and manage multiple independent streamable UI components using `createStreamableUI` from `ai/rsc`. It updates and completes `weatherUI` and `forecastUI` asynchronously, returning their values along with other data in a single response. This allows for decoupled and independently updating UI sections on the client side.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/05-multiple-streamables.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
'use server'; import { createStreamableUI } from 'ai/rsc'; export async function getWeather() { const weatherUI = createStreamableUI(); const forecastUI = createStreamableUI(); weatherUI.update(<div>Loading weather...</div>); forecastUI.update(<div>Loading forecast...</div>); getWeatherData().then(weatherData => { weatherUI.done(<div>{weatherData}</div>); }); getForecastData().then(forecastData => { forecastUI.done(<div>{forecastData}</div>); }); // Return both streamable UIs and other data fields. return { requestedAt: Date.now(), weather: weatherUI.value, forecast: forecastUI.value, };
}
``` ---------------------------------------- TITLE: Defining AI Server Actions and Tools with Vercel AI SDK (TSX)
DESCRIPTION: This snippet defines the core server-side logic for an AI conversation. It uses 'ai/rsc' to manage mutable AI state and stream UI updates. It includes tool definitions for 'showStockInformation' (fetching stock data) and 'showFlightStatus' (fetching flight status), demonstrating how to integrate external functionality into the AI's responses.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/90-render-visual-interface-in-chat.mdx#_snippet_3 LANGUAGE: TSX
CODE:
```
'use server'; import { getMutableAIState, streamUI } from 'ai/rsc';
import { openai } from '@ai-sdk/openai';
import { ReactNode } from 'react';
import { z } from 'zod';
import { generateId } from 'ai';
import { Stock } from '@/components/stock';
import { Flight } from '@/components/flight'; export interface ServerMessage { role: 'user' | 'assistant'; content: string;
} export interface ClientMessage { id: string; role: 'user' | 'assistant'; display: ReactNode;
} export async function continueConversation( input: string,
): Promise<ClientMessage> { 'use server'; const history = getMutableAIState(); const result = await streamUI({ model: openai('gpt-3.5-turbo'), messages: [...history.get(), { role: 'user', content: input }], text: ({ content, done }) => { if (done) { history.done((messages: ServerMessage[]) => [ ...messages, { role: 'assistant', content }, ]); } return <div>{content}</div>; }, tools: { showStockInformation: { description: 'Get stock information for symbol for the last numOfMonths months', parameters: z.object({ symbol: z .string() .describe('The stock symbol to get information for'), numOfMonths: z .number() .describe('The number of months to get historical information for'), }), generate: async ({ symbol, numOfMonths }) => { history.done((messages: ServerMessage[]) => [ ...messages, { role: 'assistant', content: `Showing stock information for ${symbol}`, }, ]); return <Stock symbol={symbol} numOfMonths={numOfMonths} />; }, }, showFlightStatus: { description: 'Get the status of a flight', parameters: z.object({ flightNumber: z .string() .describe('The flight number to get status for'), }), generate: async ({ flightNumber }) => { history.done((messages: ServerMessage[]) => [ ...messages, { role: 'assistant', content: `Showing flight status for ${flightNumber}`, }, ]); return <Flight flightNumber={flightNumber} />; }, }, }, }); return { id: generateId(), role: 'assistant', display: result.value, };
}
``` ---------------------------------------- TITLE: Generating AI Response Stream on Server with AI SDK RSC
DESCRIPTION: This server-side `generateResponse` function uses the AI SDK to stream text responses. It initializes a `streamableValue` and an asynchronous IIFE (Immediately Invoked Function Expression) to call `streamText` with `openai('gpt-4o')`. As text chunks are received from the AI model, `stream.update()` sends them to the client. Finally, `stream.done()` signals completion, and `stream.value` is returned for client consumption.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/06-loading-state.mdx#_snippet_1 LANGUAGE: TypeScript
CODE:
```
'use server'; import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { createStreamableValue } from 'ai/rsc'; export async function generateResponse(prompt: string) { const stream = createStreamableValue(); (async () => { const { textStream } = streamText({ model: openai('gpt-4o'), prompt, }); for await (const text of textStream) { stream.update(text); } stream.done(); })(); return stream.value;
}
``` ---------------------------------------- TITLE: Implementing Web Search with Exa in TypeScript
DESCRIPTION: This snippet defines a 'webSearch' tool using the 'ai' SDK and Exa API. It allows an AI model to perform web searches, retrieving titles, URLs, and truncated content from search results. It requires an 'EXA_API_KEY' environment variable for authentication.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/56-web-search-agent.mdx#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';
import Exa from 'exa-js'; export const exa = new Exa(process.env.EXA_API_KEY); export const webSearch = tool({ description: 'Search the web for up-to-date information', parameters: z.object({ query: z.string().min(1).max(100).describe('The search query'), }), execute: async ({ query }) => { const { results } = await exa.searchAndContents(query, { livecrawl: 'always', numResults: 3, }); return results.map(result => ({ title: result.title, url: result.url, content: result.text.slice(0, 1000), // take just the first 1000 characters publishedDate: result.publishedDate, })); },
}); const { text } = await generateText({ model: openai('gpt-4o-mini'), // can be any model that supports tools prompt: 'What happened in San Francisco last week?', tools: { webSearch, }, maxSteps: 2,
});
``` ---------------------------------------- TITLE: Generating Text with DeepInfra Language Model
DESCRIPTION: Illustrates how to use a DeepInfra language model with the `generateText` function from the AI SDK. It shows how to specify a model ID (e.g., 'meta-llama/Meta-Llama-3.1-70B-Instruct') and provide a prompt to generate text.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/11-deepinfra.mdx#_snippet_5 LANGUAGE: ts
CODE:
```
import { deepinfra } from '@ai-sdk/deepinfra';
import { generateText } from 'ai'; const { text } = await generateText({ model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'), prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
``` ---------------------------------------- TITLE: Defining a Custom Metadata Extractor for OpenAI Compatible Provider (TypeScript)
DESCRIPTION: This snippet defines a `MetadataExtractor` to capture custom data from both complete and streaming API responses. It includes `extractMetadata` for non-streaming responses and `createStreamExtractor` to accumulate data across chunks in streaming scenarios, enabling flexible extraction of provider-specific fields.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/02-openai-compatible-providers/index.mdx#_snippet_9 LANGUAGE: TypeScript
CODE:
```
const myMetadataExtractor: MetadataExtractor = { // Process complete, non-streaming responses extractMetadata: ({ parsedBody }) => { // You have access to the complete raw response // Extract any fields the provider includes return { myProvider: { standardUsage: parsedBody.usage, experimentalFeatures: parsedBody.beta_features, customMetrics: { processingTime: parsedBody.server_timing?.total_ms, modelVersion: parsedBody.model_version // ... any other provider-specific data } } }; }, // Process streaming responses createStreamExtractor: () => { let accumulatedData = { timing: [], customFields: {} }; return { // Process each chunk's raw data processChunk: parsedChunk => { if (parsedChunk.server_timing) { accumulatedData.timing.push(parsedChunk.server_timing); } if (parsedChunk.custom_data) { Object.assign(accumulatedData.customFields, parsedChunk.custom_data); } }, // Build final metadata from accumulated data buildMetadata: () => ({ myProvider: { streamTiming: accumulatedData.timing, customData: accumulatedData.customFields } }) }; }
};
``` ---------------------------------------- TITLE: Aborting Transcription with Timeout
DESCRIPTION: This snippet demonstrates how to use an `AbortSignal` with the `transcribe` function to set a timeout for the transcription process. By passing `AbortSignal.timeout(1000)`, the transcription attempt will be automatically aborted if it does not complete within 1 second, preventing long-running operations.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/36-transcription.mdx#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { experimental_transcribe as transcribe } from 'ai';
import { readFile } from 'fs/promises'; const transcript = await transcribe({ model: openai.transcription('whisper-1'), audio: await readFile('audio.mp3'), abortSignal: AbortSignal.timeout(1000), // Abort after 1 second
});
``` ---------------------------------------- TITLE: Language Model Generate Method Cache Middleware (TypeScript)
DESCRIPTION: This 'wrapGenerate' method within the 'cacheMiddleware' intercepts 'doGenerate' calls for single-response language model interactions. It constructs a cache key from the cleaned prompt, checks for a cached result, and if found, returns it immediately. Otherwise, it proceeds with the actual generation, caches the result, and then returns it, ensuring subsequent identical requests are served from cache.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/80-local-caching-middleware.mdx#_snippet_6 LANGUAGE: TypeScript
CODE:
```
export const cacheMiddleware: LanguageModelV1Middleware = { wrapGenerate: async ({ doGenerate, params }) => { const cacheKey = JSON.stringify({ ...cleanPrompt(params.prompt), _function: 'generate', }); console.log('Cache Key:', cacheKey); const cached = getCachedResult(cacheKey) as Awaited< ReturnType<LanguageModelV1['doGenerate']> > | null; if (cached && cached !== null) { console.log('Cache Hit'); return { ...cached, response: { ...cached.response, timestamp: cached?.response?.timestamp ? new Date(cached?.response?.timestamp) : undefined, }, }; } console.log('Cache Miss'); const result = await doGenerate(); updateCache(cacheKey, result); return result; }, wrapStream: async ({ doStream, params }) => { const cacheKey = JSON.stringify({ ...cleanPrompt(params.prompt), _function: 'stream', }); console.log('Cache Key:', cacheKey); // Check if the result is in the cache const cached = getCachedResult(cacheKey); // If cached, return a simulated ReadableStream that yields the cached result if (cached && cached !== null) { console.log('Cache Hit'); // Format the timestamps in the cached response const formattedChunks = (cached as LanguageModelV1StreamPart[]).map(p => { if (p.type === 'response-metadata' && p.timestamp) { return { ...p, timestamp: new Date(p.timestamp) }; } else return p; }); return { stream: simulateReadableStream({ initialDelayInMs: 0, chunkDelayInMs: 10, chunks: formattedChunks, }), rawCall: { rawPrompt: null, rawSettings: {} }, }; } console.log('Cache Miss'); // If not cached, proceed with streaming const { stream, ...rest } = await doStream(); const fullResponse: LanguageModelV1StreamPart[] = []; const transformStream = new TransformStream< LanguageModelV1StreamPart, LanguageModelV1StreamPart >({ transform(chunk, controller) { fullResponse.push(chunk); controller.enqueue(chunk); }, flush() { // Store the full response in the cache after streaming is complete updateCache(cacheKey, fullResponse); }, }); return { stream: stream.pipeThrough(transformStream), ...rest, }; },
};
``` ---------------------------------------- TITLE: Creating an Entry Point for Custom AI SDK Provider (TypeScript)
DESCRIPTION: This TypeScript code defines the `CustomProvider` interface and the `createCustomProvider` factory function, which serves as the main entry point for a custom AI SDK language model. It manages the instantiation of `CustomChatLanguageModel` instances, allowing for configurable base URLs, API keys, and custom headers, ensuring proper setup for API interactions. The snippet also exports a default `customProvider` instance for immediate use.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/03-community-providers/01-custom-providers.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { generateId, loadApiKey, withoutTrailingSlash,
} from '@ai-sdk/provider-utils';
import { CustomChatLanguageModel } from './custom-chat-language-model';
import { CustomChatModelId, CustomChatSettings } from './custom-chat-settings'; // model factory function with additional methods and properties
export interface CustomProvider { ( modelId: CustomChatModelId, settings?: CustomChatSettings, ): CustomChatLanguageModel; // explicit method for targeting a specific API in case there are several chat( modelId: CustomChatModelId, settings?: CustomChatSettings, ): CustomChatLanguageModel;
} // optional settings for the provider
export interface CustomProviderSettings { /**
Use a different URL prefix for API calls, e.g. to use proxy servers. */ baseURL?: string; /**
API key. */ apiKey?: string; /**
Custom headers to include in the requests. */ headers?: Record<string, string>;
} // provider factory function
export function createCustomProvider( options: CustomProviderSettings = {},
): CustomProvider { const createModel = ( modelId: CustomChatModelId, settings: CustomChatSettings = {}, ) => new CustomChatLanguageModel(modelId, settings, { provider: 'custom.chat', baseURL: withoutTrailingSlash(options.baseURL) ?? 'https://custom.ai/api/v1', headers: () => ({ Authorization: `Bearer ${loadApiKey({ apiKey: options.apiKey, environmentVariableName: 'CUSTOM_API_KEY', description: 'Custom Provider', })}`, ...options.headers, }), generateId: options.generateId ?? generateId, }); const provider = function ( modelId: CustomChatModelId, settings?: CustomChatSettings, ) { if (new.target) { throw new Error( 'The model factory function cannot be called with the new keyword.', ); } return createModel(modelId, settings); }; provider.chat = createModel; return provider;
} /** * Default custom provider instance. */
export const customProvider = createCustomProvider();
``` ---------------------------------------- TITLE: Initializing Vercel AI SDK (TypeScript)
DESCRIPTION: This snippet initializes the Vercel AI SDK using `createAI`, registering the `submitMessage` server action. It sets up the initial AI and UI states for the application, making the AI actions available throughout the React Server Components.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/20-rsc/120-stream-assistant-response.mdx#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { createAI } from 'ai/rsc';
import { submitMessage } from './actions'; export const AI = createAI({ actions: { submitMessage, }, initialAIState: [], initialUIState: [],
});
``` ---------------------------------------- TITLE: Using Text Stream Protocol with useCompletion in Next.js (Frontend)
DESCRIPTION: This Next.js client-side component demonstrates how to use the `useCompletion` hook from `@ai-sdk/react` to handle text streams. It explicitly sets `streamProtocol` to 'text' and displays the streamed completion in a div. The component includes an input field for user prompts and a submit button to trigger the completion request.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/04-ai-sdk-ui/50-stream-protocol.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
'use client'; import { useCompletion } from '@ai-sdk/react'; export default function Page() { const { completion, input, handleInputChange, handleSubmit } = useCompletion({ streamProtocol: 'text', }); return ( <form onSubmit={handleSubmit}> <input name="prompt" value={input} onChange={handleInputChange} /> <button type="submit">Submit</button> <div>{completion}</div> </form> );
}
``` ---------------------------------------- TITLE: Streaming Text with PDF File Prompt using AI SDK and Anthropic (TypeScript)
DESCRIPTION: This TypeScript snippet demonstrates how to stream text from an AI model by providing a PDF file as part of the user prompt. It uses `@ai-sdk/anthropic` to interact with the Claude 3.5 Sonnet model, reads a local PDF file (`./data/ai.pdf`) using Node.js `fs`, and then streams the model's textual response to standard output. Dependencies include `@ai-sdk/anthropic`, `ai`, `dotenv`, and `node:fs`.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/05-node/23-stream-text-with-file-prompt.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { anthropic } from '@ai-sdk/anthropic';
import { streamText } from 'ai';
import 'dotenv/config';
import fs from 'node:fs'; async function main() { const result = streamText({ model: anthropic('claude-3-5-sonnet-20241022'), messages: [ { role: 'user', content: [ { type: 'text', text: 'What is an embedding model according to this document?', }, { type: 'file', data: fs.readFileSync('./data/ai.pdf'), mimeType: 'application/pdf', } ] } ] }); for await (const textPart of result.textStream) { process.stdout.write(textPart); }
} main().catch(console.error);
``` ---------------------------------------- TITLE: Define MCP Tools with Explicit Schemas in TypeScript
DESCRIPTION: This example demonstrates how to explicitly define MCP tool schemas using `zod` in the client code. This approach provides full TypeScript type safety, improved IDE support with autocompletion, and allows for catching parameter mismatches during development. It also offers control over which tools are loaded, reducing unnecessary dependencies, though it requires manual synchronization with server changes.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/15-tools-and-tool-calling.mdx#_snippet_25 LANGUAGE: typescript
CODE:
```
import { z } from 'zod'; const tools = await mcpClient.tools({ schemas: { 'get-data': { parameters: z.object({ query: z.string().describe('The data query'), format: z.enum(['json', 'text']).optional(), }), }, // For tools with zero arguments, you should use an empty object: 'tool-with-no-args': { parameters: z.object({}), }, },
});
``` ---------------------------------------- TITLE: Implementing Feature with Orchestrator-Worker AI Pattern in TypeScript
DESCRIPTION: This snippet illustrates the "Orchestrator-Worker" pattern where an orchestrator model plans a feature implementation, and then specialized worker models execute the planned file changes (create, modify, delete). Each worker receives a tailored system prompt based on the change type. It uses `@ai-sdk/openai` and `zod` for structured input/output.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-foundations/06-agents.mdx#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { generateObject } from 'ai';
import { z } from 'zod'; async function implementFeature(featureRequest: string) { // Orchestrator: Plan the implementation const { object: implementationPlan } = await generateObject({ model: openai('o3-mini'), schema: z.object({ files: z.array( z.object({ purpose: z.string(), filePath: z.string(), changeType: z.enum(['create', 'modify', 'delete']), }), ), estimatedComplexity: z.enum(['low', 'medium', 'high']), }), system: 'You are a senior software architect planning feature implementations.', prompt: `Analyze this feature request and create an implementation plan: ${featureRequest}`, }); // Workers: Execute the planned changes const fileChanges = await Promise.all( implementationPlan.files.map(async file => { // Each worker is specialized for the type of change const workerSystemPrompt = { create: 'You are an expert at implementing new files following best practices and project patterns.', modify: 'You are an expert at modifying existing code while maintaining consistency and avoiding regressions.', delete: 'You are an expert at safely removing code while ensuring no breaking changes.', }[file.changeType]; const { object: change } = await generateObject({ model: openai('gpt-4o'), schema: z.object({ explanation: z.string(), code: z.string(), }), system: workerSystemPrompt, prompt: `Implement the changes for ${file.filePath} to support: ${file.purpose} Consider the overall feature context: ${featureRequest}`, }); return { file, implementation: change, }; }), ); return { plan: implementationPlan, changes: fileChanges, };
}
``` ---------------------------------------- TITLE: Generating Embedding with OpenAI Model (TypeScript)
DESCRIPTION: This snippet demonstrates how to generate an embedding for a single text value using the `embed` function from the `ai` library, integrated with an OpenAI embedding model. It shows how to import necessary modules, configure the model, and pass the input value to obtain the resulting embedding.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/07-reference/01-ai-sdk-core/05-embed.mdx#_snippet_0 LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { embed } from 'ai'; const { embedding } = await embed({ model: openai.embedding('text-embedding-3-small'), value: 'sunny day at the beach',
});
``` ---------------------------------------- TITLE: Defining Multi-Step AI Tools in API Route (TypeScript)
DESCRIPTION: This TypeScript snippet defines an API route that uses @ai-sdk/openai to stream text with multiple AI tools. It includes a weather tool to fetch location-based temperatures and a new convertFahrenheitToCelsius tool for temperature conversion. This setup enables the AI model to perform chained operations, such as getting weather in Fahrenheit and then converting it to Celsius, demonstrating complex multi-step reasoning.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/02-getting-started/04-svelte.mdx#_snippet_12 LANGUAGE: tsx
CODE:
```
import { createOpenAI } from '@ai-sdk/openai';
import { streamText, tool } from 'ai';
import { z } from 'zod'; import { OPENAI_API_KEY } from '$env/static/private'; const openai = createOpenAI({ apiKey: OPENAI_API_KEY,
}); export async function POST({ request }) { const { messages } = await request.json(); const result = streamText({ model: openai('gpt-4o'), messages, tools: { weather: tool({ description: 'Get the weather in a location (fahrenheit)', parameters: z.object({ location: z.string().describe('The location to get the weather for'), }), execute: async ({ location }) => { const temperature = Math.round(Math.random() * (90 - 32) + 32); return { location, temperature, }; }, }), convertFahrenheitToCelsius: tool({ description: 'Convert a temperature in fahrenheit to celsius', parameters: z.object({ temperature: z .number() .describe('The temperature in fahrenheit to convert'), }), execute: async ({ temperature }) => { const celsius = Math.round((temperature - 32) * (5 / 9)); return { celsius, }; }, }), }, }); return result.toDataStreamResponse();
}
``` ---------------------------------------- TITLE: Registering LangfuseExporter in Next.js OpenTelemetry (TypeScript)
DESCRIPTION: This TypeScript snippet, intended for `instrumentation.ts` in a Next.js application, demonstrates how to register the `LangfuseExporter` with the `@vercel/otel` utility. The `registerOTel` function configures the OpenTelemetry SDK, setting the service name and specifying `LangfuseExporter` as the trace exporter to send telemetry data to Langfuse.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/langfuse.mdx#_snippet_4 LANGUAGE: TypeScript
CODE:
```
import { registerOTel } from '@vercel/otel';
import { LangfuseExporter } from 'langfuse-vercel'; export function register() { registerOTel({ serviceName: 'langfuse-vercel-ai-nextjs-example', traceExporter: new LangfuseExporter(), });
}
``` ---------------------------------------- TITLE: Initialize Amazon Bedrock Provider with Credential Chain
DESCRIPTION: This TypeScript snippet demonstrates how to initialize the Amazon Bedrock provider using the AWS SDK's credential provider chain. This method allows the AI SDK to automatically determine AWS credentials from various sources like instance profiles or environment variables, enhancing security and flexibility for authentication.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/01-ai-sdk-providers/08-amazon-bedrock.mdx#_snippet_3 LANGUAGE: ts
CODE:
```
import { createAmazonBedrock } from '@ai-sdk/amazon-bedrock';
import { fromNodeProviderChain } from '@aws-sdk/credential-providers'; const bedrock = createAmazonBedrock({ region: 'us-east-1', credentialProvider: fromNodeProviderChain(),
});
``` ---------------------------------------- TITLE: Enabling Traceloop Telemetry in AI SDK (TypeScript)
DESCRIPTION: This TypeScript example demonstrates how to enable experimental telemetry for AI SDK function calls, specifically `generateText`. It shows how to set `isEnabled` to `true` and include optional `metadata` for richer tracing and monitoring with Traceloop.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/05-observability/traceloop.mdx#_snippet_1 LANGUAGE: typescript
CODE:
```
import { openai } '@ai-sdk/openai';
import { generateText } from 'ai'; const result = await generateText({ model: openai('gpt-4o-mini'), prompt: 'What is 2 + 2?', experimental_telemetry: { isEnabled: true, metadata: { query: 'weather', location: 'San Francisco', }, },
});
``` ---------------------------------------- TITLE: Client-Side AI Interaction Page in TSX
DESCRIPTION: This client-side component handles user input and displays the AI conversation. It uses `useState` for managing the input field, `useUIState` to display the conversation history, and `useActions` to call the `submitUserMessage` server action. The `handleSubmit` function sends user messages to the AI and updates the conversation UI with both user input and AI responses.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/04-multistep-interfaces.mdx#_snippet_3 LANGUAGE: TSX
CODE:
```
'use client'; import { useState } from 'react';
import { AI } from './ai';
import { useActions, useUIState } from 'ai/rsc'; export default function Page() { const [input, setInput] = useState<string>(''); const [conversation, setConversation] = useUIState<typeof AI>(); const { submitUserMessage } = useActions(); const handleSubmit = async (e: React.FormEvent<HTMLFormElement>) => { e.preventDefault(); setInput(''); setConversation(currentConversation => [ ...currentConversation, <div>{input}</div>, ]); const message = await submitUserMessage(input); setConversation(currentConversation => [...currentConversation, message]); }; return ( <div> <div> {conversation.map((message, i) => ( <div key={i}>{message}</div> ))} </div> <div> <form onSubmit={handleSubmit}> <input type="text" value={input} onChange={e => setInput(e.target.value)} /> <button>Send Message</button> </form> </div> </div> );
}
``` ---------------------------------------- TITLE: Sending Custom Data with AI Stream in Fastify (TypeScript)
DESCRIPTION: This Fastify route demonstrates sending custom data alongside an AI text stream using `createDataStream`. It allows writing initial data (`initialized call`) and merging the AI `streamText` result into the same data stream, with custom error handling for client-side error exposure.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/40-fastify.mdx#_snippet_2 LANGUAGE: ts
CODE:
```
import { openai } from '@ai-sdk/openai';
import { createDataStream, streamText } from 'ai';
import Fastify from 'fastify'; const fastify = Fastify({ logger: true }); fastify.post('/stream-data', async function (request, reply) { // immediately start streaming the response const dataStream = createDataStream({ execute: async dataStreamWriter => { dataStreamWriter.writeData('initialized call'); const result = streamText({ model: openai('gpt-4o'), prompt: 'Invent a new holiday and describe its traditions.', }); result.mergeIntoDataStream(dataStreamWriter); }, onError: error => { // Error messages are masked by default for security reasons. // If you want to expose the error message to the client, you can do so here: return error instanceof Error ? error.message : String(error); } }); // Mark the response as a v1 data stream: reply.header('X-Vercel-AI-Data-Stream', 'v1'); reply.header('Content-Type', 'text/plain; charset=utf-8'); return reply.send(dataStream);
}); fastify.listen({ port: 8080 });
``` ---------------------------------------- TITLE: Client-Side Rendering of Streamed UI from AI SDK RSC
DESCRIPTION: This snippet shows the simplified client-side rendering logic when using server-side UI streaming with AI SDK RSC. Instead of complex conditional rendering, the client simply iterates through messages and renders 'message.display', which directly contains the React components streamed from the server. This significantly reduces client-side complexity and boilerplate for managing diverse UIs.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/06-advanced/07-rendering-ui-with-language-models.mdx#_snippet_5 LANGUAGE: tsx
CODE:
```
return ( <div> {messages.map(message => ( <div>{message.display}</div> ))} </div>
);
``` ---------------------------------------- TITLE: Streaming Data with AI SDK in Express (pipeDataStreamToResponse)
DESCRIPTION: This Express route illustrates how to stream AI-generated text data to the client using the `pipeDataStreamToResponse` method from the AI SDK. It initializes an OpenAI `gpt-4o` model and pipes the stream result directly to the HTTP response object, enabling real-time data delivery.
SOURCE: https://github.com/vercel/ai/blob/main/content/cookbook/15-api-servers/20-express.mdx#_snippet_1 LANGUAGE: typescript
CODE:
```
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import express, { Request, Response } from 'express'; const app = express(); app.post('/', async (req: Request, res: Response) => { const result = streamText({ model: openai('gpt-4o'), prompt: 'Invent a new holiday and describe its traditions.', }); result.pipeDataStreamToResponse(res);
}); app.listen(8080, () => { console.log(`Example app listening on port ${8080}`);
});
``` ---------------------------------------- TITLE: Accessing Generated Image Data (TypeScript)
DESCRIPTION: After an image is generated, this snippet shows how to access the image data in different formats. The `image` object returned by `generateImage` provides properties like `base64` for base64 encoded string data and `uint8Array` for raw byte data, allowing flexible use of the generated image.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/03-ai-sdk-core/35-image-generation.mdx#_snippet_1 LANGUAGE: tsx
CODE:
```
const base64 = image.base64; // base64 image data
const uint8Array = image.uint8Array; // Uint8Array image data
``` ---------------------------------------- TITLE: Using AI SDK's useCompletion Hook in a React Component
DESCRIPTION: This React client component (`app/page.tsx`) showcases how to consume a completion stream from an API route using the AI SDK's `useCompletion` hook. It provides a simple UI to display the streamed completion, an input field for user prompts, and handles form submission to trigger the completion process. This component depends on the `@ai-sdk/react` package for its functionality.
SOURCE: https://github.com/vercel/ai/blob/main/content/providers/04-adapters/02-llamaindex.mdx#_snippet_1 LANGUAGE: TSX
CODE:
```
'use client'; import { useCompletion } from '@ai-sdk/react'; export default function Chat() { const { completion, input, handleInputChange, handleSubmit } = useCompletion(); return ( <div> {completion} <form onSubmit={handleSubmit}> <input value={input} onChange={handleInputChange} /> </form> </div> );
}
``` ---------------------------------------- TITLE: Reading Streamable Value on Client with AI SDK RSC in TypeScript
DESCRIPTION: This client-side React component demonstrates how to consume a streamable value created on the server. Upon a button click, it calls a server action (`runThread`) and then uses `readStreamableValue` to asynchronously iterate over and log each update received from the stream, providing real-time feedback to the client.
SOURCE: https://github.com/vercel/ai/blob/main/content/docs/05-ai-sdk-rsc/05-streaming-values.mdx#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { readStreamableValue } from 'ai/rsc';
import { runThread } from '@/actions'; export default function Page() { return ( <button onClick={async () => { const { status } = await runThread(); for await (const value of readStreamableValue(status)) { console.log(value); } }} > Ask </button> );
}
```