TITLE: Defining the Graph State (LangGraph.js)
DESCRIPTION: Defines the state schema for a LangGraph application using `@langchain/langgraph`'s `Annotation`. It specifies the types and default behavior for state variables like `question`, `generation`, and `documents`, which are used to manage the flow within the graph.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rag/langgraph_adaptive_rag_local.ipynb#_snippet_12 LANGUAGE: typescript
CODE:
```
import type { Document } from "@langchain/core/documents";
import { Annotation } from "@langchain/langgraph"; // This defines the agent state.
// Returned documents from a node will override the current
// "documents" value in the state object.
const GraphState = Annotation.Root({ question: Annotation<string>, generation: Annotation<string>, documents: Annotation<Document[]> ({ reducer: (_, y) => y, default: () => [], })
})
``` ---------------------------------------- TITLE: Create LangGraph Workflow (JS)
DESCRIPTION: Defines the nodes and edges for a LangGraph workflow using StateGraph. It includes functions for executing a plan step (executeStep), generating the initial plan (planStep), replanning based on results (replanStep), and determining if the workflow should end (shouldEnd). The graph connects these steps sequentially and conditionally loops back to the agent step if replanning indicates more work is needed, or ends if a final response is generated.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/plan-and-execute/plan-and-execute.ipynb#_snippet_11 LANGUAGE: JavaScript
CODE:
```
import { END, START, StateGraph } from "@langchain/langgraph";
import { RunnableConfig } from "@langchain/core/runnables"; async function executeStep( state: typeof PlanExecuteState.State, config?: RunnableConfig,
): Promise<Partial<typeof PlanExecuteState.State>> { const task = state.plan[0]; const input = { messages: [new HumanMessage(task)], }; const { messages } = await agentExecutor.invoke(input, config); return { pastSteps: [[task, messages[messages.length - 1].content.toString()]], plan: state.plan.slice(1), };
} async function planStep( state: typeof PlanExecuteState.State,
): Promise<Partial<typeof PlanExecuteState.State>> { const plan = await planner.invoke({ objective: state.input }); return { plan: plan.steps };
} async function replanStep( state: typeof PlanExecuteState.State,
): Promise<Partial<typeof PlanExecuteState.State>> { const output = await replanner.invoke({ input: state.input, plan: state.plan.join("\n"), pastSteps: state.pastSteps .map(([step, result]) => `${step}: ${result}`) .join("\n"), }); const toolCall = output[0]; if (toolCall.type == "response") { return { response: toolCall.args?.response }; } return { plan: toolCall.args?.steps };
} function shouldEnd(state: typeof PlanExecuteState.State) { return state.response ? "true" : "false";
} const workflow = new StateGraph(PlanExecuteState) .addNode("planner", planStep) .addNode("agent", executeStep) .addNode("replan", replanStep) .addEdge(START, "planner") .addEdge("planner", "agent") .addEdge("agent", "replan") .addConditionalEdges("replan", shouldEnd, { true: END, false: "agent", }); // Finally, we compile it!
// This compiles it into a LangChain Runnable,
// meaning you can use it as you would any other runnable
const app = workflow.compile(); ``` ---------------------------------------- TITLE: Grading Document Relevance in LangGraph (TypeScript)
DESCRIPTION: Determines whether the Agent should continue based on the relevance of retrieved documents. This function checks if the last message in the conversation is of type FunctionMessage, indicating that document retrieval has been performed. It then evaluates the relevance of these documents to the user's initial question using a predefined model and output parser. If the documents are relevant, the conversation is considered complete. Otherwise, the retrieval process is continued.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rag/langgraph_agentic_rag.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
/** * Determines whether the Agent should continue based on the relevance of retrieved documents. * This function checks if the last message in the conversation is of type FunctionMessage, indicating * that document retrieval has been performed. It then evaluates the relevance of these documents to the user's * initial question using a predefined model and output parser. If the documents are relevant, the conversation * is considered complete. Otherwise, the retrieval process is continued. * @param {typeof GraphState.State} state - The current state of the agent, including all messages. * @returns {Promise<Partial<typeof GraphState.State>>} - The updated state with the new message added to the list of messages. */
async function gradeDocuments(state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> { console.log("---GET RELEVANCE---"); const { messages } = state; const tool = { name: "give_relevance_score", description: "Give a relevance score to the retrieved documents.", schema: z.object({ binaryScore: z.string().describe("Relevance score 'yes' or 'no'"), }) } const prompt = ChatPromptTemplate.fromTemplate( `You are a grader assessing relevance of retrieved docs to a user question. Here are the retrieved docs: \n ------- \n {context} \n ------- \n Here is the user question: {question} If the content of the docs are relevant to the users question, score them as relevant. Give a binary score 'yes' or 'no' score to indicate whether the docs are relevant to the question. Yes: The docs are relevant to the question. No: The docs are not relevant to the question.`, ); const model = new ChatOpenAI({ model: "gpt-4o", temperature: 0, }).bindTools([tool], { tool_choice: tool.name, }); const chain = prompt.pipe(model); const lastMessage = messages[messages.length - 1]; const score = await chain.invoke({ question: messages[0].content as string, context: lastMessage.content as string, }); return { messages: [score] };
}
``` ---------------------------------------- TITLE: Enabling Agent Memory with LangChain/LangGraph Checkpointer (TypeScript)
DESCRIPTION: Demonstrates how to enable multi-turn conversations by providing a `checkpointer` instance (like `MemorySaver`) when initializing the agent. Invoking the agent with a unique `thread_id` in the configuration allows the agent's state, including message history, to be persisted and resumed across calls.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/agents.md#_snippet_4 LANGUAGE: TypeScript
CODE:
```
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { MemorySaver } from "@langchain/langgraph-checkpoint";
import { initChatModel } from "langchain/chat_models/universal"; // highlight-next-line
const checkpointer = new MemorySaver(); const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest");
const agent = createReactAgent({ llm, tools: [getWeather], // highlight-next-line checkpointer // (1)!
}); // Run the agent
// highlight-next-line
const config = { configurable: { thread_id: "1" } };
const sfResponse = await agent.invoke( { messages: [ { role: "user", content: "what is the weather in sf" } ] }, config // (2)!
);
const nyResponse = await agent.invoke( { messages: [ { role: "user", content: "what about new york?" } ] }, config
);
``` ---------------------------------------- TITLE: Returning Command for Handoff and State Update (TypeScript)
DESCRIPTION: Shows a LangGraph agent node function returning a Command object. The 'goto' property specifies the next node to execute, enabling handoffs between agents. The 'update' property allows modifying the graph's state. This is a core mechanism for controlling multi-agent workflows in LangGraph.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/multi_agent.md#_snippet_0 LANGUAGE: TypeScript
CODE:
```
const agent = (state: typeof StateAnnotation.State) => { const goto = getNextAgent(...) // 'agent' / 'another_agent' return new Command({ // Specify which agent to call next goto: goto, // Update the graph state update: { foo: "bar", } });
};
``` ---------------------------------------- TITLE: Define Travel Advisor Agent Node (TypeScript)
DESCRIPTION: Implements the travelAdvisor agent function for the LangGraph state machine. It constructs a system prompt for the agent, calls the callLlm function with potential target agents (sightseeingAdvisor, hotelAdvisor), processes the structured LLM response, and returns a Command object specifying the next node (goto) and state updates (update). It routes 'finish' to 'human'.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/multi-agent-multi-turn-convo.ipynb#_snippet_5 LANGUAGE: TypeScript
CODE:
```
async function travelAdvisor( state: typeof MessagesAnnotation.State
): Promise<Command> { const systemPrompt = "You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc). " + "If you need specific sightseeing recommendations, ask 'sightseeingAdvisor' for help. " + "If you need hotel recommendations, ask 'hotelAdvisor' for help. " + "If you have enough information to respond to the user, return 'finish'. " + "Never mention other agents by name."; const messages = [{"role": "system", "content": systemPrompt}, ...state.messages] as BaseMessage[]; const targetAgentNodes = ["sightseeingAdvisor", "hotelAdvisor"]; const response = await callLlm(messages, targetAgentNodes); const aiMsg = {"role": "ai", "content": response.response, "name": "travelAdvisor"}; let goto = response.goto; if (goto === "finish") { goto = "human"; } return new Command({goto, update: { "messages": [aiMsg] } });
}
``` ---------------------------------------- TITLE: Defining State Modifier for Dynamic Prompting (TypeScript)
DESCRIPTION: Defines a `stateModifier` function that takes the current graph state as input. If user information is present in the state, it constructs a system message including the user's name and location and prepends it to the existing message history, allowing the LLM to personalize its response based on the user data.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/update-state-from-tools.ipynb#_snippet_5 LANGUAGE: TypeScript
CODE:
```
const stateModifier = (state: typeof StateAnnotation.State) => { const userInfo = state.userInfo; if (userInfo == null) { return state.messages; } const systemMessage = `User name is ${userInfo.name}. User lives in ${userInfo.location}`; return [{ role: "system", content: systemMessage, }, ...state.messages];
};
``` ---------------------------------------- TITLE: Building Evaluator-Optimizer Graph with StateGraph (TypeScript)
DESCRIPTION: This snippet demonstrates how to build the evaluator-optimizer workflow using LangGraph's StateGraph API in TypeScript. It defines the graph state, a Zod schema for structured evaluation output, asynchronous nodes for content generation and evaluation, a conditional edge function to route based on feedback, and compiles the graph.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_19 LANGUAGE: TypeScript
CODE:
```
import { z } from "zod";
import { Annotation, StateGraph } from "@langchain/langgraph"; // Graph state
const StateAnnotation = Annotation.Root({ joke: Annotation<string>, topic: Annotation<string>, feedback: Annotation<string>, funnyOrNot: Annotation<string>,
}); // Schema for structured output to use in evaluation
const feedbackSchema = z.object({ grade: z.enum(["funny", "not funny"]).describe( "Decide if the joke is funny or not." ), feedback: z.string().describe( "If the joke is not funny, provide feedback on how to improve it." ),
}); // Augment the LLM with schema for structured output
const evaluator = llm.withStructuredOutput(feedbackSchema); // Nodes
async function llmCallGenerator(state: typeof StateAnnotation.State) { // LLM generates a joke let msg; if (state.feedback) { msg = await llm.invoke( `Write a joke about ${state.topic} but take into account the feedback: ${state.feedback}` ); } else { msg = await llm.invoke(`Write a joke about ${state.topic}`); } return { joke: msg.content };
} async function llmCallEvaluator(state: typeof StateAnnotation.State) { // LLM evaluates the joke const grade = await evaluator.invoke(`Grade the joke ${state.joke}`); return { funnyOrNot: grade.grade, feedback: grade.feedback };
} // Conditional edge function to route back to joke generator or end based upon feedback from the evaluator
function routeJoke(state: typeof StateAnnotation.State) { // Route back to joke generator or end based upon feedback from the evaluator if (state.funnyOrNot === "funny") { return "Accepted"; } else if (state.funnyOrNot === "not funny") { return "Rejected + Feedback"; }
} // Build workflow
const optimizerWorkflow = new StateGraph(StateAnnotation) .addNode("llmCallGenerator", llmCallGenerator) .addNode("llmCallEvaluator", llmCallEvaluator) .addEdge("__start__", "llmCallGenerator") .addEdge("llmCallGenerator", "llmCallEvaluator") .addConditionalEdges( "llmCallEvaluator", routeJoke, { // Name returned by routeJoke : Name of next node to visit "Accepted": "__end__", "Rejected + Feedback": "llmCallGenerator", } ) .compile(); // Invoke
const state = await optimizerWorkflow.invoke({ topic: "Cats" });
console.log(state.joke);
``` ---------------------------------------- TITLE: Trimming Messages by Token Count (Typescript)
DESCRIPTION: Shows how to use the `trimMessages` utility function from `@langchain/core/messages` to manage conversation history length. It configures the utility to keep the last messages up to a specified token limit (`maxTokens`), using a model's token counter, while adhering to common chat model constraints on message sequence (`startOn`, `endOn`, `includeSystem`).
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/memory.md#_snippet_4 LANGUAGE: Typescript
CODE:
```
import { trimMessages } from "@langchain/core/messages";
import { ChatOpenAI } from "@langchain/openai"; trimMessages(messages, { // Keep the last <= n_count tokens of the messages. strategy: "last", // Remember to adjust based on your model // or else pass a custom token_encoder tokenCounter: new ChatOpenAI({ modelName: "gpt-4" }), // Remember to adjust based on the desired conversation // length maxTokens: 45, // Most chat models expect that chat history starts with either: // (1) a HumanMessage or // (2) a SystemMessage followed by a HumanMessage startOn: "human", // Most chat models expect that chat history ends with either: // (1) a HumanMessage or // (2) a ToolMessage endOn: ["human", "tool"], // Usually, we want to keep the SystemMessage // if it's present in the original history. // The SystemMessage has special instructions for the model. includeSystem: true,
});
``` ---------------------------------------- TITLE: Handle Human Review of Tool Calls in LangGraph (TypeScript)
DESCRIPTION: This TypeScript function serves as a node in a LangGraph workflow designed for human review of LLM-generated tool calls. It utilizes the `interrupt` command to pause the graph's execution, prompting for human input. Based on the human's response ('continue', 'update', or 'feedback'), it returns a `Command` object to control the graph's flow, either proceeding with the tool call, updating the state with a modified message, or adding feedback to the state before returning to the LLM. It requires dependencies from `@langchain/langgraph`.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/human_in_the_loop.md#_snippet_6 LANGUAGE: typescript
CODE:
```
import { interrupt, Command } from "@langchain/langgraph"; function humanReviewNode(state: typeof GraphAnnotation.State): Command { // This is the value we'll be providing via Command.resume(<human_review>) const humanReview = interrupt({ question: "Is this correct?", // Surface tool calls for review tool_call: toolCall, }); const [reviewAction, reviewData] = humanReview; // Approve the tool call and continue if (reviewAction === "continue") { return new Command({ goto: "run_tool" }); } // Modify the tool call manually and then continue else if (reviewAction === "update") { const updatedMsg = getUpdatedMsg(reviewData); // Remember that to modify an existing message you will need // to pass the message with a matching ID. return new Command({ goto: "run_tool", update: { messages: [updatedMsg] }, }); } // Give natural language feedback, and then pass that back to the agent else if (reviewAction === "feedback") { const feedbackMsg = getFeedbackMsg(reviewData); return new Command({ goto: "call_llm", update: { messages: [feedbackMsg] }, }); }
}
``` ---------------------------------------- TITLE: Binding Tools to OpenAI Model - TypeScript
DESCRIPTION: Converts the defined tools into the OpenAI function calling format and binds them to the ChatOpenAI model instance, making the model aware of the available tools. Requires @langchain/core/utils/function_calling.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/chat_agent_executor_with_function_calling/base.ipynb#_snippet_5 LANGUAGE: typescript
CODE:
```
import { convertToOpenAIFunction } from "@langchain/core/utils/function_calling"; const toolsAsOpenAIFunctions = tools.map((tool) => convertToOpenAIFunction(tool)
);
const newModel = model.bind({ functions: toolsAsOpenAIFunctions,
});
``` ---------------------------------------- TITLE: Bind Tools to Chat Model (Python)
DESCRIPTION: Associates the defined tools with the initialized `ChatOpenAI` model using the `bindTools` method. This makes the model aware of the available tools and enables it to generate tool calls in the correct format.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/force-calling-a-tool-first.ipynb#_snippet_5 LANGUAGE: python
CODE:
```
const boundModel = model.bindTools(tools);
``` ---------------------------------------- TITLE: Setting up a ReAct Agent Workflow with LangGraph and Anthropic (TypeScript)
DESCRIPTION: This snippet defines the core components of a ReAct agent workflow using LangGraph. It includes setting up a fake `search` tool, initializing an Anthropic model, defining nodes for agent action (`callModel`) and tool execution (`toolNode`), and configuring the graph's conditional edges to cycle between the agent and tool based on the model's response. It also sets up memory and compiles the workflow into a runnable application.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/edit-graph-state.ipynb#_snippet_8 LANGUAGE: TypeScript
CODE:
```
// Set up the tool
import { ChatAnthropic } from "@langchain/anthropic";
import { tool } from "@langchain/core/tools";
import { StateGraph, START, END, MessagesAnnotation } from "@langchain/langgraph";
import { MemorySaver } from "@langchain/langgraph";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import { AIMessage } from "@langchain/core/messages";
import { z } from "zod"; const search = tool((_) => { return "It's sunny in San Francisco, but you better look out if you're a Gemini 😈.";
}, { name: "search", description: "Call to surf the web.", schema: z.string(),
}) const tools = [search]
const toolNode = new ToolNode(tools) // Set up the model
const model = new ChatAnthropic({ model: "claude-3-5-sonnet-20240620" })
const modelWithTools = model.bindTools(tools) // Define nodes and conditional edges // Define the function that determines whether to continue or not
function shouldContinue(state: typeof MessagesAnnotation.State): "action" | typeof END { const lastMessage = state.messages[state.messages.length - 1]; // If there is no function call, then we finish if (lastMessage && !(lastMessage as AIMessage).tool_calls?.length) { return END; } // Otherwise if there is, we continue return "action";
} // Define the function that calls the model
async function callModel(state: typeof MessagesAnnotation.State): Promise<Partial<typeof MessagesAnnotation.State>> { const messages = state.messages; const response = await modelWithTools.invoke(messages); // We return an object with a messages property, because this will get added to the existing list return { messages: [response] };
} // Define a new graph
const workflow = new StateGraph(MessagesAnnotation) // Define the two nodes we will cycle between .addNode("agent", callModel) .addNode("action", toolNode) // We now add a conditional edge .addConditionalEdges( // First, we define the start node. We use `agent`. // This means these are the edges taken after the `agent` node is called. "agent", // Next, we pass in the function that will determine which node is called next. shouldContinue ) // We now add a normal edge from `action` to `agent`. // This means that after `action` is called, `agent` node is called next. .addEdge("action", "agent") // Set the entrypoint as `agent` // This means that this node is the first one called .addEdge(START, "agent"); // Setup memory
const memory = new MemorySaver(); // Finally, we compile it!
// This compiles it into a LangChain Runnable,
// meaning you can use it as you would any other runnable
const app = workflow.compile({ checkpointer: memory, interruptBefore: ["action"]
});
``` ---------------------------------------- TITLE: Define and Compile StateGraph (JS)
DESCRIPTION: Defines the LangGraph workflow using a StateGraph. It sets up nodes for agent calls and tool execution, defines edges including a conditional edge based on the model's response, and compiles the graph with an in-memory checkpointer for state saving.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/time-travel.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
import { END, START, StateGraph } from "@langchain/langgraph";
import { AIMessage } from "@langchain/core/messages";
import { RunnableConfig } from "@langchain/core/runnables";
import { MemorySaver } from "@langchain/langgraph"; const routeMessage = (state: typeof StateAnnotation.State) => { const { messages } = state; const lastMessage = messages[messages.length - 1] as AIMessage; // If no tools are called, we can finish (respond to the user) if (!lastMessage?.tool_calls?.length) { return END; } // Otherwise if there is, we continue and call the tools return "tools";
}; const callModel = async ( state: typeof StateAnnotation.State, config?: RunnableConfig,
): Promise<Partial<typeof StateAnnotation.State>> => { const { messages } = state; const response = await boundModel.invoke(messages, config); return { messages: [response] };
}; const workflow = new StateGraph(StateAnnotation) .addNode("agent", callModel) .addNode("tools", toolNode) .addEdge(START, "agent") .addConditionalEdges("agent", routeMessage) .addEdge("tools", "agent"); // Here we only save in-memory
let memory = new MemorySaver();
const graph = workflow.compile({ checkpointer: memory });
``` ---------------------------------------- TITLE: Invoking Graph with Specific Thread and User ID (TypeScript)
DESCRIPTION: Demonstrates how to invoke a LangGraph graph with a specific configuration, including a `thread_id` and `user_id`. It shows how to pass this configuration to the `graph.stream` method and iterate over the streamed updates, illustrating how the same user's memories can be accessed across different threads.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/persistence.md#_snippet_17 LANGUAGE: TypeScript
CODE:
```
// Invoke the graph
const config = { configurable: { thread_id: "2", user_id: "1" } }; // Let's say hi again
const stream = await graph.stream( { messages: [{ role: "user", content: "hi, tell me about my memories" }] }, { ...config, streamMode: "updates" },
); for await (const update of stream) { console.log(update);
}
``` ---------------------------------------- TITLE: Defining Custom State Schema and Invoking with Initial State (TypeScript)
DESCRIPTION: Illustrates how to define a custom state schema using Annotation.Root to extend the default state (like messages) with custom fields (userName). It then shows how to configure the agent with this schema and pass initial values for the custom state when invoking the agent.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/context.md#_snippet_1 LANGUAGE: TypeScript
CODE:
```
const CustomState = Annotation.Root({ ...MessagesAnnotation.spec, userName: Annotation<string>,
}); const agent = createReactAgent({ // Other agent parameters... // highlight-next-line stateSchema: CustomState,
}) await agent.invoke( // highlight-next-line { messages: "hi!", userName: "Jane" }
)
``` ---------------------------------------- TITLE: Create a Simple ReAct Agent with LangGraph.js (TypeScript)
DESCRIPTION: Demonstrates how to create a basic ReAct agent using @langchain/langgraph/prebuilt, integrating a custom tool and an Anthropic chat model. It shows how to define a tool, initialize the model and agent, and invoke the agent with a message.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/README.md#_snippet_1 LANGUAGE: ts
CODE:
```
// npm install @langchain-anthropic
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { ChatAnthropic } from "@langchain/anthropic";
import { tool } from "@langchain/core/tools"; import { z } from "zod"; const search = tool(async ({ query }) => { if (query.toLowerCase().includes("sf") || query.toLowerCase().includes("san francisco")) { return "It's 60 degrees and foggy." } return "It's 90 degrees and sunny."
}, { name: "search", description: "Call to surf the web.", schema: z.object({ query: z.string().describe("The query to use in your search."), }),
}); const model = new ChatAnthropic({ model: "claude-3-7-sonnet-latest"
}); const agent = createReactAgent({ llm: model, tools: [search],
}); const result = await agent.invoke( { messages: [{ role: "user", content: "what is the weather in sf" }] }
);
``` ---------------------------------------- TITLE: Define and Compile Top-Level Orchestration Graph in LangGraphJS
DESCRIPTION: Constructs the main LangGraph StateGraph that orchestrates the previously defined `researchChain` and `authoringChain` (aliased as 'PaperWritingTeam'). It adds nodes for each team and the supervisor, defines edges for flow control, including conditional routing based on the supervisor's decision, and compiles the complete graph.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb#_snippet_17 LANGUAGE: JavaScript
CODE:
```
const superGraph = new StateGraph(State) .addNode("ResearchTeam", async (input) => { const getMessagesResult = await getMessages.invoke(input); const researchChainResult = await researchChain.invoke({ messages: getMessagesResult.messages, }); const joinGraphResult = await joinGraph.invoke({ messages: researchChainResult.messages, }); }) .addNode("PaperWritingTeam", getMessages.pipe(authoringChain).pipe(joinGraph)) .addNode("supervisor", supervisorNode) .addEdge("ResearchTeam", "supervisor") .addEdge("PaperWritingTeam", "supervisor") .addConditionalEdges("supervisor", (x) => x.next, { PaperWritingTeam: "PaperWritingTeam", ResearchTeam: "ResearchTeam", FINISH: END, }) .addEdge(START, "supervisor"); const compiledSuperGraph = superGraph.compile();
``` ---------------------------------------- TITLE: Initializing a Basic LangGraph Agent (TypeScript)
DESCRIPTION: Demonstrates how to create a basic LangGraph agent using `createReactAgent`. It shows defining a custom tool with schema validation, initializing a chat model, providing the tool and a static prompt to the agent, and invoking the agent with a user message.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/agents.md#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { initChatModel } from "langchain/chat_models/universal";
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const getWeather = tool( // (1)! async (input: { city: string }) => { return `It's always sunny in ${input.city}!`; }, { name: "getWeather", schema: z.object({ city: z.string().describe("The city to get the weather for"), }), description: "Get weather for a given city.", }
); const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest"); // (2)!
const agent = createReactAgent({ llm, tools: [getWeather], // (3)! prompt: "You are a helpful assistant" // (4)!
}) // Run the agent
await agent.invoke( { messages: [ { role: "user", content: "what is the weather in sf" } ] }
);
``` ---------------------------------------- TITLE: Defining Agent Entrypoint (Langchain.js)
DESCRIPTION: Defines the main 'agent' entrypoint using '@langchain/langgraph'. It implements an iterative loop that calls the model, executes any tool calls generated, appends results, and repeats until the model response contains no tool calls, effectively implementing a ReAct-like loop. It uses a 'MemorySaver' for checkpointing.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/wait-user-input-functional.ipynb#_snippet_9 LANGUAGE: javascript
CODE:
```
import { entrypoint, addMessages, MemorySaver } from "@langchain/langgraph"; const agent = entrypoint({ name: "agent", checkpointer: new MemorySaver(),
}, async (messages: BaseMessageLike[]) => { let currentMessages = messages; let llmResponse = await callModel(currentMessages); while (true) { if (!llmResponse.tool_calls?.length) { break; } // Execute tools const toolResults = await Promise.all( llmResponse.tool_calls.map((toolCall) => { return callTool(toolCall); }) ); // Append to message list currentMessages = addMessages(currentMessages, [llmResponse, ...toolResults]); // Call model again llmResponse = await callModel(currentMessages); } return llmResponse;
});
``` ---------------------------------------- TITLE: Define Agent Supervisor Chain (TypeScript)
DESCRIPTION: Sets up a supervisor chain using LangChain's ChatAnthropic model and a custom routing tool. The supervisor determines the next worker ('researcher', 'chart_generator') or signals 'FINISH' based on the conversation history.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/multi_agent/agent_supervisor.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
import { z } from "zod";
import { ChatAnthropic } from "@langchain/anthropic"; import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts"; const members = ["researcher", "chart_generator"] as const; const systemPrompt = "You are a supervisor tasked with managing a conversation between the" + " following workers: {members}. Given the following user request," + " respond with the worker to act next. Each worker will perform a" + " task and respond with their results and status. When finished," + " respond with FINISH.";
const options = [END, ...members]; // Define the routing function
const routingTool = { name: "route", description: "Select the next role.", schema: z.object({ next: z.enum([END, ...members]), }),
} const prompt = ChatPromptTemplate.fromMessages([ ["system", systemPrompt], new MessagesPlaceholder("messages"), [ "human", "Given the conversation above, who should act next?" + " Or should we FINISH? Select one of: {options}", ],
]); const formattedPrompt = await prompt.partial({ options: options.join(", "), members: members.join(", "),
}); const llm = new ChatAnthropic({ modelName: "claude-3-5-sonnet-20241022", temperature: 0,
}); const supervisorChain = formattedPrompt .pipe(llm.bindTools( [routingTool], { tool_choice: "route", }, )) // select the first one .pipe((x) => (x.tool_calls[0].args));
``` ---------------------------------------- TITLE: Summarizing Conversation History (Typescript)
DESCRIPTION: Implements a LangGraph node function `summarizeConversation` that processes the current state. It retrieves the existing summary, constructs a prompt including the current messages, invokes a chat model (e.g., ChatOpenAI) to generate an updated summary, and returns the new summary along with instructions to remove older messages from the state.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/memory.md#_snippet_3 LANGUAGE: Typescript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage, RemoveMessage } from "@langchain/core/messages"; type State = typeof MyGraphAnnotation.State; async function summarizeConversation(state: State) { // First, we get any existing summary const summary = state.summary || ""; // Create our summarization prompt let summaryMessage: string; if (summary) { // A summary already exists summaryMessage = `This is a summary of the conversation to date: ${summary}\n\n` + "Extend the summary by taking into account the new messages above:"; } else { summaryMessage = "Create a summary of the conversation above:"; } // Add prompt to our history const messages = [ ...state.messages, new HumanMessage({ content: summaryMessage }), ]; // Assuming you have a ChatOpenAI model instance const model = new ChatOpenAI(); const response = await model.invoke(messages); // Delete all but the 2 most recent messages const deleteMessages = state.messages .slice(0, -2) .map((m) => new RemoveMessage({ id: m.id })); return { summary: response.content, messages: deleteMessages, };
}
``` ---------------------------------------- TITLE: Compiling a LangGraph Graph (TypeScript)
DESCRIPTION: This snippet demonstrates how to compile a LangGraph graph using the `.compile()` method on a `graphBuilder` instance. Compilation performs basic structural checks on the graph's structure and allows for the specification of runtime arguments such as checkpointers and breakpoints. It is a mandatory step that must be completed before the graph can be used for execution.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/low_level.md#_snippet_0 LANGUAGE: TypeScript
CODE:
```
const graph = graphBuilder.compile(...);
``` ---------------------------------------- TITLE: Streaming Events with LangGraph and OpenAI (TypeScript)
DESCRIPTION: Demonstrates setting up a simple LangGraph workflow that calls an OpenAI chat model and then iterates over the events emitted during its execution using the `.streamEvents` method. It shows how to import necessary modules, define a node function, build the graph, compile it, and process the streamed events.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/streaming.md#_snippet_0 LANGUAGE: typescript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { StateGraph, MessagesAnnotation } from "langgraph"; const model = new ChatOpenAI({ model: "gpt-4-turbo-preview" }); function callModel(state: typeof MessagesAnnotation.State) { const response = model.invoke(state.messages); return { messages: response };
} const workflow = new StateGraph(MessagesAnnotation) .addNode("callModel", callModel) .addEdge("start", "callModel") .addEdge("callModel", "end");
const app = workflow.compile(); const inputs = [{ role: "user", content: "hi!" }]; for await (const event of app.streamEvents( { messages: inputs }, { version: "v2" }
)) { const kind = event.event; console.log(`${kind}: ${event.name}`);
}
``` ---------------------------------------- TITLE: Defining Supervisor Agent Graph with LangGraph.js (TypeScript)
DESCRIPTION: This snippet illustrates a supervisor-based multi-agent architecture using LangGraph.js. A central 'supervisor' node (an LLM) directs traffic between specialized agent nodes. Agents perform tasks and then return control to the supervisor, which determines the subsequent action or termination. It utilizes `StateGraph` and `Command` for state management and routing.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/multi_agent.md#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { StateGraph, MessagesAnnotation, Command,
} from "@langchain/langgraph";
import { ChatOpenAI } from "@langchain/openai"; const model = new ChatOpenAI({ model: "gpt-4o-mini",
}); const supervisor = async (state: typeof MessagesAnnotation.State) => { // you can pass relevant parts of the state to the LLM (e.g., state.messages) // to determine which agent to call next. a common pattern is to call the model // with a structured output (e.g. force it to return an output with a "next_agent" field) const response = await model.withStructuredOutput(...).invoke(...); // route to one of the agents or exit based on the supervisor's decision // if the supervisor returns "__end__", the graph will finish execution return new Command({ goto: response.next_agent, });
}; const agent1 = async (state: typeof MessagesAnnotation.State) => { // you can pass relevant parts of the state to the LLM (e.g., state.messages) // and add any additional logic (different models, custom prompts, structured output, etc.) const response = await model.invoke(...); return new Command({ goto: "supervisor", update: { messages: [response], }, });
}; const agent2 = async (state: typeof MessagesAnnotation.State) => { const response = await model.invoke(...); return new Command({ goto: "supervisor", update: { messages: [response], }, });
}; const graph = new StateGraph(MessagesAnnotation) .addNode("supervisor", supervisor, { ends: ["agent1", "agent2", "__end__"], }) .addNode("agent1", agent1, { ends: ["supervisor"], }) .addNode("agent2", agent2, { ends: ["supervisor"], }) .addEdge("__start__", "supervisor") .compile();
``` ---------------------------------------- TITLE: Define Network Graph Entrypoint (JS/TS)
DESCRIPTION: Defines the main entrypoint for the LangGraph network. It manages the conversation flow by iteratively calling either the travel or hotel advisor agent. The decision to switch agents is based on the presence and name of tool calls in the last AI message returned by the current agent. The loop continues until an agent returns a message without tool calls.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/multi-agent-network-functional.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
const networkGraph = entrypoint( "networkGraph", async (messages: BaseMessageLike[]) => { // Converts inputs to LangChain messages as a side-effect let currentMessages = addMessages([], messages); let callActiveAgent = callTravelAdvisor; while (true) { const agentMessages = await callActiveAgent(currentMessages); currentMessages = addMessages(currentMessages, agentMessages); // Find the last AI message // If one of the handoff tools is called, the last message returned // by the agent will be a ToolMessage because we set them to have // "returnDirect: true". This means that the last AIMessage will // have tool calls. // Otherwise, the last returned message will be an AIMessage with // no tool calls, which means we are ready for new input. const aiMsg = [...agentMessages].reverse() .find((m): m is AIMessage => m.getType() === "ai"); // If no tool calls, we're done if (!aiMsg?.tool_calls?.length) { break; } // Get the last tool call and determine next agent const toolCall = aiMsg.tool_calls.at(-1)!; if (toolCall.name === "transferToTravelAdvisor") { callActiveAgent = callTravelAdvisor; } else if (toolCall.name === "transferToHotelAdvisor") { callActiveAgent = callHotelAdvisor; } else { throw new Error(`Expected transfer tool, got '${toolCall.name}'`); } } return messages; });
``` ---------------------------------------- TITLE: Define LangGraphJS Task for Routing LLM Calls (TypeScript)
DESCRIPTION: Defines a LangGraphJS task named 'router' that uses an LLM to determine the next step based on user input. It instructs the LLM to route the input to 'story', 'joke', or 'poem' and returns the chosen step.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_12 LANGUAGE: TypeScript
CODE:
```
const llmCallRouter = task("router", async (input: string) => { const decision = await router.invoke([ { role: "system", content: "Route the input to story, joke, or poem based on the user's request." }, { role: "user", content: input }, ]); return decision.step;
});
``` ---------------------------------------- TITLE: Define LangGraph Workflow
DESCRIPTION: Constructs the LangGraph workflow using `StateGraph`. It defines two nodes ('agent' and 'tools'), sets up the initial edge from `START` to 'agent', adds a conditional edge from 'agent' based on the `routeMessage` function, and adds an edge from 'tools' back to 'agent'. Includes the `routeMessage` and `callModel` functions.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/stream-values.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { END, START, StateGraph } from "@langchain/langgraph";
import { AIMessage } from "@langchain/core/messages"; const routeMessage = (state: typeof StateAnnotation.State) => { const { messages } = state; const lastMessage = messages[messages.length - 1] as AIMessage; // If no tools are called, we can finish (respond to the user) if (!lastMessage?.tool_calls?.length) { return END; } // Otherwise if there is, we continue and call the tools return "tools";
}; const callModel = async ( state: typeof StateAnnotation.State,
) => { // For versions of @langchain/core < 0.2.3, you must call `.stream()` // and aggregate the message from chunks instead of calling `.invoke()`. For newer versions, invoke is fine. const { messages } = state; const responseMessage = await boundModel.invoke(messages); return { messages: [responseMessage] };
}; const workflow = new StateGraph(StateAnnotation) .addNode("agent", callModel) .addNode("tools", toolNode) .addEdge(START, "agent") .addConditionalEdges("agent", routeMessage) .addEdge("tools", "agent"); const graph = workflow.compile();
``` ---------------------------------------- TITLE: Define LangGraph ReAct Agent Workflow (JS)
DESCRIPTION: Imports necessary LangGraph and LangChain components, defines a model and tools, creates functions for routing messages and calling the model, and constructs the StateGraph workflow with nodes and edges. It also compiles the graph with memory and store.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/pass-run-time-values-to-tools.ipynb#_snippet_9 LANGUAGE: JavaScript
CODE:
```
import { END, START, StateGraph, MemorySaver, InMemoryStore,
} from "@langchain/langgraph";
import { AIMessage } from "@langchain/core/messages";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import { ChatOpenAI } from "@langchain/openai"; const model = new ChatOpenAI({ model: "gpt-4o" }); const tools = [getFavoritePets, updateFavoritePets]; const routeMessage = (state: typeof MessagesAnnotation.State) => { const { messages } = state; const lastMessage = messages[messages.length - 1] as AIMessage; // If no tools are called, we can finish (respond to the user) if (!lastMessage?.tool_calls?.length) { return END; } // Otherwise if there is, we continue and call the tools return "tools";
}; const callModel = async (state: typeof MessagesAnnotation.State) => { const { messages } = state; const modelWithTools = model.bindTools(tools); const responseMessage = await modelWithTools.invoke([ { role: "system", content: "You are a personal assistant. Store any preferences the user tells you about." }, ...messages ]); return { messages: [responseMessage] };
}; const workflow = new StateGraph(MessagesAnnotation) .addNode("agent", callModel) .addNode("tools", new ToolNode(tools)) .addEdge(START, "agent") .addConditionalEdges("agent", routeMessage) .addEdge("tools", "agent"); const memory = new MemorySaver();
const store = new InMemoryStore(); const graph = workflow.compile({ checkpointer: memory, store: store });
``` ---------------------------------------- TITLE: Building a ReAct Agent Workflow with LangGraph (TypeScript)
DESCRIPTION: This snippet defines the core workflow for a ReAct-style agent using LangGraph. It sets up a tool, initializes the LLM and tool node, defines functions for model invocation and conditional routing, and constructs the state graph with nodes and edges. Finally, it compiles the graph into a runnable application with a memory checkpointer.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/delete-messages.ipynb#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { tool } from "@langchain/core/tools";
import { MemorySaver } from "@langchain/langgraph-checkpoint";
import { MessagesAnnotation, StateGraph, START, END } from "@langchain/langgraph";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import { z } from "zod"; const memory = new MemorySaver(); const search = tool((_) => { // This is a placeholder for the actual implementation // Don't let the LLM know this though 😊 return [ "It's sunny in San Francisco, but you better look out if you're a Gemini 😈.", ];
}, { name: "search", description: "Call to surf the web.", schema: z.object({ query: z.string(), })
}); const tools = [search];
const toolNode = new ToolNode<typeof MessagesAnnotation.State>(tools);
const model = new ChatOpenAI({ model: "gpt-4o" });
const boundModel = model.bindTools(tools); function shouldContinue(state: typeof MessagesAnnotation.State): "action" | typeof END { const lastMessage = state.messages[state.messages.length - 1]; if ( "tool_calls" in lastMessage && Array.isArray(lastMessage.tool_calls) && lastMessage.tool_calls.length ) { return "action"; } // If there is no tool call, then we finish return END;
} // Define the function that calls the model
async function callModel(state: typeof MessagesAnnotation.State) { const response = await boundModel.invoke(state.messages); return { messages: [response] };
} // Define a new graph
const workflow = new StateGraph(MessagesAnnotation) // Define the two nodes we will cycle between .addNode("agent", callModel) .addNode("action", toolNode) // Set the entrypoint as `agent` // This means that this node is the first one called .addEdge(START, "agent") // We now add a conditional edge .addConditionalEdges( // First, we define the start node. We use `agent`. // This means these are the edges taken after the `agent` node is called. "agent", // Next, we pass in the function that will determine which node is called next. shouldContinue ) // We now add a normal edge from `tools` to `agent`. // This means that after `tools` is called, `agent` node is called next. .addEdge("action", "agent"); // Finally, we compile it!
// This compiles it into a LangChain Runnable,
// meaning you can use it as you would any other runnable
const app = workflow.compile({ checkpointer: memory });
``` ---------------------------------------- TITLE: Update Graph State with Values (TypeScript)
DESCRIPTION: Updates the state of a LangGraph graph for a specified thread using `graph.updateState`. The provided `values` object is processed according to the state schema's reducers. Channels without reducers are overwritten, while channels with reducers (like the 'bar' channel shown in the example) use the reducer logic for the update.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/persistence.md#_snippet_8 LANGUAGE: typescript
CODE:
```
await graph.updateState(config, { foo: "2", bar: ["b"] });
``` ---------------------------------------- TITLE: Running LangGraph Agent Stream with User ID (TypeScript)
DESCRIPTION: Executes the `agent.stream` method with an initial user message and provides the user ID "abc123" in the `configurable` object. This triggers the agent, which should use the `lookupUserInfo` tool to fetch data for this ID and potentially personalize the response based on the `stateModifier`. The code then iterates and logs the chunks from the stream.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/update-state-from-tools.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
const stream = await agent.stream({ messages: [{ role: "user", content: "hi, what should i do this weekend?", }], }, { // provide user ID in the config configurable: { user_id: "abc123" }
}); for await (const chunk of stream) { console.log(chunk);
}
``` ---------------------------------------- TITLE: Creating LangGraph ReAct Agent with Structured Weather Output (TypeScript)
DESCRIPTION: Provides a complete example of setting up a LangGraph ReAct agent. It defines a simple weather tool using `@langchain/core/tools`, specifies a Zod schema for the desired structured weather output, and initializes the agent with an OpenAI chat model, the tool, and the structured output schema.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/react-return-structured-output.ipynb#_snippet_3 LANGUAGE: typescript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const weatherTool = tool( async (input): Promise<string> => { if (input.city === "nyc") { return "It might be cloudy in nyc"; } else if (input.city === "sf") { return "It's always sunny in sf"; } else { throw new Error("Unknown city"); } }, { name: "get_weather", description: "Use this to get weather information.", schema: z.object({ city: z.enum(["nyc", "sf"]).describe("The city to get weather for"), }), }
); const WeatherResponseSchema = z.object({ conditions: z.string().describe("Weather conditions"),
}); const tools = [weatherTool]; const agent = createReactAgent({ llm: new ChatOpenAI({ model: "gpt-4o", temperature: 0 }), tools: tools, responseFormat: WeatherResponseSchema,
});
``` ---------------------------------------- TITLE: Using PostgresSaver for Checkpointing in LangGraph.js (TypeScript)
DESCRIPTION: Demonstrates the basic usage of the PostgresSaver, including importing the class, defining configuration objects, initializing the saver from a connection string, performing the necessary setup, defining a sample checkpoint structure, and using the put, get, and list methods to interact with the database.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/libs/checkpoint-postgres/README.md#_snippet_0 LANGUAGE: ts
CODE:
```
import { PostgresSaver } from "@langchain/langgraph-checkpoint-postgres"; const writeConfig = { configurable: { thread_id: "1", checkpoint_ns: "" }
};
const readConfig = { configurable: { thread_id: "1" }
}; // you can optionally pass a configuration object as the second parameter
const checkpointer = PostgresSaver.fromConnString("postgresql://...", { schema: "schema_name" // defaults to "public"
}); // You must call .setup() the first time you use the checkpointer:
await checkpointer.setup(); const checkpoint = { v: 1, ts: "2024-07-31T20:14:19.804150+00:00", id: "1ef4f797-8335-6428-8001-8a1503f9b875", channel_values: { my_key: "meow", node: "node" }, channel_versions: { __start__: 2, my_key: 3, "start:node": 3, node: 3 }, versions_seen: { __input__: {}, __start__: { __start__: 1 }, node: { "start:node": 2 } }, pending_sends: [],
} // store checkpoint
await checkpointer.put(writeConfig, checkpoint, {}, {}); // load checkpoint
await checkpointer.get(readConfig); // list checkpoints
for await (const checkpoint of checkpointer.list(readConfig)) { console.log(checkpoint);
}
``` ---------------------------------------- TITLE: Define and Test Retrieval Grader Chain (JavaScript)
DESCRIPTION: Defines a system prompt for grading document relevance, creates an LCEL chain using the prompt, LLM (in JSON mode), and JSON parser, and invokes it with a test question and retrieved document content.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rag/langgraph_adaptive_rag_local.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
const GRADER_TEMPLATE = `You are a grader assessing relevance of a retrieved document to a user question.
Here is the retrieved document: <document>
{content}
</document> Here is the user question:
<question>
{question}
</question> If the document contains keywords related to the user question, grade it as relevant.
It does not need to be a stringent test. The goal is to filter out erroneous retrievals.
Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.
Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.`; const graderPrompt = ChatPromptTemplate.fromTemplate(GRADER_TEMPLATE); const retrievalGrader = graderPrompt.pipe(jsonModeLlm).pipe( new JsonOutputParser(),
); // Test run
const testQuestion = "agent memory"; const docs2 = await retriever.invoke(testQuestion); await retrievalGrader.invoke({ question: testQuestion, content: docs2[0].pageContent,
});
``` ---------------------------------------- TITLE: Defining and Using Multiple State Schemas in LangGraph (TypeScript)
DESCRIPTION: This snippet demonstrates how to define and utilize multiple state schemas (`InputStateAnnotation`, `OutputStateAnnotation`, `OverallStateAnnotation`) within a LangGraph application. It shows how nodes can read from specific input schemas but write to a broader overall state, and how to explicitly configure input and output schemas for the `StateGraph` instance. The example illustrates a simple workflow where `node1` processes user input, `node2` transforms an intermediate state, and `node3` generates the final graph output, resulting in `{ graph_output: "My name is Lance" }`.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/low_level.md#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { Annotation, START, StateGraph, StateType, UpdateType,
} from "@langchain/langgraph"; const InputStateAnnotation = Annotation.Root({ user_input: Annotation<string>,
}); const OutputStateAnnotation = Annotation.Root({ graph_output: Annotation<string>,
}); const OverallStateAnnotation = Annotation.Root({ foo: Annotation<string>, bar: Annotation<string>, user_input: Annotation<string>, graph_output: Annotation<string>,
}); const node1 = async (state: typeof InputStateAnnotation.State) => { // Write to OverallStateAnnotation return { foo: state.user_input + " name" };
}; const node2 = async (state: typeof OverallStateAnnotation.State) => { // Read from OverallStateAnnotation, write to OverallStateAnnotation return { bar: state.foo + " is" };
}; const node3 = async (state: typeof OverallStateAnnotation.State) => { // Read from OverallStateAnnotation, write to OutputStateAnnotation return { graph_output: state.bar + " Lance" };
}; // Most of the time the StateGraph type parameters are inferred by TypeScript,
// but this is a special case where they must be specified explicitly in order
// to avoid a type error.
const graph = new StateGraph< (typeof OverallStateAnnotation)["spec"], StateType<(typeof OverallStateAnnotation)["spec"]>, UpdateType<(typeof OutputStateAnnotation)["spec"]>, typeof START, (typeof InputStateAnnotation)["spec"], (typeof OutputStateAnnotation)["spec"]
>({ input: InputStateAnnotation, output: OutputStateAnnotation, stateSchema: OverallStateAnnotation,
}) .addNode("node1", node1) .addNode("node2", node2) .addNode("node3", node3) .addEdge("__start__", "node1") .addEdge("node1", "node2") .addEdge("node2", "node3") .compile(); await graph.invoke({ user_input: "My" });
``` ---------------------------------------- TITLE: Defining a LangGraph Workflow with Human Review (TypeScript)
DESCRIPTION: This snippet defines the core components of the LangGraph workflow. It includes node functions for calling an LLM, performing human review with interruption capabilities, and executing tools. It also sets up the graph structure with nodes, edges, and conditional routing based on the LLM's output.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/review-tool-calls.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
import { MessagesAnnotation, StateGraph, START, END, MemorySaver, Command, interrupt
} from "@langchain/langgraph";
import { ChatAnthropic } from "@langchain/anthropic";
import { tool } from '@langchain/core/tools';
import { z } from 'zod';
import { AIMessage, ToolMessage } from '@langchain/core/messages';
import { ToolCall } from '@langchain/core/messages/tool'; const weatherSearch = tool((input: { city: string }) => { console.log("----"); console.log(`Searching for: ${input.city}`); console.log("----"); return "Sunny!";
}, { name: 'weather_search', description: 'Search for the weather', schema: z.object({ city: z.string() })
}); const model = new ChatAnthropic({ model: "claude-3-5-sonnet-latest"
}).bindTools([weatherSearch]); const callLLM = async (state: typeof MessagesAnnotation.State) => { const response = await model.invoke(state.messages); return { messages: [response] };
}; const humanReviewNode = async (state: typeof MessagesAnnotation.State): Promise<Command> => { const lastMessage = state.messages[state.messages.length - 1] as AIMessage; const toolCall = lastMessage.tool_calls![lastMessage.tool_calls!.length - 1]; const humanReview = interrupt< { question: string; toolCall: ToolCall; }, { action: string; data: any; }>({ question: "Is this correct?", toolCall: toolCall }); const reviewAction = humanReview.action; const reviewData = humanReview.data; if (reviewAction === "continue") { return new Command({ goto: "run_tool" }); } else if (reviewAction === "update") { const updatedMessage = { role: "ai", content: lastMessage.content, tool_calls: [{ id: toolCall.id, name: toolCall.name, args: reviewData }], id: lastMessage.id }; return new Command({ goto: "run_tool", update: { messages: [updatedMessage] } }); } else if (reviewAction === "feedback") { const toolMessage = new ToolMessage({ name: toolCall.name, content: reviewData, tool_call_id: toolCall.id }) return new Command({ goto: "call_llm", update: { messages: [toolMessage] }}); } throw new Error("Invalid review action");
}; const runTool = async (state: typeof MessagesAnnotation.State) => { const newMessages: ToolMessage[] = []; const tools = { weather_search: weatherSearch }; const lastMessage = state.messages[state.messages.length - 1] as AIMessage; const toolCalls = lastMessage.tool_calls!; for (const toolCall of toolCalls) { const tool = tools[toolCall.name as keyof typeof tools]; const result = await tool.invoke(toolCall.args); newMessages.push(new ToolMessage({ name: toolCall.name, content: result, tool_call_id: toolCall.id })); } return { messages: newMessages };
}; const routeAfterLLM = (state: typeof MessagesAnnotation.State): typeof END | "human_review_node" => { const lastMessage = state.messages[state.messages.length - 1] as AIMessage; if (!lastMessage.tool_calls?.length) { return END; } return "human_review_node";
}; const workflow = new StateGraph(MessagesAnnotation) .addNode("call_llm", callLLM) .addNode("run_tool", runTool) .addNode("human_review_node", humanReviewNode, { ends: ["run_tool", "call_llm"] }) .addEdge(START, "call_llm") .addConditionalEdges( "call_llm", routeAfterLLM, ["human_review_node", END] ) .addEdge("run_tool", "call_llm"); const memory = new MemorySaver(); const graph = workflow.compile({ checkpointer: memory });
``` ---------------------------------------- TITLE: Initialize ReAct Agent with OpenAI Model and Weather Tool (JS/TS)
DESCRIPTION: Imports necessary modules from LangChain and LangGraph, initializes an OpenAI chat model, defines a custom tool 'get_weather' with schema validation using zod, and creates a prebuilt ReAct agent instance using the model and the defined tool.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/create-react-agent.ipynb#_snippet_2 LANGUAGE: typescript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { tool } from '@langchain/core/tools';
import { z } from 'zod';
import { createReactAgent } from "@langchain/langgraph/prebuilt"; const model = new ChatOpenAI({ model: "gpt-4o",
}); const getWeather = tool((input) => { if (['sf', 'san francisco', 'san francisco, ca'].includes(input.location.toLowerCase())) { return 'It\'s 60 degrees and foggy.'; } else { return 'It\'s 90 degrees and sunny.'; }
}, { name: 'get_weather', description: 'Call to get the current weather.', schema: z.object({ location: z.string().describe("Location to get the weather for."), })
}) const agent = createReactAgent({ llm: model, tools: [getWeather] });
``` ---------------------------------------- TITLE: Defining LangGraph Entrypoint (JavaScript)
DESCRIPTION: This snippet defines the main `entrypoint` for the LangGraph agent. It takes an initial list of messages, calls the language model, and enters a loop. If the model suggests tool calls, it executes them, appends the results to the message list, and calls the model again until no more tool calls are suggested. It uses `entrypoint` and `addMessages` from `@langchain/langgraph`.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/react-agent-from-scratch-functional.ipynb#_snippet_4 LANGUAGE: JavaScript
CODE:
```
import { entrypoint, addMessages } from "@langchain/langgraph"; const agent = entrypoint( "agent", async (messages: BaseMessageLike[]) => { let currentMessages = messages; let llmResponse = await callModel(currentMessages); while (true) { if (!llmResponse.tool_calls?.length) { break; } // Execute tools const toolResults = await Promise.all( llmResponse.tool_calls.map((toolCall) => { return callTool(toolCall); }) ); // Append to message list currentMessages = addMessages(currentMessages, [llmResponse, ...toolResults]); // Call model again llmResponse = await callModel(currentMessages); } return llmResponse; }
);
``` ---------------------------------------- TITLE: Correct Deterministic Workflow Example (TypeScript)
DESCRIPTION: This example shows the correct way to handle non-deterministic operations like getting the current time in LangGraphJS. It encapsulates `Date.now()` within a `task`, ensuring that the result is recorded and reused upon resuming, maintaining determinism.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/functional_api.md#_snippet_18 LANGUAGE: typescript
CODE:
```
import { task } from "@langchain/langgraph"; // highlight-next-line
const getTime = task("getTime", () => Date.now()); const myWorkflow = entrypoint( { checkpointer, name: "myWorkflow" }, async (inputs: { t0: number }) => { // highlight-next-line const t1 = await getTime(); const deltaT = t1 - inputs.t0; if (deltaT > 1000) { const result = await slowTask(1); const value = interrupt("question"); return { result, value }; } else { const result = await slowTask(2); const value = interrupt("question"); return { result, value }; } }
);
``` ---------------------------------------- TITLE: Handling Human Interrupts in TypeScript
DESCRIPTION: Demonstrates how to signal and handle human intervention during execution. The `interruptFor` function raises a specific error type. The main execution loop catches this error, saves the current state using `_putCheckpoint`, and provides details about the interrupt reason and metadata to the caller.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/libs/langgraph/spec/pregel-execution-model.md#_snippet_7 LANGUAGE: typescript
CODE:
```
// Interrupt execution to wait for human input
function interruptFor( reason: string, metadata: Record<string, any> = {}
): never { throw new InterruptError(reason, metadata);
} // Handle interrupts in the main execution loop
try { while (await loop.tick(config)) { await runner.tick(); }
} catch (e) { if (e instanceof InterruptError) { // Save current state const checkpoint = await loop._putCheckpoint({ superstep: versions.superstep, reason: e.reason, metadata: e.metadata }); // Return to caller with interrupt info return { checkpoint, reason: e.reason, metadata: e.metadata }; } throw e;
}
``` ---------------------------------------- TITLE: Defining Structured Output Schema for LangGraph ReAct Agent (TypeScript)
DESCRIPTION: Demonstrates how to define a Zod schema for the desired structured output and pass it to the `createReactAgent` function using the `responseFormat` parameter. This configures the agent to return output conforming to the specified schema.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/react-return-structured-output.ipynb#_snippet_0 LANGUAGE: typescript
CODE:
```
import { z } from "zod";
import { createReactAgent } from "@langchain/langgraph/prebuilt"; const responseFormat = z.object({ // Respond to the user in this format mySpecialOutput: z.string(),
}) const graph = createReactAgent({ llm: llm, tools: tools, // specify the schema for the structured output using `responseFormat` parameter responseFormat: responseFormat
})
``` ---------------------------------------- TITLE: Using Closure-Based Tools in a LangGraph Node (JavaScript)
DESCRIPTION: This asynchronous JavaScript/TypeScript function represents a node within a LangGraph. It demonstrates how to use the `generateTools` function to obtain tools that have access to the current graph `state`. By calling `generateTools(state)` inside the node's execution logic, a closure is formed, allowing the tools to access the state relevant at the time the node is reached. The generated tools are then wrapped in a `ToolNode` and invoked.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/pass-run-time-values-to-tools.ipynb#_snippet_14 LANGUAGE: JavaScript
CODE:
```
const toolNodeWithClosure = async (state: typeof MessagesAnnotation.State) => { // We fetch the tools any time this node is reached to // form a closure and let it access the latest messages const tools = generateTools(state); const toolNodeWithConfig = new ToolNode(tools); return toolNodeWithConfig.invoke(state);
};
``` ---------------------------------------- TITLE: Implementing Dynamic Control Flow with Conditional Command in LangGraph
DESCRIPTION: This example illustrates how to achieve dynamic control flow by conditionally returning a Command object based on the current state. It allows a node to perform a state update and then route to a different node, similar to conditional edges, but with the added capability of state modification within the same node. This pattern is useful for complex routing logic.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/low_level.md#_snippet_21 LANGUAGE: TypeScript
CODE:
```
const myNode = async (state: typeof StateAnnotation.State) => { if (state.foo === "bar") { return new Command({ update: { foo: "baz", }, goto: "myOtherNode", }); } // ...
};
``` ---------------------------------------- TITLE: Building a State Graph - LangGraphJS - TypeScript
DESCRIPTION: Constructs a LangGraph state graph using StateGraph. It adds 'model' and 'tools' nodes, defines a 'shouldContinue' function to conditionally route based on tool calls, and sets up edges for the workflow.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/streaming-tokens-without-langchain.ipynb#_snippet_4 LANGUAGE: TypeScript
CODE:
```
import { StateGraph } from "@langchain/langgraph";
import OpenAI from "openai"; // We can reuse the same `GraphState` from above as it has not changed.
const shouldContinue = (state: typeof StateAnnotation.State) => { const { messages } = state; const lastMessage = messages[messages.length - 1] as OpenAI.ChatCompletionAssistantMessageParam; if (lastMessage?.tool_calls !== undefined && lastMessage?.tool_calls.length > 0) { return "tools"; } return "__end__";
} const graph = new StateGraph(StateAnnotation) .addNode("model", callModel) .addNode("tools", callTools) .addEdge("__start__", "model") .addConditionalEdges("model", shouldContinue, { tools: "tools", __end__: "__end__", }) .addEdge("tools", "model") .compile();
``` ---------------------------------------- TITLE: Creating LangGraph Handoff Tool (TypeScript)
DESCRIPTION: Defines a LangGraph tool using `@langchain/core/tools` that facilitates transferring control to another agent node within a parent graph. It returns a `Command` object specifying the target node (`goto`), data to pass (`update`), and indicating the target is in a parent graph (`graph: Command.PARENT`).
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/multi-agent.md#_snippet_4 LANGUAGE: TypeScript
CODE:
```
const transferToBob = tool( async (_) => { return new Command({ // name of the agent (node) to go to // highlight-next-line goto: "bob", // data to send to the agent // highlight-next-line update: { messages: ... }, // indicate to LangGraph that we need to navigate to // agent node in a parent graph // highlight-next-line graph: Command.PARENT, }); }, { name: ..., schema: ..., description: ... }
);
``` ---------------------------------------- TITLE: Defining a Human-in-the-Loop Tool and Agent with LangGraph (TypeScript)
DESCRIPTION: This snippet defines a `bookHotel` tool that pauses execution using `interrupt()` to allow human review before proceeding. It also initializes a `MemorySaver` checkpointer to persist the agent's state and creates a LangGraph agent using `createReactAgent`, incorporating the defined tool and checkpointer. This setup is essential for enabling human-in-the-loop capabilities by allowing the agent to pause and resume based on external input.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/human-in-the-loop.md#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { MemorySaver } from "@langchain/langgraph-checkpoint";
import { interrupt } from "@langchain/langgraph";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { initChatModel } from "langchain/chat_models/universal";
import { tool } from "@langchain/core/tools";
import { z } from "zod"; // An example of a sensitive tool that requires human review / approval
const bookHotel = tool( async (input: { hotelName: string; }) => { let hotelName = input.hotelName; // highlight-next-line const response = interrupt( // (1)! `Trying to call \`book_hotel\` with args {'hotel_name': ${hotelName}}. ` + `Please approve or suggest edits.` ) if (response.type === "accept") { // proceed to execute the tool logic } else if (response.type === "edit") { hotelName = response.args["hotel_name"] } else { throw new Error(`Unknown response type: ${response.type}`) } return `Successfully booked a stay at ${hotelName}.`; }, { name: "bookHotel", schema: z.object({ hotelName: z.string().describe("Hotel to book"), }), description: "Book a hotel.", }
); // highlight-next-line
const checkpointer = new MemorySaver(); // (2)! const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest");
const agent = createReactAgent({ llm, tools: [bookHotel], // highlight-next-line checkpointer // (3)!
});
``` ---------------------------------------- TITLE: Defining Structured Output Schema with Zod (TypeScript)
DESCRIPTION: This snippet defines Zod schemas (`sectionSchema` and `sectionsSchema`) to specify the expected structure for the LLM's output when planning a report. It then augments an existing LLM instance (`llm`) using `withStructuredOutput` to enforce this schema, ensuring the planner generates structured data. Requires the `zod` library and an initialized `llm` object.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_15 LANGUAGE: typescript
CODE:
```
import { z } from "zod"; // Schema for structured output to use in planning
const sectionSchema = z.object({ name: z.string().describe("Name for this section of the report."), description: z.string().describe( "Brief overview of the main topics and concepts to be covered in this section." ),
}); const sectionsSchema = z.object({ sections: z.array(sectionSchema).describe("Sections of the report."),
}); // Augment the LLM with schema for structured output
const planner = llm.withStructuredOutput(sectionsSchema);
``` ---------------------------------------- TITLE: Build LangGraph Agent with ToolNode (TypeScript)
DESCRIPTION: Constructs a LangGraph agent implementing the ReAct pattern using a `StateGraph` with `MessagesAnnotation`. It integrates the mock `getWeather` tool via a `ToolNode` and uses an Anthropic model bound with the tool. The graph defines nodes for calling the model and executing tools, with conditional edges to control flow based on the model's output.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/tool-calling-errors.ipynb#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { StateGraph, MessagesAnnotation } from "@langchain/langgraph";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import { ChatAnthropic } from "@langchain/anthropic";
import { BaseMessage, isAIMessage } from "@langchain/core/messages"; const toolNode = new ToolNode([getWeather]); const modelWithTools = new ChatAnthropic({ model: "claude-3-haiku-20240307", temperature: 0,
}).bindTools([getWeather]); const shouldContinue = async (state: typeof MessagesAnnotation.State) => { const { messages } = state; const lastMessage = messages[messages.length - 1]; if (isAIMessage(lastMessage) && lastMessage.tool_calls?.length) { return "tools"; } return "__end__";
} const callModel = async (state: typeof MessagesAnnotation.State) => { const { messages } = state; const response = await modelWithTools.invoke(messages); return { messages: [response] };
} const app = new StateGraph(MessagesAnnotation) .addNode("agent", callModel) .addNode("tools", toolNode) .addEdge("__start__", "agent") .addEdge("tools", "agent") .addConditionalEdges("agent", shouldContinue, { // Explicitly list possible destinations so that // we can automatically draw the graph below. tools: "tools", __end__: "__end__", }) .compile();
``` ---------------------------------------- TITLE: Compiling LangGraph with Checkpointer and Store (TypeScript)
DESCRIPTION: Imports `MemorySaver` to create a checkpointer for thread-specific state persistence. It then compiles the graph builder, passing both the `checkpointer` and the previously initialized `inMemoryStore` to enable both within-thread and cross-thread persistence.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/persistence.md#_snippet_13 LANGUAGE: TypeScript
CODE:
```
import { MemorySaver } from "@langchain/langgraph"; // We need this because we want to enable threads (conversations)
const checkpointer = new MemorySaver(); // ... Define the graph ... // Compile the graph with the checkpointer and store
const graph = builder.compile({ checkpointer, store: inMemoryStore
});
``` ---------------------------------------- TITLE: Defining LangGraph Workflow (TypeScript)
DESCRIPTION: Constructs the core LangGraph workflow using `StateGraph`. It defines two nodes: "agent" (which calls the bound model) and "tools" (which executes tools via the `ToolNode`). It sets up the entry point (`START`), the exit point (`END`), and conditional edges (`routeMessage`) to route between the agent and tools based on the model's response. Finally, it compiles the workflow into a runnable graph.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/stream-updates.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { END, START, StateGraph } from "@langchain/langgraph";
import { AIMessage } from "@langchain/core/messages"; const routeMessage = (state: typeof StateAnnotation.State) => { const { messages } = state; const lastMessage = messages[messages.length - 1] as AIMessage; // If no tools are called, we can finish (respond to the user) if (!lastMessage?.tool_calls?.length) { return END; } // Otherwise if there is, we continue and call the tools return "tools";
}; const callModel = async ( state: typeof StateAnnotation.State,
) => { const { messages } = state; const responseMessage = await boundModel.invoke(messages); return { messages: [responseMessage] };
}; const workflow = new StateGraph(StateAnnotation) .addNode("agent", callModel) .addNode("tools", toolNode) .addEdge(START, "agent") .addConditionalEdges("agent", routeMessage) .addEdge("tools", "agent"); const graph = workflow.compile();
``` ---------------------------------------- TITLE: Building a LangGraph StateGraph
DESCRIPTION: Demonstrates how to initialize a LangGraph StateGraph and add the previously defined nodes (initial_support, billing_support, technical_support, handle_refund) to it. It also sets the starting node of the graph.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/chatbots/customer_support_small_model.ipynb#_snippet_8 LANGUAGE: JavaScript
CODE:
```
import { StateGraph } from "@langchain/langgraph"; let builder = new StateGraph(StateAnnotation) .addNode("initial_support", initialSupport) .addNode("billing_support", billingSupport) .addNode("technical_support", technicalSupport) .addNode("handle_refund", handleRefund) .addEdge("__start__", "initial_support");
``` ---------------------------------------- TITLE: Defining Multi-Agent Network with LangGraph Functional API (TypeScript)
DESCRIPTION: This snippet defines the core components of a multi-agent network using LangGraph's functional API. It includes defining a tool for agent handoff, creating a React agent, wrapping the agent invocation in a task, and setting up the main entrypoint function that manages the agent handoff loop.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/multi-agent-network-functional.ipynb#_snippet_0 LANGUAGE: typescript
CODE:
```
import { entrypoint, task } from "@langchain/langgraph";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { tool } from "@langchain/core/tools";
import { z } from "zod"; // Define a tool to signal intent to hand off to a different agent
const transferToHotelAdvisor = tool(async () => { return "Successfully transferred to hotel advisor";
}, { name: "transferToHotelAdvisor", description: "Ask hotel advisor agent for help.", schema: z.object({}), returnDirect: true,
}); // define an agent
const travelAdvisorTools = [transferToHotelAdvisor, ...];
const travelAdvisor = createReactAgent({ llm: model, tools: travelAdvisorTools,
}); // define a task that calls an agent
const callTravelAdvisor = task("callTravelAdvisor", async (messages: BaseMessage[]) => { const response = travelAdvisor.invoke({ messages }); return response.messages;
}); const networkGraph = entrypoint( { name: "networkGraph" }, async (messages: BaseMessageLike[]) => { let callActiveAgent = callTravelAdvisor; let agentMessages; while (true) { agentMessages = await callActiveAgent(messages); messages = addMessages(messages, agentMessages); callActiveAgent = getNextAgent(messages); } return messages; });
``` ---------------------------------------- TITLE: Defining a LangGraph Task with Retry Policy (TypeScript)
DESCRIPTION: This snippet demonstrates how to define a LangGraph task using the `task` function and configure a retry policy. The `retry` parameter is used to specify retry behavior, such as setting `maxAttempts` to limit the number of execution attempts.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/functional_api.md#_snippet_14 LANGUAGE: typescript
CODE:
```
const slowComputation = task( { name: "slowComputation", // only attempt to run this task once before giving up retry: { maxAttempts: 1 }, }, async (inputValue: any) => { // A long-running operation that may fail return result; }
);
``` ---------------------------------------- TITLE: Define and Compile LangGraph Workflow (JavaScript)
DESCRIPTION: Constructs the LangGraph StateGraph by adding the defined nodes (`agent`, `action`), setting the start node, adding a conditional edge from `agent` based on `shouldContinue`, and adding a normal edge from `action` back to `agent`. Finally, compiles the workflow into a runnable application.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/chat_agent_executor_with_function_calling/base.ipynb#_snippet_8 LANGUAGE: JavaScript
CODE:
```
import { END, START, StateGraph } from "@langchain/langgraph"; // Define a new graph
const workflow = new StateGraph(AgentState) // Define the two nodes we will cycle between .addNode("agent", callModel) .addNode("action", callTool) // Set the entrypoint as `agent` // This means that this node is the first one called .addEdge(START, "agent") // We now add a conditional edge .addConditionalEdges( // First, we define the start node. We use `agent`. // This means these are the edges taken after the `agent` node is called. "agent", // Next, we pass in the function that will determine which node is called next. shouldContinue, // Finally we pass in a mapping. // The keys are strings, and the values are other nodes. // END is a special node marking that the graph should finish. // What will happen is we will call `should_continue`, and then the output of that // will be matched against the keys in this mapping. // Based on which one it matches, that node will then be called. { // If `tools`, then we call the tool node. continue: "action", // Otherwise we finish. end: END, }, ) // We now add a normal edge from `tools` to `agent`. // This means that after `tools` is called, `agent` node is called next. .addEdge("action", "agent"); // Finally, we compile it!
// This compiles it into a LangChain Runnable,
// meaning you can use it as you would any other runnable
const app = workflow.compile();
``` ---------------------------------------- TITLE: Defining and Compiling LangGraph Workflow (JS)
DESCRIPTION: Constructs a LangGraph `StateGraph` by adding agent and tool nodes, defining the entry point, adding a conditional edge from the agent node based on the `shouldContinue` function, and adding a normal edge from the tools node back to the agent. Finally, it compiles the workflow into a runnable application.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/dynamically-returning-directly.ipynb#_snippet_7 LANGUAGE: JavaScript
CODE:
```
import { START, StateGraph } from "@langchain/langgraph"; // Define a new graph
const workflow = new StateGraph(AgentState) // Define the two nodes we will cycle between .addNode("agent", callModel) // Note the "action" and "final" nodes are identical! .addNode("tools", toolNode) .addNode("final", toolNode) // Set the entrypoint as `agent` .addEdge(START, "agent") // We now add a conditional edge .addConditionalEdges( // First, we define the start node. We use `agent`. "agent", // Next, we pass in the function that will determine which node is called next. shouldContinue, ) // We now add a normal edge from `tools` to `agent`. .addEdge("tools", "agent") .addEdge("final", END); // Finally, we compile it!
const app = workflow.compile();
``` ---------------------------------------- TITLE: Define Graph Nodes (shouldContinue, callModel) - LangGraph JS
DESCRIPTION: Defines two core nodes for the LangGraph agent workflow. `shouldContinue` checks the last agent message for tool calls to decide whether to proceed to tool execution ("tools") or end the process (`END`). `callModel` prepares a limited history of recent messages (up to 5, ensuring tool messages are included) and invokes the language model (`boundModel`) with them, returning the model's response.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/managing-agent-steps.ipynb#_snippet_8 LANGUAGE: TypeScript
CODE:
```
import { END } from "@langchain/langgraph";
import { AIMessage, ToolMessage } from "@langchain/core/messages";
import { RunnableConfig } from "@langchain/core/runnables"; // Define the function that determines whether to continue or not
const shouldContinue = (state: typeof AgentState.State) => { const { messages } = state; const lastMessage = messages[messages.length - 1] as AIMessage; // If there is no function call, then we finish if (!lastMessage.tool_calls || lastMessage.tool_calls.length === 0) { return END; } // Otherwise if there is, we continue return "tools";
}; // **MODIFICATION**
//
// Here we don't pass all messages to the model but rather only pass the `N` most recent. Note that this is a terribly simplistic way to handle messages meant as an illustration, and there may be other methods you may want to look into depending on your use case. We also have to make sure we don't truncate the chat history to include the tool message first, as this would cause an API error.
const callModel = async ( state: typeof AgentState.State, config?: RunnableConfig,
) => { let modelMessages = []; for (let i = state.messages.length - 1; i >= 0; i--) { modelMessages.push(state.messages[i]); if (modelMessages.length >= 5) { if (!ToolMessage.isInstance(modelMessages[modelMessages.length - 1])) { break; } } } modelMessages.reverse(); const response = await boundModel.invoke(modelMessages, config); // We return an object, because this will get added to the existing list return { messages: [response] };
};
``` ---------------------------------------- TITLE: Defining Agent Nodes and Graph Structure - LangGraph JS
DESCRIPTION: This snippet defines a factory function `makeAgentNode` to create agent nodes with structured output for routing. It initializes the OpenAI model, creates three specific agent nodes (`travelAdvisor`, `sightseeingAdvisor`, `hotelAdvisor`) with their respective system prompts and allowed destinations, and constructs the `StateGraph` by adding these nodes and defining the initial edge.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/multi-agent-network.ipynb#_snippet_2 LANGUAGE: javascript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { Command, MessagesAnnotation, StateGraph
} from "@langchain/langgraph"; import { z } from "zod"; const model = new ChatOpenAI({ model: "gpt-4o", temperature: 0.1,
}); const makeAgentNode = (params: { name: string, destinations: string[], systemPrompt: string
}) => { return async (state: typeof MessagesAnnotation.State) => { const possibleDestinations = ["__end__", ...params.destinations] as const; // define schema for the structured output: // - model's text response (`response`) // - name of the node to go to next (or '__end__') const responseSchema = z.object({ response: z.string().describe( "A human readable response to the original question. Does not need to be a final response. Will be streamed back to the user." ), goto: z.enum(possibleDestinations).describe("The next agent to call, or __end__ if the user's query has been resolved. Must be one of the specified values."), }); const messages = [ { role: "system", content: params.systemPrompt }, ...state.messages, ]; const response = await model.withStructuredOutput(responseSchema, { name: "router", }).invoke(messages); // handoff to another agent or halt const aiMessage = { role: "assistant", content: response.response, name: params.name, }; return new Command({ goto: response.goto, update: { messages: aiMessage } }); }
}; const travelAdvisor = makeAgentNode({ name: "travel_advisor", destinations: ["sightseeing_advisor", "hotel_advisor"], systemPrompt: [ "You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc). ", "If you need specific sightseeing recommendations, ask 'sightseeing_advisor' for help. ", "If you need hotel recommendations, ask 'hotel_advisor' for help. ", "If you have enough information to respond to the user, return '__end__'. ", "Never mention other agents by name." ].join(""),
}); const sightseeingAdvisor = makeAgentNode({ name: "sightseeing_advisor", destinations: ["travel_advisor", "hotel_advisor"], systemPrompt: [ "You are a travel expert that can provide specific sightseeing recommendations for a given destination. ", "If you need general travel help, go to 'travel_advisor' for help. ", "If you need hotel recommendations, go to 'hotel_advisor' for help. ", "If you have enough information to respond to the user, return 'finish'. ", "Never mention other agents by name." ].join(""),
}); const hotelAdvisor = makeAgentNode({ name: "hotel_advisor", destinations: ["travel_advisor", "sightseeing_advisor"], systemPrompt: [ "You are a booking expert that provides hotel recommendations for a given destination. ", "If you need general travel help, ask 'travel_advisor' for help. ", "If you need specific sightseeing recommendations, ask 'sightseeing_advisor' for help. ", "If you have enough information to respond to the user, return 'finish'. ", "Never mention other agents by name.", ].join(""),
}); const graph = new StateGraph(MessagesAnnotation) .addNode("travel_advisor", travelAdvisor, { ends: ["sightseeing_advisor", "hotel_advisor", "__end__"], }) .addNode("sightseeing_advisor", sightseeingAdvisor, { ends: ["travel_advisor", "hotel_advisor", "__end__"], }) .addNode("hotel_advisor", hotelAdvisor, { ends: ["travel_advisor", "sightseeing_advisor", "__end__"], }) // we'll always start with a general travel advisor .addEdge("__start__", "travel_advisor") .compile();
``` ---------------------------------------- TITLE: Define LangGraph Agent State Schema
DESCRIPTION: Defines the schema for the agent's state using LangGraph's Annotation. The state includes a messages attribute which accumulates conversation history using a reducer.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/agent_executor/base.ipynb#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { Annotation } from "@langchain/langgraph";
import { BaseMessage } from "@langchain/core/messages"; const AgentState = Annotation.Root({ messages: Annotation<BaseMessage[]> ({ reducer: (x, y) => x.concat(y), }),
});
``` ---------------------------------------- TITLE: Streaming Agent Progress with LangGraph (TypeScript)
DESCRIPTION: Demonstrates how to stream updates after each node execution in a LangGraph agent using the `stream` method with `streamMode: "updates"`. It shows iterating over the streamed chunks, which represent the state changes after each step.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/streaming.md#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { initChatModel } from "langchain/chat_models/universal"; const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest");
const agent = createReactAgent({ llm, tools: [getWeather],
});
// highlight-next-line
for await (const chunk of await agent.stream( { messages: "what is the weather in sf" }, // highlight-next-line { streamMode: "updates" }
)) { console.log(chunk); console.log("\n");
}
``` ---------------------------------------- TITLE: Defining a Parent Graph with Subgraph Routing (LangGraph JS)
DESCRIPTION: Sets up the main LangGraph StateGraph, defining state schema, a router model, nodes for routing and normal LLM interaction, a conditional edge function, and connecting them to route between a normal LLM and a 'weatherGraph' subgraph based on user input.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/subgraphs-manage-state.ipynb#_snippet_3 LANGUAGE: JavaScript
CODE:
```
import { MemorySaver } from "@langchain/langgraph"; const memory = new MemorySaver(); const RouterStateAnnotation = Annotation.Root({ ...MessagesAnnotation.spec, route: Annotation<"weather" | "other">,
}); const routerModel = rawModel.withStructuredOutput( z.object({ route: z.enum(["weather", "other"]).describe("A step that should execute next to based on the currnet input") }), { name: "router" }
); const routerNode = async (state: typeof RouterStateAnnotation.State) => { const systemMessage = { role: "system", content: "Classify the incoming query as either about weather or not.", }; const messages = [systemMessage, ...state.messages] const { route } = await routerModel.invoke(messages); return { route };
} const normalLLMNode = async (state: typeof RouterStateAnnotation.State) => { const responseMessage = await rawModel.invoke(state.messages); return { messages: [responseMessage] };
}; const routeAfterPrediction = async (state: typeof RouterStateAnnotation.State) => { if (state.route === "weather") { return "weatherGraph"; } else { return "normalLLMNode"; }
}; const graph = new StateGraph(RouterStateAnnotation) .addNode("routerNode", routerNode) .addNode("normalLLMNode", normalLLMNode) .addNode("weatherGraph", subgraph) .addEdge("__start__", "routerNode") .addConditionalEdges("routerNode", routeAfterPrediction) .addEdge("normalLLMNode", "__end__") .addEdge("weatherGraph", "__end__") .compile({ checkpointer: memory });
``` ---------------------------------------- TITLE: Define State and Nodes (TypeScript)
DESCRIPTION: Defines the graph state schema using Annotation and implements three asynchronous node functions (nodeA, nodeB, nodeC) for a simple LangGraph example. nodeA demonstrates returning a Command for conditional routing and state update.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/command.ipynb#_snippet_3 LANGUAGE: typescript
CODE:
```
import { Annotation, Command } from "@langchain/langgraph"; // Define graph state
const StateAnnotation = Annotation.Root({ foo: Annotation<string>
}); // Define the nodes
const nodeA = async (_state: typeof StateAnnotation.State) => { console.log("Called A"); // this is a replacement for a real conditional edge function const goto = Math.random() > .5 ? "nodeB" : "nodeC"; // note how Command allows you to BOTH update the graph state AND route to the next node return new Command({ // this is the state update update: { foo: "a" }, // this is a replacement for an edge goto, });
}; // Nodes B and C are unchanged
const nodeB = async (state: typeof StateAnnotation.State) => { console.log("Called B"); return { foo: state.foo + "|b", };
} const nodeC = async (state: typeof StateAnnotation.State) => { console.log("Called C"); return { foo: state.foo + "|c", };
}
``` ---------------------------------------- TITLE: Create a Simple ReAct Agent with LangGraph.js
DESCRIPTION: Demonstrates how to create a basic ReAct agent using LangGraph.js, integrating with an Anthropic model and a custom tool for searching weather information. Requires installing `@langchain-anthropic`.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/libs/langgraph/README.md#_snippet_1 LANGUAGE: typescript
CODE:
```
// npm install @langchain-anthropic
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { ChatAnthropic } from "@langchain/anthropic";
import { tool } from "@langchain/core/tools"; import { z } from "zod"; const search = tool(async ({ query }) => { if (query.toLowerCase().includes("sf") || query.toLowerCase().includes("san francisco")) { return "It's 60 degrees and foggy." } return "It's 90 degrees and sunny."
}, { name: "search", description: "Call to surf the web.", schema: z.object({ query: z.string().describe("The query to use in your search."), }),
}); const model = new ChatAnthropic({ model: "claude-3-7-sonnet-latest"
}); const agent = createReactAgent({ llm: model, tools: [search],
}); const result = await agent.invoke( { messages: [{ role: "user", content: "what is the weather in sf" }] }
); ``` ---------------------------------------- TITLE: Define Agent State with Message Reducer (Python)
DESCRIPTION: Defines the state structure for the LangGraph `StateGraph` using `Annotation.Root`. The state includes a `messages` key, which is an array of `BaseMessage` objects, and specifies a reducer function (`x.concat(y)`) to append new messages to the existing list.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/force-calling-a-tool-first.ipynb#_snippet_6 LANGUAGE: python
CODE:
```
import { Annotation } from "@langchain/langgraph";
import { BaseMessage } from "@langchain/core/messages"; const AgentState = Annotation.Root({ messages: Annotation<BaseMessage[]>({ reducer: (x, y) => x.concat(y), }),
});
``` ---------------------------------------- TITLE: Install LangGraphJS and Dependencies (Bash)
DESCRIPTION: Installs the necessary npm packages for LangGraphJS, OpenAI integration, and LangChain core using the yarn package manager.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/create-react-agent.ipynb#_snippet_0 LANGUAGE: bash
CODE:
```
yarn add @langchain/langgraph @langchain/openai @langchain/core
``` ---------------------------------------- TITLE: Defining a Simple Loop Graph in LangGraph (TypeScript)
DESCRIPTION: Sets up the state annotation with a reducer, defines two asynchronous nodes 'a' and 'b' that log state and return updates, and defines a conditional edge function 'route' that directs execution based on the length of the 'aggregate' state. It then builds the `StateGraph` with these nodes and edges, creating a loop from 'b' back to 'a' and a conditional exit from 'a'.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/recursion-limit.ipynb#_snippet_3 LANGUAGE: ts
CODE:
```
import { StateGraph, Annotation } from "@langchain/langgraph"; // Define the state with a reducer
const StateAnnotation = Annotation.Root({ aggregate: Annotation<string[]>({ reducer: (a, b) => a.concat(b), default: () => [], }),
}); // Define nodes
const a = async function (state: typeof StateAnnotation.State) { console.log(`Node A sees ${state.aggregate}`); return { aggregate: ["A"] };
} const b = async function (state: typeof StateAnnotation.State) { console.log(`Node B sees ${state.aggregate}`); return { aggregate: ["B"] };
} // Define edges
const route = async function (state: typeof StateAnnotation.State) { if (state.aggregate.length < 7) { return "b"; } else { return "__end__"; }
} // Define the graph
const graph = new StateGraph(StateAnnotation) .addNode("a", a) .addNode("b", b) .addEdge("__start__", "a") .addConditionalEdges("a", route) .addEdge("b", "a") .compile();
``` ---------------------------------------- TITLE: Install LangGraph.js and Core Dependencies (Bash)
DESCRIPTION: Command to install the necessary npm packages for using LangGraph.js and LangChain Core in a project.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/README.md#_snippet_0 LANGUAGE: bash
CODE:
```
npm install @langchain/langgraph @langchain/core
``` ---------------------------------------- TITLE: Implementing a Stateful Chatbot Workflow with LangGraph JS
DESCRIPTION: This comprehensive snippet defines the core components and workflow for a stateful chatbot using LangGraph.js. It includes setting up state annotations, integrating an LLM (Anthropic), defining nodes for model interaction and summarization, implementing conditional routing based on conversation length, and compiling the graph with a memory checkpointer to manage conversation state across turns.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/add-summary-conversation-history.ipynb#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { ChatAnthropic } from "@langchain/anthropic";
import { SystemMessage, HumanMessage, AIMessage, RemoveMessage } from "@langchain/core/messages";
import { MemorySaver } from "@langchain/langgraph-checkpoint";
import { MessagesAnnotation, StateGraph, START, END, Annotation } from "@langchain/langgraph";
import { v4 as uuidv4 } from "uuid"; const memory = new MemorySaver(); // We will add a `summary` attribute (in addition to `messages` key,
// which MessagesAnnotation already has)
const GraphAnnotation = Annotation.Root({ ...MessagesAnnotation.spec, summary: Annotation<string>({ reducer: (_, action) => action, default: () => "", })
}) // We will use this model for both the conversation and the summarization
const model = new ChatAnthropic({ model: "claude-3-haiku-20240307" }); // Define the logic to call the model
async function callModel(state: typeof GraphAnnotation.State): Promise<Partial<typeof GraphAnnotation.State>> { // If a summary exists, we add this in as a system message const { summary } = state; let { messages } = state; if (summary) { const systemMessage = new SystemMessage({ id: uuidv4(), content: `Summary of conversation earlier: ${summary}` }); messages = [systemMessage, ...messages]; } const response = await model.invoke(messages); // We return an object, because this will get added to the existing state return { messages: [response] };
} // We now define the logic for determining whether to end or summarize the conversation
function shouldContinue(state: typeof GraphAnnotation.State): "summarize_conversation" | typeof END { const messages = state.messages; // If there are more than six messages, then we summarize the conversation if (messages.length > 6) { return "summarize_conversation"; } // Otherwise we can just end return END;
} async function summarizeConversation(state: typeof GraphAnnotation.State): Promise<Partial<typeof GraphAnnotation.State>> { // First, we summarize the conversation const { summary, messages } = state; let summaryMessage: string; if (summary) { // If a summary already exists, we use a different system prompt // to summarize it than if one didn't summaryMessage = `This is summary of the conversation to date: ${summary}\n\n` + "Extend the summary by taking into account the new messages above:"; } else { summaryMessage = "Create a summary of the conversation above:"; } const allMessages = [...messages, new HumanMessage({ id: uuidv4(), content: summaryMessage, })]; const response = await model.invoke(allMessages); // We now need to delete messages that we no longer want to show up // I will delete all but the last two messages, but you can change this const deleteMessages = messages.slice(0, -2).map((m) => new RemoveMessage({ id: m.id })); if (typeof response.content !== "string") { throw new Error("Expected a string response from the model"); } return { summary: response.content, messages: deleteMessages };
} // Define a new graph
const workflow = new StateGraph(GraphAnnotation) // Define the conversation node and the summarize node .addNode("conversation", callModel) .addNode("summarize_conversation", summarizeConversation) // Set the entrypoint as conversation .addEdge(START, "conversation") // We now add a conditional edge .addConditionalEdges( // First, we define the start node. We use `conversation`. // This means these are the edges taken after the `conversation` node is called. "conversation", // Next, we pass in the function that will determine which node is called next. shouldContinue ) // We now add a normal edge from `summarize_conversation` to END. // This means that after `summarize_conversation` is called, we end. .addEdge("summarize_conversation", END); // Finally, we compile it!
const app = workflow.compile({ checkpointer: memory });
``` ---------------------------------------- TITLE: Define Graph State Schema
DESCRIPTION: Defines the schema for the state managed by the LangGraph. It uses @langchain/langgraph's Annotation to specify that the state contains a messages property, which is an array of BaseMessage objects, using messagesStateReducer for updates.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/respond-in-format.ipynb#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { Annotation, messagesStateReducer } from "@langchain/langgraph";
import { BaseMessage } from "@langchain/core/messages"; const GraphState = Annotation.Root({ messages: Annotation<BaseMessage[]> ({ reducer: messagesStateReducer, }),
});
``` ---------------------------------------- TITLE: Define LangGraph State Machine with Error Handling (TypeScript)
DESCRIPTION: Initializes a LangGraph StateGraph with nodes for tool calling, initial agent response, failed attempt removal, and a fallback agent. Defines edges to route execution, including conditional edges based on `shouldContinue2` and `shouldFallback` for normal flow and error handling.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/tool-calling-errors.ipynb#_snippet_11 LANGUAGE: TypeScript
CODE:
```
const app2 = new StateGraph(MessagesAnnotation) .addNode("tools", callTool2) .addNode("agent", callModel2) .addNode("remove_failed_tool_call_attempt", removeFailedToolCallAttempt) .addNode("fallback_agent", callFallbackModel) .addEdge("__start__", "agent") .addConditionalEdges("agent", shouldContinue2, { // Explicitly list possible destinations so that // we can automatically draw the graph below. tools: "tools", __end__: "__end__" }) .addConditionalEdges("tools", shouldFallback, { remove_failed_tool_call_attempt: "remove_failed_tool_call_attempt", agent: "agent" }) .addEdge("remove_failed_tool_call_attempt", "fallback_agent") .addEdge("fallback_agent", "tools") .compile();
``` ---------------------------------------- TITLE: Defining Agent Tools in TypeScript
DESCRIPTION: Defines several tools using `@langchain/core/tools` and `zod`. This includes tools for fetching travel and hotel recommendations and specialized tools (`transferToHotelAdvisor`, `transferToTravelAdvisor`) designed to signal intent to hand off control to another agent, utilizing `returnDirect: true` for immediate exit.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/multi-agent-multi-turn-convo-functional.ipynb#_snippet_2 LANGUAGE: typescript
CODE:
```
import { tool } from "@langchain/core/tools";
import { z } from "zod"; // Tool for getting travel recommendations
const getTravelRecommendations = tool(async () => { const destinations = ["aruba", "turks and caicos"]; return destinations[Math.floor(Math.random() * destinations.length)];
}, { name: "getTravelRecommendations", description: "Get recommendation for travel destinations", schema: z.object({}),
}); // Tool for getting hotel recommendations
const getHotelRecommendations = tool(async (input: { location: "aruba" | "turks and caicos" }) => { const recommendations = { "aruba": [ "The Ritz-Carlton, Aruba (Palm Beach)", "Bucuti & Tara Beach Resort (Eagle Beach)" ], "turks and caicos": ["Grace Bay Club", "COMO Parrot Cay"] }; return recommendations[input.location];
}, { name: "getHotelRecommendations", description: "Get hotel recommendations for a given destination.", schema: z.object({ location: z.enum(["aruba", "turks and caicos"]) }),
}); // Define a tool to signal intent to hand off to a different agent
// Note: this is not using Command(goto) syntax for navigating to different agents:
// `workflow()` below handles the handoffs explicitly
const transferToHotelAdvisor = tool(async () => { return "Successfully transferred to hotel advisor";
}, { name: "transferToHotelAdvisor", description: "Ask hotel advisor agent for help.", schema: z.object({}), // Hint to our agent implementation that it should stop // immediately after invoking this tool returnDirect: true,
}); const transferToTravelAdvisor = tool(async () => { return "Successfully transferred to travel advisor";
}, { name: "transferToTravelAdvisor", description: "Ask travel advisor agent for help.", schema: z.object({}), // Hint to our agent implementation that it should stop // immediately after invoking this tool returnDirect: true,
});
``` ---------------------------------------- TITLE: Placing Side Effects in a Separate Node in LangGraphJS (OK)
DESCRIPTION: This example illustrates another valid approach where the side effect (apiCall) is moved into a distinct node within the graph. This separates the side effect logic from the node containing the interrupt, preventing re-execution issues and improving modularity.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/human_in_the_loop.md#_snippet_12 LANGUAGE: TypeScript
CODE:
```
import { interrupt } from "@langchain/langgraph"; function humanNode(state: typeof GraphAnnotation.State) { /** * Human node with validation. */ const answer = interrupt(question); return { answer };
} function apiCallNode(state: typeof GraphAnnotation.State) { apiCall(); // OK as it's in a separate node
}
``` ---------------------------------------- TITLE: Define Document Writing Team Agents & Supervisor (LangGraph/JS)
DESCRIPTION: Defines three agent nodes (`DocWriter`, `NoteTaker`, `ChartGenerator`) using `createReactAgent` with specific tools and prompts that include the `current_files` context. Each agent node pipes the `prelude` runnable before the agent execution. It also sets up a `createTeamSupervisor` to manage the workflow between these agents.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb#_snippet_13 LANGUAGE: JavaScript
CODE:
```
const docWritingLlm = new ChatOpenAI({ modelName: "gpt-4o" }); const docWritingNode = (state: typeof DocWritingState.State) => { const stateModifier = agentStateModifier( `You are an expert writing a research document.\nBelow are files currently in your directory:\n${state.current_files}`, [writeDocumentTool, editDocumentTool, readDocumentTool], state.team_members ?? [], ) const docWriterAgent = createReactAgent({ llm: docWritingLlm, tools: [writeDocumentTool, editDocumentTool, readDocumentTool], stateModifier, }) const contextAwareDocWriterAgent = prelude.pipe(docWriterAgent); return runAgentNode({ state, agent: contextAwareDocWriterAgent, name: "DocWriter" });
} const noteTakingNode = (state: typeof DocWritingState.State) => { const stateModifier = agentStateModifier( "You are an expert senior researcher tasked with writing a paper outline and" + ` taking notes to craft a perfect paper. ${state.current_files}`, [createOutlineTool, readDocumentTool], state.team_members ?? [], ) const noteTakingAgent = createReactAgent({ llm: docWritingLlm, tools: [createOutlineTool, readDocumentTool], stateModifier, }) const contextAwareNoteTakingAgent = prelude.pipe(noteTakingAgent); return runAgentNode({ state, agent: contextAwareNoteTakingAgent, name: "NoteTaker" });
} const chartGeneratingNode = async ( state: typeof DocWritingState.State,
) => { const stateModifier = agentStateModifier( "You are a data viz expert tasked with generating charts for a research project." + `${state.current_files}`, [readDocumentTool, chartTool], state.team_members ?? [], ) const chartGeneratingAgent = createReactAgent({ llm: docWritingLlm, tools: [readDocumentTool, chartTool], stateModifier, }) const contextAwareChartGeneratingAgent = prelude.pipe(chartGeneratingAgent); return runAgentNode({ state, agent: contextAwareChartGeneratingAgent, name: "ChartGenerator" });
} const docTeamMembers = ["DocWriter", "NoteTaker", "ChartGenerator"];
const docWritingSupervisor = await createTeamSupervisor( docWritingLlm, "You are a supervisor tasked with managing a conversation between the" + " following workers: {team_members}. Given the following user request," + " respond with the worker to act next. Each worker will perform a" + " task and respond with their results and status. When finished," + " respond with FINISH.\n\n" +
``` ---------------------------------------- TITLE: Defining LangGraphJS State Schema with Annotation
DESCRIPTION: Defines the structure of the graph's state using LangGraph's Annotation class, specifying a 'messages' field that uses a reducer to concatenate new messages.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/time-travel.ipynb#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { Annotation } from "@langchain/langgraph";
import { BaseMessage } from "@langchain/core/messages"; const StateAnnotation = Annotation.Root({ messages: Annotation<BaseMessage[]> ({ reducer: (x, y) => x.concat(y), }),
});
``` ---------------------------------------- TITLE: Build Research Team Graph Structure (LangGraphJS, JS/TS)
DESCRIPTION: Initializes a StateGraph with the defined ResearchTeamState. It adds the 'Search', 'supervisor', and 'WebScraper' nodes and defines the edges that control the flow, including conditional transitions from the 'supervisor' node based on its output.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb#_snippet_9 LANGUAGE: JavaScript
CODE:
```
import { END, START, StateGraph } from "@langchain/langgraph"; const researchGraph = new StateGraph(ResearchTeamState) .addNode("Search", searchNode) .addNode("supervisor", supervisorAgent) .addNode("WebScraper", researchNode) // Define the control flow .addEdge("Search", "supervisor") .addEdge("WebScraper", "supervisor") .addConditionalEdges("supervisor", (x) => x.next, { Search: "Search", WebScraper: "WebScraper", FINISH: END, }) .addEdge(START, "supervisor"); const researchChain = researchGraph.compile();
``` ---------------------------------------- TITLE: Building Orchestrator-Worker Graph with LangGraph (TypeScript)
DESCRIPTION: This snippet defines the state schemas for both the main orchestrator graph (`StateAnnotation`) and individual worker nodes (`WorkerStateAnnotation`) using LangGraph's `Annotation`. It implements the core nodes: `orchestrator` (plans sections), `llmCall` (generates a single section), and `synthesizer` (combines sections). The `assignWorkers` function uses the `Send` API to dynamically create `llmCall` worker nodes for each planned section, demonstrating the key feature of this pattern. Finally, it constructs the `StateGraph` defining the workflow transitions.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_16 LANGUAGE: typescript
CODE:
```
import { Annotation, StateGraph, Send } from "@langchain/langgraph"; // Graph state
const StateAnnotation = Annotation.Root({ topic: Annotation<string>, sections: Annotation<Array<z.infer<typeof sectionSchema>>>, completedSections: Annotation<string[]>({ default: () => [], reducer: (a, b) => a.concat(b), }), finalReport: Annotation<string>,
}); // Worker state
const WorkerStateAnnotation = Annotation.Root({ section: Annotation<z.infer<typeof sectionSchema>>, completedSections: Annotation<string[]>({ default: () => [], reducer: (a, b) => a.concat(b), }),
}); // Nodes
async function orchestrator(state: typeof StateAnnotation.State) { // Generate queries const reportSections = await planner.invoke([ { role: "system", content: "Generate a plan for the report." }, { role: "user", content: `Here is the report topic: ${state.topic}` }, ]); return { sections: reportSections.sections };
} async function llmCall(state: typeof WorkerStateAnnotation.State) { // Generate section const section = await llm.invoke([ { role: "system", content: "Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.", }, { role: "user", content: `Here is the section name: ${state.section.name} and description: ${state.section.description}`, }, ]); // Write the updated section to completed sections return { completedSections: [section.content] };
} async function synthesizer(state: typeof StateAnnotation.State) { // List of completed sections const completedSections = state.completedSections; // Format completed section to str to use as context for final sections const completedReportSections = completedSections.join("\n\n---\n\n"); return { finalReport: completedReportSections };
} // Conditional edge function to create llm_call workers that each write a section of the report
function assignWorkers(state: typeof StateAnnotation.State) { // Kick off section writing in parallel via Send() API return state.sections.map((section) => new Send("llmCall", { section }) );
} // Build workflow
const orchestratorWorker = new StateGraph(StateAnnotation) .addNode("orchestrator", orchestrator) .addNode("llmCall", llmCall) .addNode("synthesizer", synthesizer) .addEdge("__start__", "orchestrator") .addConditionalEdges( "orchestrator", assignWorkers, ["llmCall"] );
``` ---------------------------------------- TITLE: Defining Tools with LangGraph JS
DESCRIPTION: Demonstrates how to define a custom tool using the `@langchain/core/tools` `tool` function and `zod` for schema validation. It shows how to create a simple multiplication tool and include it when initializing a LangGraph agent.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/tools.md#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { initChatModel } from "langchain/chat_models/universal";
// highlight-next-line
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const multiply = tool( async (input: { a: number; b: number }) => { return input.a * input.b; }, { name: "multiply", schema: z.object({ a: z.number().describe("First operand"), b: z.number().describe("Second operand"), }), description: "Multiply two numbers.", }
); const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest");
const agent = createReactAgent({ llm, tools: [multiply],
});
``` ---------------------------------------- TITLE: Using `Send` Objects for Dynamic State Routing (TypeScript)
DESCRIPTION: This snippet demonstrates how to use `Send` objects within a conditional edge's routing function. `Send` allows dynamically routing to a specified node while passing a custom, isolated state to it. This is particularly useful for patterns like map-reduce, where multiple parallel executions with distinct inputs are required.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/low_level.md#_snippet_19 LANGUAGE: typescript
CODE:
```
const continueToJokes = (state: { subjects: string[] }) => { return state.subjects.map( (subject) => new Send("generate_joke", { subject }) );
}; const graph = new StateGraph(...) .addConditionalEdges("nodeA", continueToJokes) .compile();
``` ---------------------------------------- TITLE: Adding Custom Retry Policies to LangGraph Nodes (JS/TS)
DESCRIPTION: Illustrates how to apply custom retry policies to individual nodes within a LangGraph `StateGraph`. It shows configuring `maxAttempts` for one node and a custom `retryOn` function for another, demonstrating flexible error handling.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/node-retry-policies.ipynb#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import Database from "better-sqlite3"
import { ChatAnthropic } from "@langchain/anthropic"
import { MessagesAnnotation, StateGraph, START, END } from "@langchain/langgraph"
import { AIMessage } from "@langchain/core/messages" // Create an in-memory database
const db: typeof Database.prototype = new Database(':memory:'); const model = new ChatAnthropic({ model: "claude-3-5-sonnet-20240620" }); const callModel = async (state: typeof MessagesAnnotation.State) => { const response = await model.invoke(state.messages); return { messages: [response] };
} const queryDatabase = async (state: typeof MessagesAnnotation.State) => { const queryResult: string = JSON.stringify(db.prepare("SELECT * FROM Artist LIMIT 10;").all()); return { messages: [new AIMessage({content: "queryResult"})]};
}; const workflow = new StateGraph(MessagesAnnotation) // Define the two nodes we will cycle between .addNode("call_model", callModel, { retryPolicy: {maxAttempts: 5}}) .addNode("query_database", queryDatabase, { retryPolicy: { retryOn: (e: any): boolean => { if (e instanceof Database.SqliteError) { // Retry on "SQLITE_BUSY" error return e.code === 'SQLITE_BUSY'; } return false; // Don't retry on other errors }}}) .addEdge(START, "call_model") .addEdge("call_model", "query_database") .addEdge("query_database", END); const graph = workflow.compile();
``` ---------------------------------------- TITLE: Resuming LangGraph (Edit Tool Call) - TypeScript
DESCRIPTION: Demonstrates how to resume a paused LangGraph execution and edit the pending tool call. It uses the `Command` input with `resume: { action: "update", data: { ... } }` to provide updated arguments for the tool call and continues streaming the graph execution, logging subsequent messages.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/review-tool-calls.ipynb#_snippet_13 LANGUAGE: TypeScript
CODE:
```
for await (const event of await graph.stream( new Command({ resume: { action: "update", data: { city: "San Francisco" } } }), config
)) { const recentMsg = event.messages[event.messages.length - 1]; console.log(`================================ ${recentMsg._getType()} Message (1) =================================`) console.log(recentMsg.content);
}
``` ---------------------------------------- TITLE: Implementing Agent Routing Logic in LangGraphJS (TypeScript)
DESCRIPTION: Defines a LangGraphJS node function `agent` that determines the next step based on internal logic (e.g., LLM output). It returns a `Command` to either route to another agent/itself or route back to the `human` node for more input.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/multi-agent-multi-turn-convo.ipynb#_snippet_1 LANGUAGE: TypeScript
CODE:
```
function agent(state: typeof MessagesAnnotation.State): Command { // The condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc. const goto = getNextAgent(...); // 'agent' / 'anotherAgent' if (goto) { return new Command({ goto, update: { myStateKey: "myStateValue" } }); } else { return new Command({ goto: "human" }); }
}
``` ---------------------------------------- TITLE: Implementing Custom Tool Executor with Command Handling in TypeScript
DESCRIPTION: This snippet demonstrates how to define a custom tool that returns a Command object and a tool executor function that processes tool calls, handles Command outputs, and returns state updates for a LangGraph StateGraph. It shows how to integrate this custom executor into a simple graph.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/update-state-from-tools.ipynb#_snippet_9 LANGUAGE: typescript
CODE:
```
import { MessagesAnnotation, isCommand, Command, StateGraph,
} from "@langchain/langgraph";
import { tool } from "@langchain/core/tools";
import { isAIMessage } from "@langchain/core/messages"; import { z } from "zod"; const myTool = tool(async () => { return new Command({ update: { messages: [ { role: "assistant", content: "hi there!", name: "Greeter", } ], }, });
}, { name: "greeting", description: "Updates the current state with a greeting", schema: z.object({}),
}); const toolExecutor = async (state: typeof MessagesAnnotation.State) => { const message = state.messages.at(-1); if (!isAIMessage(message) || message.tool_calls === undefined || message.tool_calls.length === 0) { throw new Error("Most recent message must be an AIMessage with a tool call.") } const outputs = await Promise.all( message.tool_calls.map(async (toolCall) => { // Using a single tool for simplicity, would need to select tools by toolCall.name // in practice. const toolResult = await myTool.invoke(toolCall); return toolResult; }) ); // Handle mixed Command and non-Command outputs const combinedOutputs = outputs.map((output) => { if (isCommand(output)) { return output; } // Tool invocation result is a ToolMessage, return a normal state update return { messages: [output] }; }); // Return an array of values instead of an object return combinedOutputs;
}; // Simple one node graph
const customGraph = new StateGraph(MessagesAnnotation) .addNode("runTools", toolExecutor) .addEdge("__start__", "runTools") .compile(); await customGraph.invoke({ messages: [{ role: "user", content: "how are you?", }, { role: "assistant", content: "Let me call the greeting tool and find out!", tool_calls: [{ id: "123", args: {}, name: "greeting", }], }],
});
``` ---------------------------------------- TITLE: Defining Multi-Turn Agent Conversation Graph (TypeScript)
DESCRIPTION: Defines the main `multiTurnGraph` using `entrypoint`. This graph orchestrates the conversation flow, managing which agent is active (`callActiveAgent`), processing agent responses, handling agent-to-agent transfers based on tool calls (`transferToHotelAdvisor`, `transferToTravelAdvisor`), incorporating user input via interrupts (`interrupt`), and saving/restoring state using the `checkpointer`.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/multi-agent-multi-turn-convo-functional.ipynb#_snippet_9 LANGUAGE: TypeScript
CODE:
```
const multiTurnGraph = entrypoint({ name: "multiTurnGraph", checkpointer
}, async (messages: BaseMessageLike[]) => { let callActiveAgent = callTravelAdvisor; let agentMessages: BaseMessage[]; let currentMessages = messages; while (true) { agentMessages = await callActiveAgent(currentMessages); // Find the last AI message // If one of the handoff tools is called, the last message returned // by the agent will be a ToolMessages because we set them to have // "returnDirect: true". This means that the last AIMessage will // have tool calls. // Otherwise, the last returned message will be an AIMessage with // no tool calls, which means we are ready for new input. const reversedMessages = [...agentMessages].reverse(); const aiMsgIndex = reversedMessages .findIndex((m): m is AIMessage => m.getType() === "ai"); const aiMsg: AIMessage = reversedMessages[aiMsgIndex]; // We append all messages up to the last AI message to the current messages. // This may include ToolMessages (if the handoff tool was called) const messagesToAdd = reversedMessages.slice(0, aiMsgIndex + 1).reverse(); // Add the agent's responses currentMessages = addMessages(currentMessages, messagesToAdd); if (!aiMsg?.tool_calls?.length) { const userInput = await interrupt("Ready for user input."); if (typeof userInput !== "string") { throw new Error("User input must be a string."); } if (userInput.toLowerCase() === "done") { break; } currentMessages = addMessages(currentMessages, [{ role: "human", content: userInput, }]); continue; } const toolCall = aiMsg.tool_calls.at(-1)!; if (toolCall.name === "transferToHotelAdvisor") { callActiveAgent = callHotelAdvisor; } else if (toolCall.name === "transferToTravelAdvisor") { callActiveAgent = callTravelAdvisor; } else { throw new Error(`Expected transfer tool, got '${toolCall.name}'`); } } return entrypoint.final({ value: agentMessages[agentMessages.length - 1], save: currentMessages, });
});
``` ---------------------------------------- TITLE: Defining Simple Graph State Channels with Annotation
DESCRIPTION: This snippet defines `QuestionAnswerAnnotation` using `Annotation.Root` to declare two simple channels, `question` and `answer`, both typed as `string`. This demonstrates defining channels without explicit `reducer` or `default` functions, relying on the implicit behavior of `Annotation<Type>`.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/define-state.ipynb#_snippet_2 LANGUAGE: typescript
CODE:
```
const QuestionAnswerAnnotation = Annotation.Root({ question: Annotation<string>, answer: Annotation<string>,
});
``` ---------------------------------------- TITLE: Define and Compile LangGraph StateGraph
DESCRIPTION: Defines the state structure, asynchronous nodes for text generation and reflection, sets up the graph with nodes and edges, including a conditional edge to control iteration, and compiles the workflow with a memory saver for state persistence.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/reflection/reflection.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
import { END, MemorySaver, StateGraph, START, Annotation } from "@langchain/langgraph"; // Define the top-level State interface
const State = Annotation.Root({ messages: Annotation<BaseMessage[]> ({ reducer: (x, y) => x.concat(y), })
}) const generationNode = async (state: typeof State.State) => { const { messages } = state; return { messages: [await essayGenerationChain.invoke({ messages })], };
}; const reflectionNode = async (state: typeof State.State) => { const { messages } = state; // Other messages we need to adjust const clsMap: { [key: string]: new (content: string) => BaseMessage } = { ai: HumanMessage, human: AIMessage, }; // First message is the original user request. We hold it the same for all nodes const translated = [ messages[0], ...messages .slice(1) .map((msg) => new clsMap[msg._getType()](msg.content.toString())), ]; const res = await reflect.invoke({ messages: translated }); // We treat the output of this as human feedback for the generator return { messages: [new HumanMessage({ content: res.content })], };
}; // Define the graph
const workflow = new StateGraph(State) .addNode("generate", generationNode) .addNode("reflect", reflectionNode) .addEdge(START, "generate"); const shouldContinue = (state: typeof State.State) => { const { messages } = state; if (messages.length > 6) { // End state after 3 iterations return END; } return "reflect";
}; workflow .addConditionalEdges("generate", shouldContinue) .addEdge("reflect", "generate"); const app = workflow.compile({ checkpointer: new MemorySaver() });
``` ---------------------------------------- TITLE: Defining LangGraph.js Workflow with Memory
DESCRIPTION: Sets up a LangGraph workflow including a task to call the chat model and an entrypoint function. It uses MemorySaver as a checkpointer to persist conversation history across interactions. The workflow retrieves previous messages, adds new inputs, calls the model, and saves the combined messages.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/persistence-functional.ipynb#_snippet_7 LANGUAGE: JavaScript
CODE:
```
import type { BaseMessage, BaseMessageLike } from "@langchain/core/messages";
import { addMessages, entrypoint, task, getPreviousState, MemorySaver
} from "@langchain/langgraph"; const callModel = task("callModel", async (messages: BaseMessageLike[]) => { const response = model.invoke(messages); return response;
}); const checkpointer = new MemorySaver(); const workflow = entrypoint({ name: "workflow", checkpointer
}, async (inputs: BaseMessageLike[]) => { const previous = getPreviousState<BaseMessage>() ?? []; const messages = addMessages(previous, inputs); const response = await callModel(messages); return entrypoint.final({ value: response, save: addMessages(messages, response) });
});
``` ---------------------------------------- TITLE: Implementing Human Input Node in LangGraphJS (TypeScript)
DESCRIPTION: Defines a LangGraphJS node function `human` that uses `interrupt` to prompt for user input. It creates a `Command` to update the state with the human message and route back to the active agent.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/multi-agent-multi-turn-convo.ipynb#_snippet_0 LANGUAGE: TypeScript
CODE:
```
function human(state: typeof MessagesAnnotation.State): Command { const userInput: string = interrupt("Ready for user input."); // Determine the active agent const activeAgent = ...; return new Command({ update: { messages: [{ role: "human", content: userInput, }] }, goto: activeAgent, });
}
``` ---------------------------------------- TITLE: Defining Agent Tasks (Langchain.js)
DESCRIPTION: Defines two core tasks for the agent's workflow: 'callModel' which invokes the bound model with messages and returns the response, and 'callTool' which executes a given tool call using the defined tools and returns a 'ToolMessage' with the observation. It also creates a map of tools by name for easy lookup.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/wait-user-input-functional.ipynb#_snippet_8 LANGUAGE: javascript
CODE:
```
import { type BaseMessageLike, AIMessage, ToolMessage,
} from "@langchain/core/messages";
import { type ToolCall } from "@langchain/core/messages/tool";
import { task } from "@langchain/langgraph"; const toolsByName = Object.fromEntries(tools.map((tool) => [tool.name, tool])); const callModel = task("callModel", async (messages: BaseMessageLike[]) => { const response = await model.bindTools(tools).invoke(messages); return response;
}); const callTool = task( "callTool", async (toolCall: ToolCall): Promise<AIMessage> => { const tool = toolsByName[toolCall.name]; const observation = await tool.invoke(toolCall.args); return new ToolMessage({ content: observation, tool_call_id: toolCall.id }); // Can also pass toolCall directly into the tool to return a ToolMessage // return tool.invoke(toolCall); });
``` ---------------------------------------- TITLE: Compile and Invoke LangGraph Workflow (Compiled API)
DESCRIPTION: Compiles a LangGraph workflow defined using `.addEdge` and invokes it with a specific input topic. Prints the final report from the state.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_17 LANGUAGE: TypeScript
CODE:
```
.addEdge("llmCall", "synthesizer") .addEdge("synthesizer", "__end__") .compile(); // Invoke const state = await orchestratorWorker.invoke({ topic: "Create a report on LLM scaling laws" }); console.log(state.finalReport);
``` ---------------------------------------- TITLE: Accessing Tool Context via Agent State in LangGraph (TypeScript)
DESCRIPTION: This snippet shows how a LangChain tool within a LangGraph agent can access custom state defined for the agent. It uses getCurrentTaskInput() to get the current state and retrieves a userId from a custom userId field added to the state schema using Annotation. This allows tools to read information directly from the agent's state.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/context.md#_snippet_5 LANGUAGE: TypeScript
CODE:
```
import { initChatModel } from "langchain/chat_models/universal";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { Annotation, MessagesAnnotation, getCurrentTaskInput } from "@langchain/langgraph";
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const CustomState = Annotation.Root({ ...MessagesAnnotation.spec, // highlight-next-line userId: Annotation<string>(),
}); const getUserInfo = tool( async ( input: Record<string, any>, ) => { // highlight-next-line const state = getCurrentTaskInput() as typeof CustomState.State; // highlight-next-line const userId = state.userId; return userId === "user_123" ? "User is John Smith" : "Unknown user"; }, { name: "get_user_info", description: "Look up user info.", schema: z.object({}) }
); const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest");
const agent = createReactAgent({ llm, tools: [getUserInfo], // highlight-next-line stateSchema: CustomState,
}); await agent.invoke( // highlight-next-line { messages: "look up user information", userId: "user_123" }
);
``` ---------------------------------------- TITLE: Defining LangGraph Agent Entrypoint with Human Review (TypeScript)
DESCRIPTION: Defines the main `entrypoint` for the LangGraph agent. It manages the conversation state, calls the model, processes tool calls by first reviewing them using `reviewToolCall`, executes the accepted/revised calls, and continues the loop until the model provides a final response without tool calls. It uses `MemorySaver` for checkpointing.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/review-tool-calls-functional.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { MemorySaver, addMessages, entrypoint, getPreviousState,
} from "@langchain/langgraph"; const checkpointer = new MemorySaver(); const agent = entrypoint({ checkpointer, name: "agent",
}, async (messages: BaseMessageLike[]) => { const previous = getPreviousState<BaseMessageLike[]>() ?? []; let currentMessages = addMessages(previous, messages); let llmResponse = await callModel(currentMessages); while (true) { if (!llmResponse.tool_calls?.length) { break; } // Review tool calls const toolResults: ToolMessage[] = []; const toolCalls: ToolCall[] = []; for (let i = 0; i < llmResponse.tool_calls.length; i++) { const review = await reviewToolCall(llmResponse.tool_calls[i]); if (review instanceof ToolMessage) { toolResults.push(review); } else { // is a validated tool call toolCalls.push(review); if (review !== llmResponse.tool_calls[i]) { llmResponse.tool_calls[i] = review; } } } // Execute remaining tool calls const remainingToolResults = await Promise.all( toolCalls.map((toolCall) => callTool(toolCall)) ); // Append to message list currentMessages = addMessages( currentMessages, [llmResponse, ...toolResults, ...remainingToolResults] ); // Call model again llmResponse = await callModel(currentMessages); } // Generate final response currentMessages = addMessages(currentMessages, llmResponse); return entrypoint.final({ value: llmResponse, save: currentMessages });
});
``` ---------------------------------------- TITLE: Stream Custom Data and Updates using .stream (TypeScript)
DESCRIPTION: Shows how to use the `.stream` method with multiple stream modes, specifically `["custom", "updates"]`. This allows receiving both the custom data chunks and the state updates from the graph execution.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/streaming-content.ipynb#_snippet_3 LANGUAGE: typescript
CODE:
```
const streamMultiple = await graph.stream( { messages: inputs }, { streamMode: ["custom", "updates"] }
); for await (const chunk of streamMultiple) { console.log(chunk);
}
``` ---------------------------------------- TITLE: Implementing Multi-Agent Handoff System (TypeScript)
DESCRIPTION: A comprehensive example showing how to set up a multi-agent system with handoffs. It includes necessary imports, a helper function `createHandoffTool` to generate handoff tools dynamically, definitions for functional tools (`bookHotel`, `bookFlight`), creation of specific handoff tool instances, LLM initialization, agent creation with assigned tools and names, and the beginning of the graph definition.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/multi-agent.md#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { ChatAnthropic } from "@langchain/anthropic";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { StateGraph, MessagesAnnotation, Command, START, getCurrentTaskInput, END } from "@langchain/langgraph";
import { tool } from "@langchain/core/tools";
import { z } from "zod";
import { ToolMessage } from "@langchain/core/messages"; interface CreateHandoffToolParams { agentName: string; description?: string;
} const createHandoffTool = ({ agentName, description,
}: CreateHandoffToolParams) => { const toolName = `transfer_to_${agentName}`; const toolDescription = description || `Ask agent '${agentName}' for help`; const handoffTool = tool( async (_, config) => { const toolMessage = new ToolMessage({ content: `Successfully transferred to ${agentName}`, name: toolName, tool_call_id: config.toolCall.id, }); // inject the current agent state const state = // highlight-next-line getCurrentTaskInput() as (typeof MessagesAnnotation)["State"]; // (1)! return new Command({ // (2)! // highlight-next-line goto: agentName, // (3)! // highlight-next-line update: { messages: state.messages.concat(toolMessage) }, // (4)! // highlight-next-line graph: Command.PARENT, // (5)! }); }, { name: toolName, schema: z.object({}), description: toolDescription, } ); return handoffTool;
}; const bookHotel = tool( async (input: { hotel_name: string }) => { return `Successfully booked a stay at ${input.hotel_name}.`; }, { name: "book_hotel", description: "Book a hotel", schema: z.object({ hotel_name: z.string().describe("The name of the hotel to book"), }), }
); const bookFlight = tool( async (input: { from_airport: string; to_airport: string }) => { return `Successfully booked a flight from ${input.from_airport} to ${input.to_airport}.`; }, { name: "book_flight", description: "Book a flight", schema: z.object({ from_airport: z.string().describe("The departure airport code"), to_airport: z.string().describe("The arrival airport code"), }), }
); const transferToHotelAssistant = createHandoffTool({ agentName: "hotel_assistant", description: "Transfer user to the hotel-booking assistant.",
}); const transferToFlightAssistant = createHandoffTool({ agentName: "flight_assistant", description: "Transfer user to the flight-booking assistant.",
}); const llm = new ChatAnthropic({ modelName: "claude-3-5-sonnet-latest" }); const flightAssistant = createReactAgent({ llm, // highlight-next-line tools: [bookFlight, transferToHotelAssistant], prompt: "You are a flight booking assistant", // highlight-next-line name: "flight_assistant",
}); const hotelAssistant = createReactAgent({ llm, // highlight-next-line tools: [bookHotel, transferToFlightAssistant], prompt: "You are a hotel booking assistant", // highlight-next-line ``` ---------------------------------------- TITLE: Augment LLM with Structured Output
DESCRIPTION: Defines a Zod schema for a search query and justification, then augments the initialized LLM to enforce this schema for structured output, demonstrating how to get predictable JSON-like responses.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_2 LANGUAGE: typescript
CODE:
```
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const searchQuerySchema = z.object({ searchQuery: z.string().describe("Query that is optimized web search."), justification: z.string("Why this query is relevant to the user's request.")
}); // Augment the LLM with schema for structured output
const structuredLlm = llm.withStructuredOutput(searchQuerySchema, { name: "searchQuery"
}); // Invoke the augmented LLM
const output = await structuredLlm.invoke( "How does Calcium CT score relate to high cholesterol?"
);
``` ---------------------------------------- TITLE: Defining LangGraph StateGraph Workflow - JavaScript
DESCRIPTION: This snippet defines a LangGraph workflow using StateGraph. It initializes the graph with a state, adds two nodes ('callModel' and 'executeTools'), sets 'callModel' as the entry point, defines a conditional edge from 'callModel' based on the 'shouldContinue' function, and adds a direct edge from 'executeTools' back to 'callModel'. Finally, it compiles the workflow.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/agent_executor/base.ipynb#_snippet_6 LANGUAGE: javascript
CODE:
```
import { START, StateGraph } from "@langchain/langgraph"; // Define a new graph
const workflow = new StateGraph(AgentState) // Define the two nodes we will cycle between .addNode("callModel", callModel) .addNode("executeTools", toolNode) // Set the entrypoint as `callModel` // This means that this node is the first one called .addEdge(START, "callModel") // We now add a conditional edge .addConditionalEdges( // First, we define the start node. We use `callModel`. // This means these are the edges taken after the `agent` node is called. "callModel", // Next, we pass in the function that will determine which node is called next. shouldContinue, ) // We now add a normal edge from `tools` to `agent`. // This means that after `tools` is called, `agent` node is called next. .addEdge("executeTools", "callModel"); const app = workflow.compile();
``` ---------------------------------------- TITLE: Defining Edge Router Logic (TypeScript)
DESCRIPTION: Implements the routing logic for conditional edges in the graph. The `router` function examines the last message in the state to determine the next step: "call_tool" if the agent requested tools, "end" if a final answer is indicated, or "continue" otherwise.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/multi_agent/multi_agent_collaboration.ipynb#_snippet_8 LANGUAGE: typescript
CODE:
```
import { AIMessage } from "@langchain/core/messages";
// Either agent can decide to end
function router(state: typeof AgentState.State) { const messages = state.messages; const lastMessage = messages[messages.length - 1] as AIMessage; if (lastMessage?.tool_calls && lastMessage.tool_calls.length > 0) { // The previous agent is invoking a tool return "call_tool"; } if ( typeof lastMessage.content === "string" && lastMessage.content.includes("FINAL ANSWER") ) { // Any agent decided the work is done return "end"; } return "continue";
}
``` ---------------------------------------- TITLE: Implementing LangGraph with ToolNode for Custom Input
DESCRIPTION: Demonstrates building a LangGraph application using the standard ToolNode to interact with a tool that expects a specific input schema (an array of three strings). It defines the tool, the graph structure with agent and tool nodes, conditional logic for transitions, and shows how to invoke the graph.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/tool-calling-errors.ipynb#_snippet_5 LANGUAGE: TypeScript
CODE:
```
import { StringOutputParser } from "@langchain/core/output_parsers";
import { tool } from "@langchain/core/tools";
import { ChatAnthropic } from "@langchain/anthropic";
import { z } from "zod";
import { StateGraph, ToolNode } from "@langchain/langgraph";
import { isAIMessage } from "@langchain/core/messages";
import { MessagesAnnotation } from "@langchain/langgraph/prebuilt"; const haikuRequestSchema = z.object({ topic: z.array(z.string()).length(3),
}); const masterHaikuGenerator = tool(async ({ topic }) => { const model = new ChatAnthropic({ model: "claude-3-haiku-20240307", temperature: 0, }); const chain = model.pipe(new StringOutputParser()); const topics = topic.join(", "); const haiku = await chain.invoke(`Write a haiku about ${topics}`); return haiku;
}, { name: "master_haiku_generator", description: "Generates a haiku based on the provided topics.", schema: haikuRequestSchema,
}); const customStrategyToolNode = new ToolNode([masterHaikuGenerator]); const customStrategyModel = new ChatAnthropic({ model: "claude-3-haiku-20240307", temperature: 0,
});
const customStrategyModelWithTools = customStrategyModel.bindTools([masterHaikuGenerator]); const customStrategyShouldContinue = async (state: typeof MessagesAnnotation.State) => { const { messages } = state; const lastMessage = messages[messages.length - 1]; if (isAIMessage(lastMessage) && lastMessage.tool_calls?.length) { return "tools"; } return "__end__";
} const customStrategyCallModel = async (state: typeof MessagesAnnotation.State) => { const { messages } = state; const response = await customStrategyModelWithTools.invoke(messages); return { messages: [response] };
} const customStrategyApp = new StateGraph(MessagesAnnotation) .addNode("tools", customStrategyToolNode) .addNode("agent", customStrategyCallModel) .addEdge("__start__", "agent") .addEdge("tools", "agent") .addConditionalEdges("agent", customStrategyShouldContinue, { // Explicitly list possible destinations so that // we can automatically draw the graph below. tools: "tools", __end__: "__end__", }) .compile(); const response2 = await customStrategyApp.invoke( { messages: [{ role: "user", content: "Write me an incredible haiku about water." }], }, { recursionLimit: 10 }
); for (const message of response2.messages) { // Anthropic returns tool calls in content as well as in `AIMessage.tool_calls` const content = JSON.stringify(message.content, null, 2); console.log(`${message._getType().toUpperCase()}: ${content}`);
}
``` ---------------------------------------- TITLE: Handoff from Subgraph Node to Parent Graph (TypeScript)
DESCRIPTION: Illustrates how a node inside a subgraph (like an agent represented as a subgraph) can use Command.PARENT in the 'graph' property to route execution to a node ("bob") in the parent graph. This is useful for complex hierarchical multi-agent systems where sub-agents need to handoff to other top-level agents.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/multi_agent.md#_snippet_1 LANGUAGE: TypeScript
CODE:
```
const some_node_inside_alice = (state) => { return new Command({ goto: "bob", update: { foo: "bar", }, // specify which graph to navigate to (defaults to the current graph) graph: Command.PARENT, })
}
``` ---------------------------------------- TITLE: Defining Dynamic Prompts with LangChain/LangGraph (TypeScript)
DESCRIPTION: Illustrates how to define a dynamic prompt as a function that accepts the agent's state and runtime configuration. This allows the prompt to be constructed dynamically, incorporating context such as user-specific information or internal agent state before being sent to the LLM.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/agents.md#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { BaseMessageLike } from "@langchain/core/messages";
import { RunnableConfig } from "@langchain/core/runnables";
import { initChatModel } from "langchain/chat_models/universal";
import { MessagesAnnotation } from "@langchain/langgraph";
import { createReactAgent } from "@langchain/langgraph/prebuilt"; const prompt = ( state: typeof MessagesAnnotation.State, config: RunnableConfig
): BaseMessageLike[] => { // (1)! const userName = config.configurable?.userName; const systemMsg = `You are a helpful assistant. Address the user as ${userName}.`; return [{ role: "system", content: systemMsg }, ...state.messages];
}; const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest");
const agent = createReactAgent({ llm, tools: [getWeather], // highlight-next-line prompt
}); await agent.invoke( { messages: [ { role: "user", content: "what is the weather in sf" } ] }, // highlight-next-line { configurable: { userName: "John Smith" } }
);
``` ---------------------------------------- TITLE: Customize Prompt Using Custom State in LangGraph (TypeScript)
DESCRIPTION: This snippet shows how to define a custom state schema that includes additional context, like a user name. The prompt function accesses this context directly from the state object, and the context is passed as part of the state during the `invoke` call.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/context.md#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { BaseMessageLike } from "@langchain/core/messages";
import { RunnableConfig } from "@langchain/core/runnables";
import { initChatModel } from "langchain/chat_models/universal";
import { Annotation, MessagesAnnotation } from "@langchain/langgraph";
import { createReactAgent } from "@langchain/langgraph/prebuilt"; const CustomState = Annotation.Root({ ...MessagesAnnotation.spec, // highlight-next-line userName: Annotation<string>,
}); const prompt = ( // highlight-next-line state: typeof CustomState.State,
): BaseMessageLike[] => { // highlight-next-line const userName = state.userName; const systemMsg = `You are a helpful assistant. Address the user as ${userName}.`; return [{ role: "system", content: systemMsg }, ...state.messages];
}; const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest");
const agent = createReactAgent({ llm, tools: [getWeather], // highlight-next-line prompt, // highlight-next-line stateSchema: CustomState,
}); await agent.invoke( // highlight-next-line { messages: "hi!", userName: "John Smith" },
);
``` ---------------------------------------- TITLE: Binding Tools to ChatOpenAI Model (TypeScript)
DESCRIPTION: Uses the `bindTools` method to inform the `ChatOpenAI` model about the available tools. This allows the model to generate tool call requests in its responses.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/stream-updates.ipynb#_snippet_5 LANGUAGE: TypeScript
CODE:
```
const boundModel = model.bindTools(tools);
``` ---------------------------------------- TITLE: Resume LangGraph Stream with Update Command - TypeScript
DESCRIPTION: Creates a `Command` object with the `resume` action set to `update` and provides updated data (`{ location: "SF, CA" }`) to revise the arguments of a pending tool call, then streams the result.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/review-tool-calls-functional.ipynb#_snippet_11 LANGUAGE: TypeScript
CODE:
```
// highlight-next-line
const humanInput2 = new Command({ // highlight-next-line resume: { // highlight-next-line action: "update", // highlight-next-line data: { location: "SF, CA" }, // highlight-next-line }, // highlight-next-line
}); const resumedStream2 = await agent.stream(humanInput2, config2) for await (const step of resumedStream2) { printStep(step);
}
``` ---------------------------------------- TITLE: Implement Review & Edit State Pattern with LangGraph.js (TypeScript)
DESCRIPTION: This snippet shows how to pause a LangGraph.js workflow to allow a human to review and edit the graph state using the `interrupt` function. The function returns the human's input, which is then used to update the state object. It requires the `@langchain/langgraph` library.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/human_in_the_loop.md#_snippet_5 LANGUAGE: typescript
CODE:
```
import { interrupt } from "@langchain/langgraph"; function humanEditing(state: typeof GraphAnnotation.State): Command { const result = interrupt({ // Interrupt information to surface to the client. // Can be any JSON serializable value. task: "Review the output from the LLM and make any necessary edits.", llm_generated_summary: state.llm_generated_summary, }); // Update the state with the edited text return { llm_generated_summary: result.edited_text, };
} // Add the node to the graph in an appropriate location
// and connect it to the relevant nodes.
const graph = graphBuilder .addNode("human_editing", humanEditing) .compile({ checkpointer }); // After running the graph and hitting the interrupt, the graph will pause.
// Resume it with the edited text.
const threadConfig = { configurable: { thread_id: "some_id" } };
await graph.invoke( new Command({ resume: { edited_text: "The edited text" } }), threadConfig
);
``` ---------------------------------------- TITLE: Customize Prompt Using RunnableConfig in LangGraph (TypeScript)
DESCRIPTION: This snippet demonstrates how to access runtime context, such as a user name, from the `RunnableConfig` object within the prompt function. The context is passed to the agent during the `invoke` call via the `configurable` property.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/context.md#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { BaseMessageLike } from "@langchain/core/messages";
import { RunnableConfig } from "@langchain/core/runnables";
import { initChatModel } from "langchain/chat_models/universal";
import { MessagesAnnotation } from "@langchain/langgraph";
import { createReactAgent } from "@langchain/langgraph/prebuilt"; const prompt = ( state: typeof MessagesAnnotation.State, // highlight-next-line config: RunnableConfig
): BaseMessageLike[] => { // highlight-next-line const userName = config.configurable?.userName; const systemMsg = `You are a helpful assistant. Address the user as ${userName}.`; return [{ role: "system", content: systemMsg }, ...state.messages];
}; const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest");
const agent = createReactAgent({ llm, tools: [getWeather], // highlight-next-line prompt
}); await agent.invoke( { messages: "hi!" }, // highlight-next-line { configurable: { userName: "John Smith" } }
);
``` ---------------------------------------- TITLE: Define LangGraph with Subgraph Node (TypeScript)
DESCRIPTION: Defines a LangGraph parent graph that includes a subgraph as one of its nodes. It sets up state annotations for both the subgraph and the parent graph and defines simple nodes within each. The subgraph is compiled before being added as a node to the parent graph builder.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/subgraph-persistence.ipynb#_snippet_1 LANGUAGE: typescript
CODE:
```
import { StateGraph, Annotation } from "@langchain/langgraph"; // subgraph const SubgraphStateAnnotation = Annotation.Root({ foo: Annotation<string>, bar: Annotation<string>,
}); const subgraphNode1 = async (state: typeof SubgraphStateAnnotation.State) => { return { bar: "bar" };
}; const subgraphNode2 = async (state: typeof SubgraphStateAnnotation.State) => { // note that this node is using a state key ('bar') that is only available in the subgraph // and is sending update on the shared state key ('foo') return { foo: state.foo + state.bar };
}; const subgraph = new StateGraph(SubgraphStateAnnotation) .addNode("subgraphNode1", subgraphNode1) .addNode("subgraphNode2", subgraphNode2) .addEdge("__start__", "subgraphNode1") .addEdge("subgraphNode1", "subgraphNode2") .compile(); // parent graph
const StateAnnotation = Annotation.Root({ foo: Annotation<string>,
}); const node1 = async (state: typeof StateAnnotation.State) => { return { foo: "hi! " + state.foo, };
}; const builder = new StateGraph(StateAnnotation) .addNode("node1", node1) // note that we're adding the compiled subgraph as a node to the parent graph .addNode("node2", subgraph) .addEdge("__start__", "node1") .addEdge("node1", "node2");
``` ---------------------------------------- TITLE: Interrupting and Resuming Graph Execution as a Subgraph Node (JavaScript)
DESCRIPTION: This snippet shows how to interrupt a graph execution before a specific node within a subgraph, retrieve the state, manually update the state as if the target node ('weatherNode') had produced the output, and then resume the graph execution. It demonstrates controlling the output of a specific node without executing its logic.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/subgraphs-manage-state.ipynb#_snippet_19 LANGUAGE: javascript
CODE:
```
const streamWithAsNode = await graph.stream({ messages: [{ role: "user", content: "What's the weather in sf", }]
}, { configurable: { thread_id: "14", }
}); for await (const update of streamWithAsNode) { console.log(update);
} // Graph execution should stop before the weather node
console.log("interrupted!"); const interruptedState = await graph.getState({ configurable: { thread_id: "14", }
}, { subgraphs: true }); console.log(interruptedState); // We update the state by passing in the message we want returned from the weather node
// and make sure to pass `"weatherNode"` to signify that we want to act as this node.
await graph.updateState((interruptedState.tasks[0].state as StateSnapshot).config, { messages: [{ "role": "assistant", "content": "rainy" }]
}, "weatherNode"); const resumedStreamWithAsNode = await graph.stream(null, { configurable: { thread_id: "14", }, streamMode: "updates", subgraphs: true,
}); for await (const update of resumedStreamWithAsNode) { console.log(update);
} console.log(await graph.getState({ configurable: { thread_id: "14", }
}, { subgraphs: true }));
``` ---------------------------------------- TITLE: Configuring LangGraph Agents and Tasks in JS
DESCRIPTION: This snippet initializes the language model and defines the tools available to each agent (`travelAdvisorTools`, `hotelAdvisorTools`). It then creates two ReAct agents using `createReactAgent`, assigning their respective tools and defining their system prompts (`stateModifier`). It also shows how to define a task to invoke an agent within the graph.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/multi-agent-network-functional.ipynb#_snippet_4 LANGUAGE: TypeScript
CODE:
```
import { AIMessage, type BaseMessageLike
} from "@langchain/core/messages";
import { ChatAnthropic } from "@langchain/anthropic";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { addMessages, entrypoint, task,
} from "@langchain/langgraph"; const model = new ChatAnthropic({ model: "claude-3-5-sonnet-latest",
}); const travelAdvisorTools = [ getTravelRecommendations, transferToHotelAdvisor,
]; // Define travel advisor ReAct agent
const travelAdvisor = createReactAgent({ llm: model, tools: travelAdvisorTools, stateModifier: [ "You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc).", "If you need hotel recommendations, ask 'hotel_advisor' for help.", "You MUST include human-readable response before transferring to another agent.", ].join(" "),
}); // You can also add additional logic like changing the input to the agent / output from the agent, etc.
// NOTE: we're invoking the ReAct agent with the full history of messages in the state
const callTravelAdvisor = task("callTravelAdvisor", async (messages: BaseMessageLike[]) => { const response = await travelAdvisor.invoke({ messages }); return response.messages;
}); const hotelAdvisorTools = [ getHotelRecommendations, transferToTravelAdvisor,
]; // Define hotel advisor ReAct agent
const hotelAdvisor = createReactAgent({ llm: model, tools: hotelAdvisorTools, stateModifier: [ "You are a hotel expert that can provide hotel recommendations for a given destination.", "If you need help picking travel destinations, ask 'travel_advisor' for help.", ``` ---------------------------------------- TITLE: Define Document Writing Team State (LangGraph/JS)
DESCRIPTION: Defines the structure of the state object used by the document writing team agents and supervisor using LangGraph's `Annotation`. It includes fields for `messages`, `team_members`, `next` (for routing), `current_files` (for the shared workspace context), and `instructions`. Each field uses `Annotation` to specify how values should be reduced or provide default values.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb#_snippet_12 LANGUAGE: JavaScript
CODE:
```
// This defines the agent state for the document writing team
const DocWritingState = Annotation.Root({ messages: Annotation<BaseMessage[]>({ reducer: (x, y) => x.concat(y), }), team_members: Annotation<string[]>({ reducer: (x, y) => x.concat(y), }), next: Annotation<string>({ reducer: (x, y) => y ?? x, default: () => "supervisor", }), current_files: Annotation<string>({ reducer: (x, y) => (y ? `${x}\n${y}` : x), default: () => "No files written.", }), instructions: Annotation<string>({ reducer: (x, y) => y ?? x, default: () => "Solve the human's question.", }),
})
``` ---------------------------------------- TITLE: Defining LangGraph.js Workflow with StateGraph
DESCRIPTION: This snippet defines a LangGraph workflow using `StateGraph`, orchestrating an agent's behavior. It includes `routeMessage` to conditionally route based on whether tool calls are present in the last message, and `callModel` to invoke the bound language model. The graph establishes a flow where the agent calls the model, then conditionally routes to tools or ends, forming a reactive loop.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/stream-tokens.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { StateGraph, END } from "@langchain/langgraph";
import { AIMessage } from "@langchain/core/messages"; const routeMessage = (state: typeof StateAnnotation.State) => { const { messages } = state; const lastMessage = messages[messages.length - 1] as AIMessage; // If no tools are called, we can finish (respond to the user) if (!lastMessage?.tool_calls?.length) { return END; } // Otherwise if there is, we continue and call the tools return "tools";
}; const callModel = async ( state: typeof StateAnnotation.State,
) => { // For versions of @langchain/core < 0.2.3, you must call `.stream()` // and aggregate the message from chunks instead of calling `.invoke()`. const { messages } = state; const responseMessage = await boundModel.invoke(messages); return { messages: [responseMessage] };
}; const workflow = new StateGraph(StateAnnotation) .addNode("agent", callModel) .addNode("tools", toolNode) .addEdge("__start__", "agent") .addConditionalEdges("agent", routeMessage) .addEdge("tools", "agent"); const agent = workflow.compile();
``` ---------------------------------------- TITLE: Define LangGraph State Schema
DESCRIPTION: Defines the structure of the graph's state using `@langchain/langgraph`'s `Annotation`. The state includes a `messages` field which is an array of `BaseMessage` objects, with a reducer to concatenate new messages.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/stream-values.ipynb#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { Annotation } from "@langchain/langgraph";
import { BaseMessage } from "@langchain/core/messages"; const StateAnnotation = Annotation.Root({ messages: Annotation<BaseMessage[]> ({ reducer: (x, y) => x.concat(y), }),
});
``` ---------------------------------------- TITLE: Define LangGraph Workflow and Task (TypeScript)
DESCRIPTION: Defines the `callModel` task which interacts with a language model, searches for user memories, includes them in the system prompt, and optionally stores new memories if the user's message contains "remember". It also defines the main `workflow` entrypoint, configuring it with a `MemorySaver` checkpointer and an in-memory store, and orchestrates the message handling and model call.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/cross-thread-persistence-functional.ipynb#_snippet_5 LANGUAGE: TypeScript
CODE:
```
import { v4 } from "uuid";
import { ChatAnthropic } from "@langchain/anthropic";
import { entrypoint, task, MemorySaver, addMessages, type BaseStore, getStore,
} from "@langchain/langgraph";
import type { BaseMessage, BaseMessageLike } from "@langchain/core/messages"; const model = new ChatAnthropic({ model: "claude-3-5-sonnet-latest",
}); const callModel = task("callModel", async ( messages: BaseMessage[], memoryStore: BaseStore, userId: string
) => { const namespace = ["memories", userId]; const lastMessage = messages.at(-1); if (typeof lastMessage?.content !== "string") { throw new Error("Received non-string message content."); } const memories = await memoryStore.search(namespace, { query: lastMessage.content, }); const info = memories.map((memory) => memory.value.data).join("\n"); const systemMessage = `You are a helpful assistant talking to the user. User info: ${info}`; // Store new memories if the user asks the model to remember if (lastMessage.content.toLowerCase().includes("remember")) { // Hard-coded for demo const memory = `Username is Bob`; await memoryStore.put(namespace, v4(), { data: memory }); } const response = await model.invoke([ { role: "system", content: systemMessage }, ...messages ]); return response;
}); // NOTE: we're passing the store object here when creating a workflow via entrypoint()
const workflow = entrypoint({ checkpointer: new MemorySaver(), store: inMemoryStore, name: "workflow",
}, async (params: { messages: BaseMessageLike[]; userId: string;
}, config) => { const messages = addMessages([], params.messages) const response = await callModel(messages, config.store, params.userId); return entrypoint.final({ value: response, save: addMessages(messages, response), });
});
``` ---------------------------------------- TITLE: Define Web Search and Chart Generation Tools (TypeScript)
DESCRIPTION: Defines two tools: `TavilySearchResults` for web search and a `DynamicStructuredTool` (`generate_bar_chart`) using D3.js and Canvas for generating bar charts. The chart tool requires system dependencies for the `canvas` package.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/multi_agent/agent_supervisor.ipynb#_snippet_3 LANGUAGE: typescript
CODE:
```
import "tsx"; // Only for running this in TSLab. See: https://github.com/yunabe/tslab/issues/72
import { TavilySearchResults } from "@langchain/community/tools/tavily_search";
import { DynamicStructuredTool } from "@langchain/core/tools";
import * as d3 from "d3";
// ----------ATTENTION----------
// If attempting to run this notebook locally, you must follow these instructions
// to install the necessary system dependencies for the `canvas` package.
// https://www.npmjs.com/package/canvas#compiling
// -----------------------------
import { createCanvas } from "canvas";
import { z } from "zod";
import * as tslab from "tslab"; const chartTool = new DynamicStructuredTool({ name: "generate_bar_chart", description: "Generates a bar chart from an array of data points using D3.js and displays it for the user.", schema: z.object({ data: z .object({ label: z.string(), value: z.number(), }) .array(), }), func: async ({ data }) => { const width = 500; const height = 500; const margin = { top: 20, right: 30, bottom: 30, left: 40 }; const canvas = createCanvas(width, height); const ctx = canvas.getContext("2d"); const x = d3 .scaleBand() .domain(data.map((d) => d.label)) .range([margin.left, width - margin.right]) .padding(0.1); const y = d3 .scaleLinear() .domain([0, d3.max(data, (d) => d.value) ?? 0]) .nice() .range([height - margin.bottom, margin.top]); const colorPalette = [ "#e6194B", "#3cb44b", "#ffe119", "#4363d8", "#f58231", "#911eb4", "#42d4f4", "#f032e6", "#bfef45", "#fabebe", ]; data.forEach((d, idx) => { ctx.fillStyle = colorPalette[idx % colorPalette.length]; ctx.fillRect( x(d.label) ?? 0, y(d.value), x.bandwidth(), height - margin.bottom - y(d.value), ); }); ctx.beginPath(); ctx.strokeStyle = "black"; ctx.moveTo(margin.left, height - margin.bottom); ctx.lineTo(width - margin.right, height - margin.bottom); ctx.stroke(); ctx.textAlign = "center"; ctx.textBaseline = "top"; x.domain().forEach((d) => { const xCoord = (x(d) ?? 0) + x.bandwidth() / 2; ctx.fillText(d, xCoord, height - margin.bottom + 6); }); ctx.beginPath(); ctx.moveTo(margin.left, height - margin.top); ctx.lineTo(margin.left, height - margin.bottom); ctx.stroke(); ctx.textAlign = "right"; ctx.textBaseline = "middle"; const ticks = y.ticks(); ticks.forEach((d) => { const yCoord = y(d); // height - margin.bottom - y(d); ctx.moveTo(margin.left, yCoord); ctx.lineTo(margin.left - 6, yCoord); ctx.stroke(); ctx.fillText(d.toString(), margin.left - 8, yCoord); }); await tslab.display.png(canvas.toBuffer()); return "Chart has been generated and displayed to the user!"; },
}); const tavilyTool = new TavilySearchResults();
``` ---------------------------------------- TITLE: Run and Resume LangGraph Workflow with Interrupt (TypeScript)
DESCRIPTION: Shows how to instantiate and stream the LangGraph workflow defined earlier. It demonstrates how the workflow pauses at the `interrupt` and how to resume it by streaming a `Command` object containing the human review result. Includes setup for thread configuration and shows example streamed output.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/functional_api.md#_snippet_1 LANGUAGE: typescript
CODE:
```
import { task, entrypoint, interrupt, MemorySaver, Command } from "@langchain/langgraph"; const writeEssay = task("write_essay", (topic: string): string => { return `An essay about topic: ${topic}`;
}); const workflow = entrypoint( { checkpointer: new MemorySaver(), name: "workflow" }, async (topic: string) => { const essay = await writeEssay(topic); const isApproved = interrupt({ essay, // The essay we want reviewed. action: "Please approve/reject the essay", }); return { essay, isApproved, }; }
); const threadId = crypto.randomUUID(); const config = { configurable: { thread_id: threadId, },
}; for await (const item of await workflow.stream("cat", config)) { console.log(item);
}
``` LANGUAGE: typescript
CODE:
```
// Get review from a user (e.g., via a UI)
// In this case, we're using a bool, but this can be any json-serializable value.
const humanReview = true; for await (const item of await workflow.stream(new Command({ resume: humanReview }), config)) { console.log(item);
}
``` ---------------------------------------- TITLE: Resume LangGraph Execution (JS)
DESCRIPTION: Resumes a previously paused LangGraph execution by calling the `stream` method with a `null` input and the configuration from the last checkpoint. The graph loads the saved state and continues execution from where it left off, streaming the subsequent steps. Requires a graph compiled with checkpointing and a valid checkpoint configuration.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/time-travel.ipynb#_snippet_13 LANGUAGE: JavaScript
CODE:
```
for await ( const { messages } of await graphWithInterrupt.stream(null, { ...snapshot.config, streamMode: "values" })
) { let msg = messages[messages?.length - 1]; if (msg?.content) { console.log(msg.content); } else if (msg?.tool_calls?.length > 0) { console.log(msg.tool_calls); } else { console.log(msg); } console.log("-----\n");
}
``` ---------------------------------------- TITLE: Building LangGraph Agent Workflow (Graph API)
DESCRIPTION: Constructs an agent workflow using LangGraph's `StateGraph`. It defines nodes for LLM calls and tool execution, and uses conditional edges to route the flow based on whether the LLM decides to call a tool or provide a final response.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_26 LANGUAGE: TypeScript
CODE:
```
import { MessagesAnnotation, StateGraph } from "@langchain/langgraph";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import { SystemMessage, ToolMessage
} from "@langchain/core/messages"; // Nodes
async function llmCall(state: typeof MessagesAnnotation.State) { // LLM decides whether to call a tool or not const result = await llmWithTools.invoke([ { role: "system", content: "You are a helpful assistant tasked with performing arithmetic on a set of inputs." }, ...state.messages ]); return { messages: [result] };
} const toolNode = new ToolNode(tools); // Conditional edge function to route to the tool node or end
function shouldContinue(state: typeof MessagesAnnotation.State) { const messages = state.messages; const lastMessage = messages.at(-1); // If the LLM makes a tool call, then perform an action if (lastMessage?.tool_calls?.length) { return "Action"; } // Otherwise, we stop (reply to the user) return "__end__";
} // Build workflow
const agentBuilder = new StateGraph(MessagesAnnotation) .addNode("llmCall", llmCall) .addNode("tools", toolNode) // Add edges to connect nodes .addEdge("__start__", "llmCall") .addConditionalEdges( "llmCall", shouldContinue, { // Name returned by shouldContinue : Name of next node to visit "Action": "tools", "__end__": "__end__", } ) .addEdge("tools", "llmCall") .compile(); // Invoke
const messages = [{ role: "user", content: "Add 3 and 4."
}];
const result = await agentBuilder.invoke({ messages });
console.log(result.messages);
``` ---------------------------------------- TITLE: Implementing Human Review with LangGraph Interrupt (TypeScript)
DESCRIPTION: Defines the `reviewToolCall` function which uses `interrupt` to pause the agent's execution for human review of a tool call. It processes the human's decision ('continue', 'update', 'feedback') and returns either the original/updated tool call or a `ToolMessage` based on the action.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/review-tool-calls-functional.ipynb#_snippet_5 LANGUAGE: TypeScript
CODE:
```
import { interrupt } from "@langchain/langgraph"; function reviewToolCall(toolCall: ToolCall): ToolCall | ToolMessage { // Interrupt for human review const humanReview = interrupt({ question: "Is this correct?", tool_call: toolCall, }); const { action, data } = humanReview; if (action === "continue") { return toolCall; } else if (action === "update") { return { ...toolCall, args: data, }; } else if (action === "feedback") { return new ToolMessage({ content: data, name: toolCall.name, tool_call_id: toolCall.id, }); } throw new Error(`Unsupported review action: ${action}`);
}
``` ---------------------------------------- TITLE: Defining LangGraph State with Default Reducers (TypeScript)
DESCRIPTION: This snippet demonstrates defining a LangGraph state schema using `Annotation.Root` without explicit reducer functions. When no reducer is specified for a key, updates to that key will override its existing value. It initializes a `StateGraph` with this schema, showing how basic state fields like `foo` (number) and `bar` (string array) are declared.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/low_level.md#_snippet_2 LANGUAGE: typescript
CODE:
```
import { StateGraph, Annotation } from "@langchain/langgraph"; const State = Annotation.Root({ foo: Annotation<number>, bar: Annotation<string[]>,
}); const graphBuilder = new StateGraph(State);
``` ---------------------------------------- TITLE: Defining LangGraph Agent State with Zod in TypeScript
DESCRIPTION: This snippet illustrates how to define an `AgentState` schema using Zod in TypeScript for a LangGraph application. It configures the `messages` field as an array of strings with a custom reducer for concatenation and a default empty array. Additionally, it defines `question` as a string and `answer` as a string with a minimum length validation, ensuring structured and validated state management.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/define-state.ipynb#_snippet_9 LANGUAGE: TypeScript
CODE:
```
import "@langchain/langgraph/zod";
import { z } from "zod"; const AgentState = z.object({ messages: z .array(z.string()) .default(() => []) .langgraph.reducer( (a, b) => a.concat(Array.isArray(b) ? b : [b]), z.union([z.string(), z.array(z.string())]) ), question: z.string(), answer: z.string().min(1),
}); const graph = new StateGraph(AgentState);
``` ---------------------------------------- TITLE: Defining State and Building a RAG Graph with Private State (JavaScript/TypeScript)
DESCRIPTION: This snippet defines the state annotations for a LangGraphJS graph, including the overall state and intermediate states for query and documents. It then defines three asynchronous nodes (`generateQuery`, `retrieveDocuments`, `generate`) representing steps in a RAG pipeline. Finally, it constructs a `StateGraph`, adds the nodes, specifies input annotations for intermediate state, defines edges, compiles the graph, and invokes it with an initial question. The example demonstrates how intermediate state (`query`, `docs`) is managed via node inputs rather than the main graph state.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/pass_private_state.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
import { Annotation, StateGraph } from "@langchain/langgraph"; // The overall state of the graph
const OverallStateAnnotation = Annotation.Root({ question: Annotation<string>, answer: Annotation<string>,
}); // This is what the node that generates the query will return
const QueryOutputAnnotation = Annotation.Root({ query: Annotation<string>,
}); // This is what the node that retrieves the documents will return
const DocumentOutputAnnotation = Annotation.Root({ docs: Annotation<string[]>,
}); // This is what the node that retrieves the documents will return
const GenerateOutputAnnotation = Annotation.Root({ ...OverallStateAnnotation.spec, ...DocumentOutputAnnotation.spec
}); // Node to generate query
const generateQuery = async (state: typeof OverallStateAnnotation.State) => { // Replace this with real logic return { query: state.question + " rephrased as a query!", };
}; // Node to retrieve documents
const retrieveDocuments = async (state: typeof QueryOutputAnnotation.State) => { // Replace this with real logic return { docs: [state.query, "some random document"], };
}; // Node to generate answer
const generate = async (state: typeof GenerateOutputAnnotation.State) => { return { answer: state.docs.concat([state.question]).join("\n\n"), };
}; const graph = new StateGraph(OverallStateAnnotation) .addNode("generate_query", generateQuery) .addNode("retrieve_documents", retrieveDocuments, { input: QueryOutputAnnotation }) .addNode("generate", generate, { input: GenerateOutputAnnotation }) .addEdge("__start__", "generate_query") .addEdge("generate_query", "retrieve_documents") .addEdge("retrieve_documents", "generate") .compile(); await graph.invoke({ question: "How are you?",
});
``` ---------------------------------------- TITLE: Defining and Initializing Agents and Nodes (JS/TS)
DESCRIPTION: This snippet defines two agents, a researcher and a chart generator, using `createReactAgent` from `@langchain/langgraph/prebuilt`. It configures them with an LLM, tools, and system messages. It also defines asynchronous node functions (`researcherNode`, `chartGenNode`) that wrap the agent invocation for use within a LangGraph StateGraph, extracting the last message from the agent's result.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/multi_agent/agent_supervisor.ipynb#_snippet_6 LANGUAGE: javascript
CODE:
```
import { RunnableConfig } from "@langchain/core/runnables";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { SystemMessage } from "@langchain/core/messages"; // Recall llm was defined as ChatOpenAI above
// It could be any other language model
const researcherAgent = createReactAgent({ llm, tools: [tavilyTool], stateModifier: new SystemMessage("You are a web researcher. You may use the Tavily search engine to search the web for" + " important information, so the Chart Generator in your team can make useful plots.")
}) const researcherNode = async ( state: typeof AgentState.State, config?: RunnableConfig,
) => { const result = await researcherAgent.invoke(state, config); const lastMessage = result.messages[result.messages.length - 1]; return { messages: [ new HumanMessage({ content: lastMessage.content, name: "Researcher" }), ], };
}; const chartGenAgent = createReactAgent({ llm, tools: [chartTool], stateModifier: new SystemMessage("You excel at generating bar charts. Use the researcher's information to generate the charts.")
}) const chartGenNode = async ( state: typeof AgentState.State, config?: RunnableConfig,
) => { const result = await chartGenAgent.invoke(state, config); const lastMessage = result.messages[result.messages.length - 1]; return { messages: [ new HumanMessage({ content: lastMessage.content, name: "ChartGenerator" }), ], };
};
``` ---------------------------------------- TITLE: Implementing Parallel LLM Calls using LangGraph Graph API (TypeScript)
DESCRIPTION: This snippet defines a LangGraph StateGraph with nodes for three parallel LLM calls and an aggregator node. It configures edges so the three LLM calls run in parallel from the start and all transition to the aggregator node before ending the workflow.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { StateGraph, Annotation } from "@langchain/langgraph"; // Graph state
const StateAnnotation = Annotation.Root({ topic: Annotation<string>, joke: Annotation<string>, story: Annotation<string>, poem: Annotation<string>, combinedOutput: Annotation<string>,
}); // Nodes
// First LLM call to generate initial joke
async function callLlm1(state: typeof StateAnnotation.State) { const msg = await llm.invoke(`Write a joke about ${state.topic}`); return { joke: msg.content };
} // Second LLM call to generate story
async function callLlm2(state: typeof StateAnnotation.State) { const msg = await llm.invoke(`Write a story about ${state.topic}`); return { story: msg.content };
} // Third LLM call to generate poem
async function callLlm3(state: typeof StateAnnotation.State) { const msg = await llm.invoke(`Write a poem about ${state.topic}`); return { poem: msg.content };
} // Combine the joke, story and poem into a single output
async function aggregator(state: typeof StateAnnotation.State) { const combined = `Here's a story, joke, and poem about ${state.topic}!\n\n` + `STORY:\n${state.story}\n\n` + `JOKE:\n${state.joke}\n\n` + `POEM:\n${state.poem}`; return { combinedOutput: combined };
} // Build workflow
const parallelWorkflow = new StateGraph(StateAnnotation) .addNode("callLlm1", callLlm1) .addNode("callLlm2", callLlm2) .addNode("callLlm3", callLlm3) .addNode("aggregator", aggregator) .addEdge("__start__", "callLlm1") .addEdge("__start__", "callLlm2") .addEdge("__start__", "callLlm3") .addEdge("callLlm1", "aggregator") .addEdge("callLlm2", "aggregator") .addEdge("callLlm3", "aggregator") .addEdge("aggregator", "__end__") .compile(); // Invoke
const result = await parallelWorkflow.invoke({ topic: "cats" });
console.log(result.combinedOutput); ``` ---------------------------------------- TITLE: Defining and Adding Custom Nodes to StateGraph (TypeScript)
DESCRIPTION: Shows how to define custom nodes as TypeScript functions that accept graph state and an optional `RunnableConfig`. It also demonstrates adding these nodes to a `StateGraph` using the `addNode` method.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/low_level.md#_snippet_10 LANGUAGE: TypeScript
CODE:
```
import { RunnableConfig } from "@langchain/core/runnables";
import { StateGraph, Annotation } from "@langchain/langgraph"; const GraphAnnotation = Annotation.Root({ input: Annotation<string>, results: Annotation<string>,
}); // The state type can be extracted using `typeof <annotation variable name>.State`
const myNode = (state: typeof GraphAnnotation.State, config?: RunnableConfig) => { console.log("In node: ", config.configurable?.user_id); return { results: `Hello, ${state.input}!` };
}; // The second argument is optional
const myOtherNode = (state: typeof GraphAnnotation.State) => { return state;
}; const builder = new StateGraph(GraphAnnotation) .addNode("myNode", myNode) .addNode("myOtherNode", myOtherNode) ...
``` ---------------------------------------- TITLE: Defining and Invoking a Simple LangGraph with Persistence (TypeScript)
DESCRIPTION: Shows how to define a simple StateGraph with annotated state, add nodes and edges, compile it with a MemorySaver checkpointer, and invoke it with a specific thread_id for persistence. Requires @langchain/langgraph.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/persistence.md#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { StateGraph, START, END, MemorySaver, Annotation } from "@langchain/langgraph"; const GraphAnnotation = Annotation.Root({ foo: Annotation<string>, bar: Annotation<string[]>({ reducer: (a, b) => [...a, ...b], default: () => [], })
}); function nodeA(state: typeof GraphAnnotation.State) { return { foo: "a", bar: ["a"] };
} function nodeB(state: typeof GraphAnnotation.State) { return { foo: "b", bar: ["b"] };
} const workflow = new StateGraph(GraphAnnotation) .addNode("nodeA", nodeA) .addNode("nodeB", nodeB) .addEdge(START, "nodeA") .addEdge("nodeA", "nodeB") .addEdge("nodeB", END); const checkpointer = new MemorySaver();
const graph = workflow.compile({ checkpointer }); const config = { configurable: { thread_id: "1" } };
await graph.invoke({ foo: "" }, config);
``` ---------------------------------------- TITLE: Creating and Streaming LangGraph Multi-Agent Graph (JS)
DESCRIPTION: Initializes a LangGraph StateGraph with 'flight_assistant' and 'hotel_assistant' nodes, defines transitions from START and between nodes, compiles the graph, and streams a user message through it, logging the output chunks.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/multi-agent.md#_snippet_8 LANGUAGE: javascript
CODE:
```
const multiAgentGraph = new StateGraph(MessagesAnnotation) .addNode("flight_assistant", flightAssistant, { ends: ["hotel_assistant", END] }) .addNode("hotel_assistant", hotelAssistant, { ends: ["flight_assistant", END] }) .addEdge(START, "flight_assistant") .compile(); const stream = await multiAgentGraph.stream({ messages: [{ role: "user", content: "book a flight from BOS to JFK and a stay at McKittrick Hotel" }]
}); for await (const chunk of stream) { console.log(chunk); console.log("\n");
}
``` ---------------------------------------- TITLE: Running LangSmith Evaluation with an Agent and Evaluator (TypeScript)
DESCRIPTION: Demonstrates how to use the `evaluate` function from `langsmith/evaluation` to run an agent against a dataset using a specified evaluator. It shows invoking the agent within the evaluation function and configuring the evaluation run with the dataset name and evaluator list.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/evals.md#_snippet_4 LANGUAGE: ts
CODE:
```
import { evaluate } from "langsmith/evaluation";
import { createTrajectoryMatchEvaluator } from "agentevals";
import { createReactAgent } from "@langchain/langgraph/prebuilt"; const agent = createReactAgent({ ... })
const evaluator = createTrajectoryMatchEvaluator({ ... })
await evaluate( async (inputs) => await agent.invoke(inputs), { // replace with your dataset name data: "<Name of your dataset>", evaluators: [evaluator], }
);
``` ---------------------------------------- TITLE: Use PostgresSaver with Connection Pool and React Agent (TypeScript)
DESCRIPTION: This snippet demonstrates creating a PostgresSaver instance using a node-postgres connection pool, setting it up, and then using it with the pre-built createReactAgent. It also shows how to invoke the agent with a specific thread ID for persistence.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/persistence-postgres.ipynb#_snippet_3 LANGUAGE: ts
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { PostgresSaver } from "@langchain/langgraph-checkpoint-postgres";
import { createReactAgent } from "@langchain/langgraph/prebuilt"; import pg from "pg"; const { Pool } = pg; const pool = new Pool({ connectionString: "postgresql://user:password@localhost:5434/testdb"
}); const checkpointer = new PostgresSaver(pool); // NOTE: you need to call .setup() the first time you're using your checkpointer await checkpointer.setup(); const graph = createReactAgent({ tools: [getWeather], llm: new ChatOpenAI({ model: "gpt-4o-mini", }), checkpointSaver: checkpointer,
});
const config = { configurable: { thread_id: "1" } }; await graph.invoke({ messages: [{ role: "user", content: "what's the weather in sf" }],
}, config);
``` ---------------------------------------- TITLE: Implementing Custom Agent Loop and Invocation with LangGraph
DESCRIPTION: This snippet defines an asynchronous function representing a custom agent loop that processes messages, calls an LLM, executes tools based on the LLM's response, and updates the message history. It then demonstrates how to invoke this custom agent with an initial message and stream its output.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_28 LANGUAGE: TypeScript
CODE:
``` async (messages: BaseMessageLike[]) => { let llmResponse = await callLlm(messages); while (true) { if (!llmResponse.tool_calls?.length) { break; } // Execute tools const toolResults = await Promise.all( llmResponse.tool_calls.map((toolCall) => callTool(toolCall)) ); messages = addMessages(messages, [llmResponse, ...toolResults]); llmResponse = await callLlm(messages); } messages = addMessages(messages, [llmResponse]); return messages; } ); // Invoke const messages = [{ role: "user", content: "Add 3 and 4." }]; const stream = await agent.stream([messages], { streamMode: "updates", }); for await (const step of stream) { console.log(step); } ``` ---------------------------------------- TITLE: Define and Bind Response Tool
DESCRIPTION: Defines a special tool (finalResponseTool) with a zod schema (Response) specifying the desired structured output format (temperature and other notes). This tool is mocked and used only to guide the model's output format. It is then bound along with other tools to the ChatOpenAI model instance.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/respond-in-format.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { tool } from "@langchain/core/tools"; const Response = z.object({ temperature: z.number().describe("the temperature"), other_notes: z.string().describe("any other notes about the weather"),
}); const finalResponseTool = tool(async () => "mocked value", { name: "Response", description: "Always respond to the user using this tool.", schema: Response
}) const boundModel = model.bindTools([ ...tools, finalResponseTool
]);
``` ---------------------------------------- TITLE: Accessing Agent State and Config in LangGraph JS Tools
DESCRIPTION: Shows how to define a tool function that accesses runtime-specific data from the agent's `config` and `state` (via `getCurrentTaskInput`). This is useful for passing information not controlled by the LLM, such as user context.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/tools.md#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { z } from "zod";
import { tool } from "@langchain/core/tools";
// highlight-next-line
import { // highlight-next-line getCurrentTaskInput, // highlight-next-line LangGraphRunnableConfig,
} from "@langchain/langgraph";
import { MessagesAnnotation } from "@langchain/langgraph"; const myTool = tool( async (input: { // This will be populated by an LLM toolArg: string, }, // access static data that is passed at agent invocation // highlight-next-line config: LangGraphRunnableConfig
) => { // Fetch the current agent state // highlight-next-line const state = getCurrentTaskInput() as typeof MessagesAnnotation.State; doSomethingWithState(state.messages); doSomethingWithConfig(config); // ... }, { name: "myTool", schema: z.object({ myToolArg: z.number().describe("Tool arg"), }), description: "My tool.", }
);
``` ---------------------------------------- TITLE: Interacting with Agent (Thread 1, First Message)
DESCRIPTION: Demonstrates the first interaction with the initialized agent using `agent.stream`. It sends a message asking about the weather in NYC with `thread_id: "1"` and iterates through the streamed output, logging the content or tool calls.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/react-memory.ipynb#_snippet_3 LANGUAGE: javascript
CODE:
```
let inputs = { messages: [{ role: "user", content: "what is the weather in NYC?" }] };
let config = { configurable: { thread_id: "1" } };
let stream = await agent.stream(inputs, { ...config, streamMode: "values",
}); for await ( const { messages } of stream
) { let msg = messages[messages?.length - 1]; if (msg?.content) { console.log(msg.content); } else if (msg?.tool_calls?.length > 0) { console.log(msg.tool_calls); } else { console.log(msg); } console.log("-----\n");
}
``` ---------------------------------------- TITLE: Edit Tool Call and Update Agent State (JavaScript)
DESCRIPTION: Retrieves the current agent state, accesses the last message containing tool calls, modifies the arguments of the first tool call, and then updates the agent's state with the modified message using `updateState`. This prepares the graph for a corrected execution.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/react-human-in-the-loop.ipynb#_snippet_6 LANGUAGE: javascript
CODE:
```
// First, lets get the current state
const currentState = await agent.getState(config); // Let's now get the last message in the state
// This is the one with the tool calls that we want to update
let lastMessage = currentState.values.messages[currentState.values.messages.length - 1] // Let's now update the args for that tool call
lastMessage.tool_calls[0].args = { location: "San Francisco" } // Let's now call `updateState` to pass in this message in the `messages` key
// This will get treated as any other update to the state
// It will get passed to the reducer function for the `messages` key
// That reducer function will use the ID of the message to update it
// It's important that it has the right ID! Otherwise it would get appended
// as a new message
await agent.updateState(config, { messages: lastMessage });
``` ---------------------------------------- TITLE: Define LLM, Conditional Logic, and Model Node
DESCRIPTION: Defines the Anthropic LLM and binds the tools to it. It also defines shouldContinue, a function for conditional routing based on the last message, and callModel, a node function to invoke the LLM.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/agent_executor/base.ipynb#_snippet_5 LANGUAGE: TypeScript
CODE:
```
import type { RunnableConfig } from "@langchain/core/runnables";
import { ChatAnthropic } from "@langchain/anthropic";
import { END } from "@langchain/langgraph"; // Define the LLM to be used in the agent
const llm = new ChatAnthropic({ model: "claude-3-5-sonnet-20240620", temperature: 0,
}).bindTools(tools); // Ensure you bind the same tools passed to the ToolExecutor to the LLM, so these tools can be used in the agent // Define logic that will be used to determine which conditional edge to go down
const shouldContinue = (data: typeof AgentState.State): "executeTools" | typeof END => { const { messages } = data; const lastMsg = messages[messages.length - 1]; // If the agent called a tool, we should continue. If not, we can end. if (!("tool_calls" in lastMsg) || !Array.isArray(lastMsg.tool_calls) || !lastMsg?.tool_calls?.length) { return END; } // By returning the name of the next node we want to go to // LangGraph will automatically route to that node return "executeTools";
}; const callModel = async (data: typeof AgentState.State, config?: RunnableConfig): Promise<Partial<typeof AgentState.State>> => { const { messages } = data; const result = await llm.invoke(messages, config); return { messages: [result], };
};
``` ---------------------------------------- TITLE: Using Pre-built React Agent with LangGraph
DESCRIPTION: This snippet demonstrates how to use the `createReactAgent` function from LangGraph's pre-built module to quickly set up an agent. It shows how to pass an LLM instance and a list of tools to the function and then invoke the created agent with an initial message.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_29 LANGUAGE: TypeScript
CODE:
```
import { createReactAgent } from "@langchain/langgraph/prebuilt"; // Pass in:
// (1) an LLM instance
// (2) the tools list (which is used to create the tool node)
const prebuiltAgent = createReactAgent({ llm: llmWithTools, tools,
}); // invoke
const result = await prebuiltAgent.invoke({ messages: [ { role: "user", content: "Add 3 and 4.", }, ],
});
console.log(result.messages); ``` ---------------------------------------- TITLE: Full Example of LangGraph Interrupt with Streaming (TypeScript)
DESCRIPTION: A complete example demonstrating the use of `interrupt` in a LangGraph workflow. Includes defining state annotations, the `humanNode`, building the graph with `StateGraph`, compiling with a checkpointer, and using `stream` to handle the interrupt and resume with a `Command`.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/human_in_the_loop.md#_snippet_2 LANGUAGE: typescript
CODE:
```
import { MemorySaver, Annotation, interrupt, Command, StateGraph } from "@langchain/langgraph"; // Define the graph state
const StateAnnotation = Annotation.Root({ some_text: Annotation<string>()
}); function humanNode(state: typeof StateAnnotation.State) { const value = interrupt( // Any JSON serializable value to surface to the human. // For example, a question or a piece of text or a set of keys in the state { text_to_revise: state.some_text } ); return { // Update the state with the human's input some_text: value };
} // Build the graph
const workflow = new StateGraph(StateAnnotation)
// Add the human-node to the graph .addNode("human_node", humanNode) .addEdge("__start__", "human_node") // A checkpointer is required for `interrupt` to work.
const checkpointer = new MemorySaver();
const graph = workflow.compile({ checkpointer
}); // Using stream() to directly surface the `__interrupt__` information.
for await (const chunk of await graph.stream( { some_text: "Original text" }, threadConfig
)) { console.log(chunk);
} // Resume using Command
for await (const chunk of await graph.stream( new Command({ resume: "Edited text" }), threadConfig
)) { console.log(chunk);
}
``` ---------------------------------------- TITLE: Resuming a Paused LangGraph Agent with a Command (TypeScript)
DESCRIPTION: This snippet shows how to resume a LangGraph agent that has been paused by an `interrupt()` call. It uses the `stream()` method again, but this time passes a `Command` object with a `resume` payload. This payload contains the human's decision (e.g., `type: "accept"`) which allows the agent to continue its execution based on the provided input.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/human-in-the-loop.md#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { Command } from "@langchain/langgraph"; for await (const chunk of await agent.stream( new Command({ resume: { type: "accept" } }), // (1)! // new Command({ resume: { type: "edit", args: { "hotel_name": "McKittrick Hotel" } } }), // highlight-next-line config
)) { console.log(chunk); console.log("\n");
};
``` ---------------------------------------- TITLE: Call OpenAI Model with Streaming and Tool Handling (TypeScript)
DESCRIPTION: Defines an asynchronous function `callModel` intended for a LangGraph node. It calls the OpenAI chat completions API with streaming, processes the streamed chunks to extract content and tool call information, dispatches custom events for streaming updates, and formats the final response message.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/streaming-tokens-without-langchain.ipynb#_snippet_2 LANGUAGE: typescript
CODE:
```
import { dispatchCustomEvent } from "@langchain/core/callbacks/dispatch";
import { wrapOpenAI } from "langsmith/wrappers/openai";
import { Annotation } from "@langchain/langgraph"; const StateAnnotation = Annotation.Root({ messages: Annotation<OpenAI.ChatCompletionMessageParam[]>({ reducer: (x, y) => x.concat(y), }),
}); // If using LangSmith, use "wrapOpenAI" on the whole client or
// "traceable" to wrap a single method for nicer tracing:
// https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code
const wrappedClient = wrapOpenAI(openaiClient); const callModel = async (state: typeof StateAnnotation.State) => { const { messages } = state; const stream = await wrappedClient.chat.completions.create({ messages, model: "gpt-4o-mini", tools: [toolSchema], stream: true, }); let responseContent = ""; let role: string = "assistant"; let toolCallId: string | undefined; let toolCallName: string | undefined; let toolCallArgs = ""; for await (const chunk of stream) { const delta = chunk.choices[0].delta; if (delta.role !== undefined) { role = delta.role; } if (delta.content) { responseContent += delta.content; await dispatchCustomEvent("streamed_token", { content: delta.content, }); } if (delta.tool_calls !== undefined && delta.tool_calls.length > 0) { // note: for simplicity we're only handling a single tool call here const toolCall = delta.tool_calls[0]; if (toolCall.function?.name !== undefined) { toolCallName = toolCall.function.name; } if (toolCall.id !== undefined) { toolCallId = toolCall.id; } await dispatchCustomEvent("streamed_tool_call_chunk", toolCall); toolCallArgs += toolCall.function?.arguments ?? ""; } } let finalToolCalls; if (toolCallName !== undefined && toolCallId !== undefined) { finalToolCalls = [{ id: toolCallId, function: { name: toolCallName, arguments: toolCallArgs }, type: "function" as const, }]; } const responseMessage = { role: role as any, content: responseContent, tool_calls: finalToolCalls, }; return { messages: [responseMessage] };
}
``` ---------------------------------------- TITLE: Initializing a LangGraph StateGraph for Web Environments
DESCRIPTION: Demonstrates how to import LangGraph components from the `@langchain/langgraph/web` entrypoint and define a simple `StateGraph` with a single node and basic state management, suitable for environments without `async_hooks`. It shows how to define the graph state, a node function, compile the workflow, and invoke it.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/use-in-web-environments.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
// Import from "@langchain/langgraph/web"
import { END, START, StateGraph, Annotation,
} from "@langchain/langgraph/web";
import { BaseMessage, HumanMessage } from "@langchain/core/messages"; const GraphState = Annotation.Root({ messages: Annotation<BaseMessage[]>({ reducer: (x, y) => x.concat(y), }),
}); const nodeFn = async (_state: typeof GraphState.State) => { return { messages: [new HumanMessage("Hello from the browser!")] };
}; // Define a new graph
const workflow = new StateGraph(GraphState) .addNode("node", nodeFn) .addEdge(START, "node") .addEdge("node", END); const app = workflow.compile({}); // Use the Runnable
const finalState = await app.invoke( { messages: [] },
); console.log(finalState.messages[finalState.messages.length - 1].content);
``` ---------------------------------------- TITLE: Defining Hotel Advisor ReAct Agent (TypeScript)
DESCRIPTION: Defines the `hotelAdvisor` agent using `createReactAgent`. It is configured with an LLM (`model`), the defined `hotelAdvisorTools`, and a `stateModifier` string providing instructions and context for the agent, including its role as a hotel expert and how to request help from the travel advisor.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/multi-agent-multi-turn-convo-functional.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
const hotelAdvisor = createReactAgent({ llm: model, tools: hotelAdvisorTools, stateModifier: [ "You are a hotel expert that can provide hotel recommendations for a given destination.", "If you need help picking travel destinations, ask 'travel_advisor' for help.", "You MUST include a human-readable response before transferring to another agent." ].join(" ")
});
``` ---------------------------------------- TITLE: Streaming LangGraph Agent Output (TypeScript)
DESCRIPTION: Illustrates how to stream the output of a LangGraph agent using the `.stream()` method. It iterates over the asynchronous generator returned by `stream`, logging each chunk. The example uses the `streamMode: "updates"` option and a message input.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/run_agents.md#_snippet_1 LANGUAGE: TypeScript
CODE:
```
for await ( const chunk of await agent.stream( { messages: [ { role: "user", content: "what is the weather in sf" } ] }, { streamMode: "updates" }, )
) { console.log(chunk);
}
``` ---------------------------------------- TITLE: Define Search Tool
DESCRIPTION: Creates a placeholder search tool using @langchain/core/tools. It defines the tool's name, description, and schema using zod, specifying that it accepts a query string. The tool's function is mocked to return a fixed weather string.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/respond-in-format.ipynb#_snippet_4 LANGUAGE: TypeScript
CODE:
```
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const searchTool = tool((_) => { // This is a placeholder, but don't tell the LLM that... return "67 degrees. Cloudy with a chance of rain.";
}, { name: "search", description: "Call to surf the web.", schema: z.object({ query: z.string().describe("The query to use in your search."), }),
}); const tools = [searchTool];
``` ---------------------------------------- TITLE: Run Agent with Persistence Config (First Message)
DESCRIPTION: Demonstrates how to invoke the agent with the thread configuration for the initial message in a conversation. It shows how to stream the response and process the steps, utilizing the configured checkpointer.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/react-agent-from-scratch-functional.ipynb#_snippet_8 LANGUAGE: JavaScript
CODE:
```
const streamWithMemory = await agentWithMemory.stream([{ role: "user", content: "What's the weather in san francisco?"
}], config); for await (const step of streamWithMemory) { for (const [taskName, update] of Object.entries(step)) { const message = update as BaseMessage; // Only print task updates if (taskName === "agentWithMemory") continue; console.log(`\n${taskName}:`); prettyPrintMessage(message); }
}
``` ---------------------------------------- TITLE: Creating and Running a Swarm Workflow
DESCRIPTION: Demonstrates how to create a multi-agent swarm using LangGraph Swarm, define agents with specialized tools (including handoff tools), compile the workflow with memory, and invoke it for multi-turn interactions.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/libs/langgraph-swarm/README.md#_snippet_2 LANGUAGE: ts
CODE:
```
import { z } from "zod";
import { ChatOpenAI } from "@langchain/openai";
import { tool } from "@langchain/core/tools";
import { MemorySaver } from "@langchain/langgraph";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { createSwarm, createHandoffTool } from "@langchain/langgraph-swarm"; const model = new ChatOpenAI({ modelName: "gpt-4o" }); // Create specialized tools
const add = tool( async (args) => args.a + args.b, { name: "add", description: "Add two numbers.", schema: z.object({ a: z.number(), b: z.number() }) }
); // Create agents with handoff tools
const alice = createReactAgent({ llm: model, tools: [add, createHandoffTool({ agentName: "Bob" })], name: "Alice", prompt: "You are Alice, an addition expert."
}); const bob = createReactAgent({ llm: model, tools: [createHandoffTool({ agentName: "Alice", description: "Transfer to Alice, she can help with math" })], name: "Bob", prompt: "You are Bob, you speak like a pirate."
}); // Create swarm workflow
const checkpointer = new MemorySaver();
const workflow = createSwarm({ agents: [alice, bob], defaultActiveAgent: "Alice"
}); export const app = workflow.compile({ checkpointer }); const config = { configurable: { thread_id: "1" } };
const turn1 = await app.invoke( { messages: [{ role: "user", content: "i'd like to speak to Bob" }] }, config
);
console.log(turn1); const turn2 = await app.invoke( { messages: [{ role: "user", content: "what's 5 + 7?" }] }, config
);
console.log(turn2);
``` ---------------------------------------- TITLE: Declaring LangGraph Entrypoint with Injectable Parameters (TypeScript)
DESCRIPTION: Demonstrates how to declare a LangGraph entrypoint using the `entrypoint` function, specifying a store for long-term memory. It shows how to access injectable parameters like the previous state using `getPreviousState` within the entrypoint's async function.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/functional_api.md#_snippet_3 LANGUAGE: typescript
CODE:
```
import { entrypoint, getPreviousState, BaseStore, InMemoryStore,
} from "@langchain/langgraph";
import { RunnableConfig } from "@langchain/core/runnables"; const inMemoryStore = new InMemoryStore(...); // An instance of InMemoryStore for long-term memory const myWorkflow = entrypoint( { checkpointer, // Specify the checkpointer store: inMemoryStore, // Specify the store name: "myWorkflow", }, async (someInput: Record<string, any>) => { const previous = getPreviousState<any>(); // For short-term memory // Rest of workflow logic... }
);
``` ---------------------------------------- TITLE: Resuming LangGraph Workflow Execution from Checkpoint (TypeScript)
DESCRIPTION: Shows how to resume a previously failed or interrupted LangGraph workflow execution using the same configuration and checkpointer. The workflow will continue from the last saved state, skipping tasks that have already completed successfully.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/functional_api.md#_snippet_26 LANGUAGE: typescript
CODE:
```
await main.invoke(null, config);
``` ---------------------------------------- TITLE: Defining a Node with Dynamic Breakpoint (TypeScript)
DESCRIPTION: This TypeScript function `myNode` demonstrates how to implement a dynamic breakpoint within a LangGraph node. It checks if the `input` property in the state exceeds 5 characters. If the condition is met, it throws a `NodeInterrupt` error, halting the graph execution at this node. Otherwise, it returns the state, allowing execution to continue.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/v0-human-in-the-loop.md#_snippet_1 LANGUAGE: typescript
CODE:
```
function myNode(state: typeof GraphAnnotation.State): typeof GraphAnnotation.State { if (state.input.length > 5) { throw new NodeInterrupt(`Received input that is longer than 5 characters: ${state['input']}`); } return state;
}
``` ---------------------------------------- TITLE: Initialize Retriever - LangChain/JS
DESCRIPTION: Sets up a retriever by loading documents from specified URLs using Cheerio, splitting them into chunks, embedding them with OpenAI, and storing them in a memory vector store.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rag/langgraph_agentic_rag.ipynb#_snippet_2 LANGUAGE: JavaScript
CODE:
```
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "@langchain/openai"; const urls = [ "https://lilianweng.github.io/posts/2023-06-23-agent/", "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/", "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/"
]; const docs = await Promise.all( urls.map((url) => new CheerioWebBaseLoader(url).load()),
);
const docsList = docs.flat(); const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 500, chunkOverlap: 50,
});
const docSplits = await textSplitter.splitDocuments(docsList); // Add to vectorDB
const vectorStore = await MemoryVectorStore.fromDocuments( docSplits, new OpenAIEmbeddings(),
); const retriever = vectorStore.asRetriever();
``` ---------------------------------------- TITLE: Setting Static Breakpoints at Compile Time (TypeScript)
DESCRIPTION: Demonstrates how to configure static breakpoints (`interruptBefore`, `interruptAfter`) when compiling a LangGraph builder. It shows how to include a checkpointer, define a thread configuration, invoke the graph to run until the breakpoint, optionally update the graph state, and then resume execution by invoking again.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/breakpoints.md#_snippet_0 LANGUAGE: typescript
CODE:
```
const graph = graphBuilder.compile({ interruptBefore: ["nodeA"], interruptAfter: ["nodeB", "nodeC"], checkpointer: ..., // Specify a checkpointer
}); const threadConfig = { configurable: { thread_id: "someThread" }
}; // Run the graph until the breakpoint
await graph.invoke(inputs, threadConfig); // Optionally update the graph state based on user input
await graph.updateState(update, threadConfig); // Resume the graph
await graph.invoke(null, threadConfig);
``` ---------------------------------------- TITLE: Compose Tasks into LangGraph Entrypoint (TypeScript)
DESCRIPTION: Composes the `step1`, `humanFeedback`, and `step3` tasks sequentially within a LangGraph `entrypoint`. A `MemorySaver` checkpointer is configured to persist state across interrupts.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/wait-user-input-functional.ipynb#_snippet_3 LANGUAGE: typescript
CODE:
```
import { MemorySaver, entrypoint } from "@langchain/langgraph"; const checkpointer = new MemorySaver(); const graph = entrypoint({ name: "graph", checkpointer,
}, async (inputQuery: string) => { const result1 = await step1(inputQuery); const result2 = await humanFeedback(result1); const result3 = await step3(result2); return result3;
});
``` ---------------------------------------- TITLE: Streaming LangGraph Updates (TypeScript)
DESCRIPTION: Demonstrates how to run the compiled graph with `streamMode="updates"`. It provides initial inputs and then iterates asynchronously over the streamed chunks. For each chunk, it logs the node name and the state values received from that node.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/stream-updates.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
let inputs = { messages: [{ role: "user", content: "what's the weather in sf" }] }; for await ( const chunk of await graph.stream(inputs, { streamMode: "updates", })
) { for (const [node, values] of Object.entries(chunk)) { console.log(`Receiving update from node: ${node}`); console.log(values); console.log("\n====\n"); }
}
``` ---------------------------------------- TITLE: Implementing Error Handling and Resumption with LangGraph Checkpointer (TypeScript)
DESCRIPTION: Demonstrates how to use a `MemorySaver` checkpointer to save the state of a LangGraph workflow. It includes a task (`getInfo`) that simulates failure on the first attempt, showing how the workflow state is preserved before the error. Requires `@langchain/langgraph`.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/functional_api.md#_snippet_25 LANGUAGE: typescript
CODE:
```
import { entrypoint, task, MemorySaver } from "@langchain/langgraph"; // Global variable to track the number of attempts
let attempts = 0; const getInfo = task("getInfo", () => { /* * Simulates a task that fails once before succeeding. * Throws an error on the first attempt, then returns "OK" on subsequent tries. */ attempts += 1; if (attempts < 2) { throw new Error("Failure"); // Simulate a failure on the first attempt } return "OK";
}); // Initialize an in-memory checkpointer for persistence
const checkpointer = new MemorySaver(); const slowTask = task("slowTask", async () => { /* * Simulates a slow-running task by introducing a 1-second delay. */ await new Promise((resolve) => setTimeout(resolve, 1000)); return "Ran slow task.";
}); const main = entrypoint( { checkpointer, name: "main" }, async (inputs: Record<string, any>) => { /* * Main workflow function that runs the slowTask and getInfo tasks sequentially. * * Parameters: * - inputs: Record<string, any> containing workflow input values. * * The workflow first executes `slowTask` and then attempts to execute `getInfo`, * which will fail on the first invocation. */ const slowTaskResult = await slowTask(); // Blocking call to slowTask await getInfo(); // Error will be thrown here on the first attempt return slowTaskResult; }
); // Workflow execution configuration with a unique thread identifier
const config = { configurable: { thread_id: "1", // Unique identifier to track workflow execution },
}; // This invocation will take ~1 second due to the slowTask execution
try { // First invocation will throw an error due to the `getInfo` task failing await main.invoke({ anyInput: "foobar" }, config);
} catch (err) { // Handle the failure gracefully
}
``` ---------------------------------------- TITLE: Implementing Human Review Node with LangGraph TypeScript
DESCRIPTION: Defines a node function `humanReviewNode` for a LangGraph state machine. It uses `interrupt()` to pause execution for human review of tool calls, allowing the user to approve, modify, or provide feedback, and then directs the graph flow accordingly using `Command`.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/review-tool-calls.ipynb#_snippet_0 LANGUAGE: typescript
CODE:
```
function humanReviewNode(state: typeof GraphAnnotation.State) { // this is the value we'll be providing via new Command({ resume: <human_review> }) const humanReview = interrupt({ question: "Is this correct?", // Surface tool calls for review tool_call, }); const [reviewAction, reviewData] = humanReview; // Approve the tool call and continue if (reviewAction === "continue") { return new Command({ goto: "run_tool" }); } // Modify the tool call manually and then continue if (reviewAction === "update") { const updatedMsg = getUpdatedMsg(reviewData); return new Command({ goto: "run_tool", update: { messages: [updatedMsg] } }); } // Give natural language feedback, and then pass that back to the agent if (reviewAction === "feedback") { const feedbackMsg = getFeedbackMsg(reviewData); return new Command({ goto: "call_llm", update: { messages: [feedbackMsg] } }); } throw new Error("Unreachable");
}
``` ---------------------------------------- TITLE: Creating Planning Chain (TypeScript)
DESCRIPTION: Sets up the runnable chain responsible for generating the plan. It combines a `ChatPromptTemplate` that guides the LLM to create a step-by-step plan with an OpenAI model configured to output structured data according to the defined plan schema.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/plan-and-execute/plan-and-execute.ipynb#_snippet_8 LANGUAGE: typescript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts"; const plannerPrompt = ChatPromptTemplate.fromTemplate( `For the given objective, come up with a simple step by step plan. \\
This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\
The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n\n{objective}`,
); const model = new ChatOpenAI({ modelName: "gpt-4-0125-preview",
}).withStructuredOutput(planFunction); const planner = plannerPrompt.pipe(model);
``` ---------------------------------------- TITLE: Executing ReAct Agent with Multiple Tool Calls (TypeScript)
DESCRIPTION: This snippet demonstrates running the compiled LangGraph agent with a user query designed to trigger multiple successive tool calls. It streams the output and logs the details of the final message chunk, including the type, content, and tool calls.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/tool-calling.ipynb#_snippet_12 LANGUAGE: TypeScript
CODE:
```
// example with a multiple tool calls in succession
const streamWithMultiToolCalls = await app.stream( { messages: [{ role: "user", content: "what's the weather in the coolest cities?" }], }, { streamMode: "values" }
)
for await (const chunk of streamWithMultiToolCalls) { const lastMessage = chunk.messages[chunk.messages.length - 1]; const type = lastMessage._getType(); const content = lastMessage.content; const toolCalls = lastMessage.tool_calls; console.dir({ type, content, toolCalls }, { depth: null });
}
``` ---------------------------------------- TITLE: Pausing Graph Execution with `interrupt` in LangGraph (TypeScript)
DESCRIPTION: This snippet demonstrates how to use the `interrupt` function from `@langchain/langgraph` within a node to pause graph execution. It sends a JSON-serializable value (e.g., a question) to the client, allowing for external input or decision-making before resuming the graph. This is crucial for human-in-the-loop workflows.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/low_level.md#_snippet_27 LANGUAGE: TypeScript
CODE:
```
import { interrupt } from "@langchain/langgraph"; const humanApprovalNode = (state: typeof StateAnnotation.State) => { ... const answer = interrupt( // This value will be sent to the client. // It can be any JSON serializable value. { question: "is it ok to continue?"}, ); ...
``` ---------------------------------------- TITLE: Streaming Initial Messages to LangGraph (JavaScript)
DESCRIPTION: Shows how to send a sequence of `HumanMessage` instances to the LangGraph application using the `app.stream` method. It configures the stream with a specific thread ID and update mode, then iterates through the streamed events for each message, printing them using the `printUpdate` function. This demonstrates a basic conversational turn with the graph.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/add-summary-conversation-history.ipynb#_snippet_5 LANGUAGE: JavaScript
CODE:
```
import { HumanMessage } from "@langchain/core/messages"; const config = { configurable: { thread_id: "4" }, streamMode: "updates" as const } const inputMessage = new HumanMessage("hi! I'm bob")
console.log(inputMessage.content)
for await (const event of await app.stream({ messages: [inputMessage] }, config)) { printUpdate(event)
} const inputMessage2 = new HumanMessage("What did I sat my name was?")
console.log(inputMessage2.content)
for await (const event of await app.stream({ messages: [inputMessage2] }, config)) { printUpdate(event)
} const inputMessage3 = new HumanMessage("i like the celtics!")
console.log(inputMessage3.content)
for await (const event of await app.stream({ messages: [inputMessage3] }, config)) { printUpdate(event)
}
``` ---------------------------------------- TITLE: Defining LangGraph State and Workflow in LangGraphJS
DESCRIPTION: Sets up the state schema using LangGraph's Annotation, defines nodes for fetching user information and calling a language model, and constructs the graph workflow connecting these nodes. It includes the prompt template and node logic.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/configuration.ipynb#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { BaseMessage } from "@langchain/core/messages";
import { ChatOpenAI } from "@langchain/openai";
import { ChatAnthropic } from "@langchain/anthropic";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableConfig } from "@langchain/core/runnables";
import { END, START, StateGraph, Annotation,
} from "@langchain/langgraph"; const AgentState = Annotation.Root({ messages: Annotation<BaseMessage[]> ({ reducer: (x, y) => x.concat(y), }), userInfo: Annotation<string | undefined>({ reducer: (x, y) => { return y ? y : x ? x : "N/A"; }, default: () => "N/A", })
}); const promptTemplate = ChatPromptTemplate.fromMessages([ ["system", "You are a helpful assistant.\n\n## User Info:\n{userInfo}"], ["placeholder", "{messages}"],
]); const callModel = async ( state: typeof AgentState.State, config?: RunnableConfig,
) => { const { messages, userInfo } = state; const modelName = config?.configurable?.model; const model = modelName === "claude" ? new ChatAnthropic({ model: "claude-3-haiku-20240307" }) : new ChatOpenAI({ model: "gpt-4o" }); const chain = promptTemplate.pipe(model); const response = await chain.invoke( { messages, userInfo, }, config, ); return { messages: [response] };
}; const fetchUserInformation = async ( _: typeof AgentState.State, config?: RunnableConfig,
) => { const userDB = { user1: { name: "John Doe", email: "jod@langchain.ai", phone: "+1234567890", }, user2: { name: "Jane Doe", email: "jad@langchain.ai", phone: "+0987654321", }, }; const userId = config?.configurable?.user; if (userId) { const user = userDB[userId as keyof typeof userDB]; if (user) { return { userInfo: `Name: ${user.name}\nEmail: ${user.email}\nPhone: ${user.phone}`, }; } } return { userInfo: "N/A" };
}; const workflow = new StateGraph(AgentState) .addNode("fetchUserInfo", fetchUserInformation) .addNode("agent", callModel) .addEdge(START, "fetchUserInfo") .addEdge("fetchUserInfo", "agent") .addEdge("agent", END); const graph = workflow.compile();
``` ---------------------------------------- TITLE: Define Node Returning Command (goto)
DESCRIPTION: Demonstrates a LangGraph node function that returns a Command object to perform a state update ('update') and specify the next node ('goto') within the same function execution.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/command.ipynb#_snippet_0 LANGUAGE: typescript
CODE:
```
const myNode = (state: typeof StateAnnotation.State) => { return new Command({ // state update update: { foo: "bar" }, // control flow goto: "myOtherNode" });
};
``` ---------------------------------------- TITLE: Defining Nodes and Building a Fan-Out/Fan-In Graph
DESCRIPTION: Defines a state annotation with a reducer for aggregating string arrays, creates four simple nodes (a, b, c, d) that append to the state, and builds a StateGraph with edges for fan-out from 'a' to 'b' and 'c', and fan-in from 'b' and 'c' to 'd', ending at 'END'.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/branching.ipynb#_snippet_2 LANGUAGE: javascript
CODE:
```
import { END, START, StateGraph, Annotation } from "@langchain/langgraph"; const StateAnnotation = Annotation.Root({ aggregate: Annotation<string[]> ({ reducer: (x, y) => x.concat(y), })
}); // Create the graph
const nodeA = (state: typeof StateAnnotation.State) => { console.log(`Adding I'm A to ${state.aggregate}`); return { aggregate: [`I'm A`] };
};
const nodeB = (state: typeof StateAnnotation.State) => { console.log(`Adding I'm B to ${state.aggregate}`); return { aggregate: [`I'm B`] };
};
const nodeC = (state: typeof StateAnnotation.State) => { console.log(`Adding I'm C to ${state.aggregate}`); return { aggregate: [`I'm C`] };
};
const nodeD = (state: typeof StateAnnotation.State) => { console.log(`Adding I'm D to ${state.aggregate}`); return { aggregate: [`I'm D`] };
}; const builder = new StateGraph(StateAnnotation) .addNode("a", nodeA) .addEdge(START, "a") .addNode("b", nodeB) .addNode("c", nodeC) .addNode("d", nodeD) .addEdge("a", "b") .addEdge("a", "c") .addEdge("b", "d") .addEdge("c", "d") .addEdge("d", END); const graph = builder.compile();
``` ---------------------------------------- TITLE: Updating Instructions in Store with LLM in LangGraph TypeScript
DESCRIPTION: This snippet shows a LangGraph node responsible for updating agent instructions stored in a BaseStore. It retrieves current instructions, uses an LLM (ChatOpenAI) to generate new instructions based on the current ones and conversation history, and then saves the updated instructions back to the store under a specific namespace and key. It requires BaseStore, State, and ChatOpenAI.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/memory.md#_snippet_7 LANGUAGE: TypeScript
CODE:
```
// Node that updates instructions
const updateInstructions = async (state: State, store: BaseStore) => { const namespace = ["instructions"]; const currentInstructions = await store.search(namespace); // Memory logic const prompt = promptTemplate.format({ instructions: currentInstructions[0].value.instructions, conversation: state.messages, }); const llm = new ChatOpenAI(); const output = await llm.invoke(prompt); const newInstructions = output.content; // Assuming the LLM returns the new instructions await store.put(["agent_instructions"], "agent_a", { instructions: newInstructions, }); // ... rest of the logic
};
``` ---------------------------------------- TITLE: LangGraph: State Definition With Reducer (Resolves Error)
DESCRIPTION: This snippet demonstrates how to define a state property ('someKey') with a reducer. The reducer function specifies how values from concurrent updates should be combined (in this case, concatenating strings into an array), preventing the INVALID_CONCURRENT_GRAPH_UPDATE error.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE.ipynb#_snippet_1 LANGUAGE: typescript
CODE:
```
const StateAnnotation = Annotation.Root({ someKey: Annotation<string[]>({ default: () => [], reducer: (a, b) => a.concat(b), }),
});
``` ---------------------------------------- TITLE: Adding a Conditional Edge with Output Mapping (TypeScript)
DESCRIPTION: This snippet illustrates how to use `addConditionalEdges` with an output mapping object. The `routingFunction` returns a value (e.g., `true` or `false`), which is then mapped to a specific next node ('nodeB' or 'nodeC' in this example). This provides explicit control over routing based on function output.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/low_level.md#_snippet_15 LANGUAGE: typescript
CODE:
```
graph.addConditionalEdges("nodeA", routingFunction, { true: "nodeB", false: "nodeC"
});
``` ---------------------------------------- TITLE: Rewriting Query with LLM (TypeScript)
DESCRIPTION: This function transforms the initial query to potentially improve it. It extracts the first message content as the question, creates a prompt template, initializes a ChatOpenAI model, pipes the prompt to the model, and invokes it with the question, returning the model's response.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rag/langgraph_agentic_rag.ipynb#_snippet_10 LANGUAGE: TypeScript
CODE:
```
async function rewrite(state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> { console.log("---TRANSFORM QUERY---"); const { messages } = state; const question = messages[0].content as string; const prompt = ChatPromptTemplate.fromTemplate( `Look at the input and try to reason about the underlying semantic intent / meaning. \n \nHere is the initial question:\n\n ------- \n{question} \n ------- \nFormulate an improved question:`, ); // Grader const model = new ChatOpenAI({ model: "gpt-4o", temperature: 0, streaming: true, }); const response = await prompt.pipe(model).invoke({ question }); return { messages: [response], };
}
``` ---------------------------------------- TITLE: Defining LangGraph Workflow Edges (TypeScript)
DESCRIPTION: This code defines the transitions between the nodes in the LangGraph workflow. It sets the starting node, adds conditional edges based on the 'agent' and 'gradeDocuments' node outputs to direct the flow to 'retrieve', 'generate', or 'rewrite', and defines the final 'END' node.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rag/langgraph_agentic_rag.ipynb#_snippet_13 LANGUAGE: TypeScript
CODE:
```
import { START } from "@langchain/langgraph"; // Call agent node to decide to retrieve or not
workflow.addEdge(START, "agent"); // Decide whether to retrieve
workflow.addConditionalEdges( "agent", // Assess agent decision shouldRetrieve,
); workflow.addEdge("retrieve", "gradeDocuments"); // Edges taken after the `action` node is called.
workflow.addConditionalEdges( "gradeDocuments", // Assess agent decision checkRelevance, { // Call tool node yes: "generate", no: "rewrite", // placeholder },
); workflow.addEdge("generate", END);
workflow.addEdge("rewrite", "agent"); // Compile
const app = workflow.compile();
``` ---------------------------------------- TITLE: Define Child LangGraph with State Transformation
DESCRIPTION: Defines a 'child' LangGraph subgraph. It includes a node 'child1' that calls the 'grandchildGraph'. Crucially, the 'callGrandchildGraph' function transforms the child's state ('myChildKey') into the grandchild's required input state ('myGrandchildKey') before invocation and then transforms the grandchild's output back into the child's state format.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/subgraph-transform-state.ipynb#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { StateGraph, START, Annotation } from "@langchain/langgraph"; const ChildAnnotation = Annotation.Root({ myChildKey: Annotation<string>,
}); const callGrandchildGraph = async (state: typeof ChildAnnotation.State) => { // NOTE: parent or grandchild keys won't be accessible here // we're transforming the state from the child state channels (`myChildKey`) // to the grandchild state channels (`myGrandchildKey`) const grandchildGraphInput = { myGrandchildKey: state.myChildKey }; // we're transforming the state from the grandchild state channels (`myGrandchildKey`) // back to the child state channels (`myChildKey`) const grandchildGraphOutput = await grandchildGraph.invoke(grandchildGraphInput); return { myChildKey: grandchildGraphOutput.myGrandchildKey + " today?" };
}; const child = new StateGraph(ChildAnnotation) // NOTE: we're passing a function here instead of just compiled graph (`childGraph`) .addNode("child1", callGrandchildGraph) .addEdge(START, "child1"); const childGraph = child.compile();
``` ---------------------------------------- TITLE: Manage LangChain Messages with RemoveMessage in LangGraph (TypeScript)
DESCRIPTION: Demonstrates using the built-in `MessagesAnnotation` to manage a list of LangChain messages. `myNode1` shows how to append new messages (`AIMessage`). `myNode2` shows how to delete messages by generating `RemoveMessage` objects for specific message IDs, causing the `MessagesAnnotation` reducer to remove them from the state list.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/memory.md#_snippet_1 LANGUAGE: typescript
CODE:
```
import { RemoveMessage, AIMessage } from "@langchain/core/messages";
import { MessagesAnnotation } from "@langchain/langgraph"; type State = typeof MessagesAnnotation.State; function myNode1(state: State) { // Add an AI message to the `messages` list in the state return { messages: [new AIMessage({ content: "Hi" })] };
} function myNode2(state: State) { // Delete all but the last 2 messages from the `messages` list in the state const deleteMessages = state.messages .slice(0, -2) .map((m) => new RemoveMessage({ id: m.id })); return { messages: deleteMessages };
}
``` ---------------------------------------- TITLE: Creating and Running Swarm Multi-Agent System (TypeScript)
DESCRIPTION: Defines handoff tools to transfer control between agents, creates specialized React agents, and then uses `createSwarm` to coordinate them. The compiled swarm graph is invoked with a user query, and the output stream is logged, demonstrating dynamic handoffs.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/multi-agent.md#_snippet_3 LANGUAGE: typescript
CODE:
```
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { ChatAnthropic } from "@langchain/anthropic";
// highlight-next-line
import { createSwarm, createHandoffTool } from "@langchain/langgraph-swarm"; const transferToHotelAssistant = createHandoffTool({ agentName: "hotel_assistant", description: "Transfer user to the hotel-booking assistant.",
}); const transferToFlightAssistant = createHandoffTool({ agentName: "flight_assistant", description: "Transfer user to the flight-booking assistant.",
}); const llm = new ChatAnthropic({ modelName: "claude-3-5-sonnet-latest" }); const flightAssistant = createReactAgent({ llm, tools: [bookFlight, transferToHotelAssistant], prompt: "You are a flight booking assistant", name: "flight_assistant",
}); const hotelAssistant = createReactAgent({ llm, tools: [bookHotel, transferToFlightAssistant], prompt: "You are a hotel booking assistant", name: "hotel_assistant",
}); // highlight-next-line
const swarm = createSwarm({ agents: [flightAssistant, hotelAssistant], defaultActiveAgent: "flight_assistant",
}).compile(); const stream = await swarm.stream({ messages: [{ role: "user", content: "first book a flight from BOS to JFK and then book a stay at McKittrick Hotel" }]
}); for await (const chunk of stream) { console.log(chunk); console.log("\n");
}
``` ---------------------------------------- TITLE: Defining Input/Output Schema for StateGraph in JavaScript
DESCRIPTION: This snippet demonstrates how to define separate input and output schemas for a LangGraph StateGraph using Annotation.Root. It defines the schemas, a simple node function, constructs the graph with the specified schemas, and invokes it with sample input.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/input_output_schema.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
import { Annotation, StateGraph } from "@langchain/langgraph"; const InputAnnotation = Annotation.Root({ question: Annotation<string>,
}); const OutputAnnotation = Annotation.Root({ answer: Annotation<string>,
}); const answerNode = (_state: typeof InputAnnotation.State) => { return { answer: "bye" };
}; const graph = new StateGraph({ input: InputAnnotation, output: OutputAnnotation,
}) .addNode("answerNode", answerNode) .addEdge("__start__", "answerNode") .compile(); await graph.invoke({ question: "hi",
});
``` ---------------------------------------- TITLE: Updating State in Deeply Nested Subgraph and Continuing - langgraphjs - Javascript
DESCRIPTION: Demonstrates how to update the state of a specific node ("weatherNode") within the innermost subgraph using its configuration (subgraphState.config) and then continue the execution stream of the grandparentGraph. It simulates providing an assistant response.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/subgraphs-manage-state.ipynb#_snippet_25 LANGUAGE: javascript
CODE:
```
await grandparentGraph.updateState(subgraphState.config, { messages: [{ role: "assistant", content: "rainy" }]
}, "weatherNode"); const updatedGrandparentGraphStream = await grandparentGraph.stream(null, { ...grandparentConfig, streamMode: "updates", subgraphs: true,
}); for await (const update of updatedGrandparentGraphStream) { console.log(update);
} console.log((await grandparentGraph.getState(grandparentConfig)).values.messages)
``` ---------------------------------------- TITLE: Defining and Compiling LangGraph.js Workflow (JavaScript)
DESCRIPTION: Constructs a `StateGraph` workflow by adding the defined nodes (`first_agent`, `agent`, `action`), setting `first_agent` as the entrypoint, defining conditional edges based on `shouldContinue`, adding normal edges, and finally compiling the workflow into a runnable application.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/force-calling-a-tool-first.ipynb#_snippet_9 LANGUAGE: JavaScript
CODE:
```
import { END, START, StateGraph } from "@langchain/langgraph"; // Define a new graph
const workflow = new StateGraph(AgentState) // Define the new entrypoint .addNode("first_agent", firstModel) // Define the two nodes we will cycle between .addNode("agent", callModel) .addNode("action", toolNode) // Set the entrypoint as `first_agent` // by creating an edge from the virtual __start__ node to `first_agent` .addEdge(START, "first_agent") // We now add a conditional edge .addConditionalEdges( // First, we define the start node. We use `agent`. // This means these are the edges taken after the `agent` node is called. "agent", // Next, we pass in the function that will determine which node is called next. shouldContinue, // Finally we pass in a mapping. // The keys are strings, and the values are other nodes. // END is a special node marking that the graph should finish. // What will happen is we will call `should_continue`, and then the output of that // will be matched against the keys in this mapping. // Based on which one it matches, that node will then be called. { // If `tools`, then we call the tool node. continue: "action", // Otherwise we finish. end: END, }, ) // We now add a normal edge from `tools` to `agent`. // This means that after `tools` is called, `agent` node is called next. .addEdge("action", "agent") // After we call the first agent, we know we want to go to action .addEdge("first_agent", "action"); // Finally, we compile it!
// This compiles it into a LangChain Runnable,
// meaning you can use it as you would any other runnable
const app = workflow.compile();
``` ---------------------------------------- TITLE: Defining State, Nodes, Routing, and Building Graph with Conditional Edges (JS)
DESCRIPTION: Defines the state schema using Annotation.Root, creates several simple nodes (nodeA2 through nodeE2) that modify the state, defines a routing function routeCDorBC that returns target node names based on the state's which property, and constructs a StateGraph instance, adding nodes and edges, including a conditional edge from node "a" using addConditionalEdges.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/branching.ipynb#_snippet_5 LANGUAGE: javascript
CODE:
```
const ConditionalBranchingAnnotation = Annotation.Root({ aggregate: Annotation<string[]> ({ reducer: (x, y) => x.concat(y), }), which: Annotation<string>({ reducer: (x: string, y: string) => (y ?? x), })
}) // Create the graph
const nodeA2 = (state: typeof ConditionalBranchingAnnotation.State) => { console.log(`Adding I'm A to ${state.aggregate}`); return { aggregate: [`I'm A`] };
};
const nodeB2 = (state: typeof ConditionalBranchingAnnotation.State) => { console.log(`Adding I'm B to ${state.aggregate}`); return { aggregate: [`I'm B`] };
};
const nodeC2 = (state: typeof ConditionalBranchingAnnotation.State) => { console.log(`Adding I'm C to ${state.aggregate}`); return { aggregate: [`I'm C`] };
};
const nodeD2 = (state: typeof ConditionalBranchingAnnotation.State) => { console.log(`Adding I'm D to ${state.aggregate}`); return { aggregate: [`I'm D`] };
};
const nodeE2 = (state: typeof ConditionalBranchingAnnotation.State) => { console.log(`Adding I'm E to ${state.aggregate}`); return { aggregate: [`I'm E`] };
}; // Define the route function
function routeCDorBC(state: typeof ConditionalBranchingAnnotation.State): string[] { if (state.which === "cd") { return ["c", "d"]; } return ["b", "c"];
} const builder2 = new StateGraph(ConditionalBranchingAnnotation) .addNode("a", nodeA2) .addEdge(START, "a") .addNode("b", nodeB2) .addNode("c", nodeC2) .addNode("d", nodeD2) .addNode("e", nodeE2) // Add conditional edges // Third parameter is to support visualizing the graph .addConditionalEdges("a", routeCDorBC, ["b", "c", "d"]) .addEdge("b", "e") .addEdge("c", "e") .addEdge("d", "e") .addEdge("e", END); const graph2 = builder2.compile();
``` ---------------------------------------- TITLE: Defining LangGraph State with Annotation in JavaScript
DESCRIPTION: This JavaScript snippet defines the state object (`AgentState`) used in the LangGraph workflow. It utilizes `Annotation.Root` to structure the state, including a `messages` array (concatenated by a reducer) and a `sender` string (updated by a reducer to track the last sender).
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/multi_agent/multi_agent_collaboration.ipynb#_snippet_2 LANGUAGE: javascript
CODE:
```
import { BaseMessage } from "@langchain/core/messages";
import { Annotation } from "@langchain/langgraph"; // This defines the object that is passed between each node
// in the graph. We will create different nodes for each agent and tool
const AgentState = Annotation.Root({ messages: Annotation<BaseMessage[]> ({ reducer: (x, y) => x.concat(y), }), sender: Annotation<string>({ reducer: (x, y) => y ?? x ?? "user", default: () => "user", }),
})
``` ---------------------------------------- TITLE: Implementing a Custom ReAct Agent with LangGraph (TypeScript)
DESCRIPTION: This TypeScript code defines a custom ReAct agent workflow using LangGraph. It sets up tools, a language model, defines nodes for calling the model and executing tools, and uses a conditional edge to route based on the model's output (tool call or final response). It requires LangChain and LangGraph dependencies and API keys for OpenAI and Tavily.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/quickstart.ipynb#_snippet_8 LANGUAGE: typescript
CODE:
```
// agent.ts // IMPORTANT - Add your API keys here. Be careful not to publish them.
process.env.OPENAI_API_KEY = "sk-...";
process.env.TAVILY_API_KEY = "tvly-..."; import { TavilySearchResults } from "@langchain/community/tools/tavily_search";
import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage, AIMessage } from "@langchain/core/messages";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import { StateGraph, MessagesAnnotation } from "@langchain/langgraph"; // Define the tools for the agent to use
const tools = [new TavilySearchResults({ maxResults: 3 })];
const toolNode = new ToolNode(tools); // Create a model and give it access to the tools
const model = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0,
}).bindTools(tools); // Define the function that determines whether to continue or not
function shouldContinue({ messages }: typeof MessagesAnnotation.State) { const lastMessage = messages[messages.length - 1] as AIMessage; // If the LLM makes a tool call, then we route to the "tools" node if (lastMessage.tool_calls?.length) { return "tools"; } // Otherwise, we stop (reply to the user) using the special "__end__" node return "__end__";
} // Define the function that calls the model
async function callModel(state: typeof MessagesAnnotation.State) { const response = await model.invoke(state.messages); // We return a list, because this will get added to the existing list return { messages: [response] };
} // Define a new graph
const workflow = new StateGraph(MessagesAnnotation) .addNode("agent", callModel) .addEdge("__start__", "agent") // __start__ is a special name for the entrypoint .addNode("tools", toolNode) .addEdge("tools", "agent") .addConditionalEdges("agent", shouldContinue); // Finally, we compile it into a LangChain Runnable.
const app = workflow.compile(); // Use the agent
const finalState = await app.invoke({ messages: [new HumanMessage("what is the weather in sf")]
});
console.log(finalState.messages[finalState.messages.length - 1].content); const nextState = await app.invoke({ // Including the messages from the previous run gives the LLM context. // This way it knows we're asking about the weather in NY messages: [...finalState.messages, new HumanMessage("what about ny")]
});
console.log(nextState.messages[nextState.messages.length - 1].content);
``` ---------------------------------------- TITLE: Creating Loop with Conditional Edge in LangGraph (TypeScript)
DESCRIPTION: Demonstrates how to define a conditional edge function `route` that checks a termination condition based on the state. It then constructs a `StateGraph` with nodes 'a' and 'b', adding the conditional edge from 'a' and a standard edge from 'b' back to 'a' to form a loop, and compiles the graph.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/recursion-limit.ipynb#_snippet_0 LANGUAGE: ts
CODE:
```
const route = async function (state: typeof StateAnnotation.State) { if (terminationCondition(state)) { return "__END__"; } else { return "a"; }
} const graph = StateGraph(State) .addNode(a) .addNode(b) .addConditionalEdges("a", route) .addEdge("b", "a") .compile();
``` ---------------------------------------- TITLE: Implementing Authentication with LangGraph JS/TS SDK
DESCRIPTION: This snippet demonstrates how to set up an authentication handler using `Auth().authenticate()`. It shows validating an API key from the request headers and returning a user object with `identity` and `permissions`. It also shows how to raise an `HTTPException` for invalid credentials.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/auth.md#_snippet_0 LANGUAGE: typescript
CODE:
```
import { Auth, HTTPException } from "@langchain/langgraph-sdk/auth"; // (1) Validate the credentials
const isValidKey = (key: string) => { return true;
}; export const auth = new Auth().authenticate(async (request: Request) => { const apiKey = request.headers.get("x-api-key"); if (!apiKey || !isValidKey(apiKey)) { // (3) Raise an HTTPException throw new HTTPException(401, { message: "Invalid API key" }); } // (2) Return user information containing the user's identity and user information if valid return { // required, unique user identifier identity: "user-123", // required, list of permissions permissions: [], // optional, assumed `true` by default is_authenticated: true, // You can add more custom fields if you want to implement other auth patterns role: "admin", org_id: "org-123", };
});
``` ---------------------------------------- TITLE: Generating LangGraph Tools with Closures (JavaScript)
DESCRIPTION: This JavaScript/TypeScript function demonstrates how to create a LangGraph tool (`updateFavoritePets`) using a closure. The function `generateTools` takes the current `state` and defines a tool that can access this state, as well as configuration parameters (`userId`) and the LangGraph store via the `config` object passed during invocation. The tool's schema is defined using `z.object` for LLM interaction.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/pass-run-time-values-to-tools.ipynb#_snippet_13 LANGUAGE: JavaScript
CODE:
```
function generateTools(state: typeof MessagesAnnotation.State) { const updateFavoritePets = tool( async (input, config: LangGraphRunnableConfig) => { // Some arguments are populated by the LLM; these are included in the schema below const { pets } = input; // Others (such as a UserID) are best provided via the config // This is set when when invoking or streaming the graph const userId = config.configurable?.userId; // LangGraph's managed key-value store is also accessible via the config const store = config.store; await store.put([userId, "pets"], "names", pets ); await store.put([userId, "pets"], "context", {content: state.messages[0].content}); return "update_favorite_pets called."; }, { // The LLM "sees" the following schema: name: "update_favorite_pets", description: "add to the list of favorite pets.", schema: z.object({ pets: z.array(z.string()) }) } ); return [updateFavoritePets];
};
``` ---------------------------------------- TITLE: Manage State with Checkpoints and getPreviousState - TypeScript
DESCRIPTION: Shows how to configure an entrypoint with a checkpointer to automatically save state between invocations. The getPreviousState function is used within the entrypoint to access the state from the previous run.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/functional_api.md#_snippet_10 LANGUAGE: typescript
CODE:
```
const myWorkflow = entrypoint( { checkpointer, name: "myWorkflow" }, async (number: number) => { const previous = getPreviousState<number>(); return number + (previous ?? 0); }
); const config = { configurable: { thread_id: "some_thread_id", },
}; await myWorkflow.invoke(1, config); // 1 (previous was undefined)
await myWorkflow.invoke(2, config); // 3 (previous was 1 from the previous invocation)
``` ---------------------------------------- TITLE: Building Prompt Chain Graph with LangGraph (Graph API) - TypeScript
DESCRIPTION: Defines the state, node functions (LLM calls and a gate), and constructs a state graph using LangGraph's Graph API to implement a prompt chaining workflow for generating and refining a joke. It includes a conditional edge based on a punchline check.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_4 LANGUAGE: TypeScript
CODE:
```
import { StateGraph, Annotation } from "@langchain/langgraph"; // Graph state
const StateAnnotation = Annotation.Root({ topic: Annotation<string>, joke: Annotation<string>, improvedJoke: Annotation<string>, finalJoke: Annotation<string>,
}); // Define node functions // First LLM call to generate initial joke
async function generateJoke(state: typeof StateAnnotation.State) { const msg = await llm.invoke(`Write a short joke about ${state.topic}`); return { joke: msg.content };
} // Gate function to check if the joke has a punchline
function checkPunchline(state: typeof StateAnnotation.State) { // Simple check - does the joke contain "?" or "!" if (state.joke?.includes("?") || state.joke?.includes("!")) { return "Pass"; } return "Fail";
} // Second LLM call to improve the joke
async function improveJoke(state: typeof StateAnnotation.State) { const msg = await llm.invoke( `Make this joke funnier by adding wordplay: ${state.joke}` ); return { improvedJoke: msg.content };
} // Third LLM call for final polish
async function polishJoke(state: typeof StateAnnotation.State) { const msg = await llm.invoke( `Add a surprising twist to this joke: ${state.improvedJoke}` ); return { finalJoke: msg.content };
} // Build workflow
const chain = new StateGraph(StateAnnotation) .addNode("generateJoke", generateJoke) .addNode("improveJoke", improveJoke) .addNode("polishJoke", polishJoke) .addEdge("__start__", "generateJoke") .addConditionalEdges("generateJoke", checkPunchline, { Pass: "improveJoke", Fail: "__end__" }) .addEdge("improveJoke", "polishJoke") .addEdge("polishJoke", "__end__") .compile(); // Invoke
const state = await chain.invoke({ topic: "cats" });
console.log("Initial joke:");
console.log(state.joke);
console.log("\n--- --- ---\n");
if (state.improvedJoke !== undefined) { console.log("Improved joke:"); console.log(state.improvedJoke); console.log("\n--- --- ---\n"); console.log("Final joke:"); console.log(state.finalJoke);
} else { console.log("Joke failed quality gate - no punchline detected!");
}
``` ---------------------------------------- TITLE: Updating Graph State from Within a Tool using Command
DESCRIPTION: This example shows how to return a Command object from a LangChain tool to update the graph state directly. It demonstrates updating custom state keys like 'user_info' and crucially, the 'messages' array with a ToolMessage, which is required for valid message history when using LLM providers. This pattern is common for tools that fetch data and need to propagate it to the main graph state.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/low_level.md#_snippet_23 LANGUAGE: TypeScript
CODE:
```
import { tool } from "@langchain/core/tools"; const lookupUserInfo = tool(async (input, config) => { const userInfo = getUserInfo(config); return new Command({ // update state keys update: { user_info: userInfo, messages: [ new ToolMessage({ content: "Successfully looked up user information", tool_call_id: config.toolCall.id, }), ], }, });
}, { name: "lookup_user_info", description: "Use this to look up user information to better assist them with their questions.", schema: z.object(...)
});
``` ---------------------------------------- TITLE: Defining Game State and NPC Agents with LangGraph.js
DESCRIPTION: This snippet defines the shared game state structure using `Annotation.Root` with reducers for resources. It then defines asynchronous functions for each NPC agent (`villager`, `guard`, `merchant`, `thief`). Each agent function inspects the current state and returns a `Command` to either loop back to itself (`goto: "agentName"`) with state updates or halt (`goto: "__end__"`). Finally, it constructs the `StateGraph`, adds the defined nodes (agents), and defines edges from `__start__` to all agents, allowing them to run concurrently.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/multi-agent-network.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { Command, StateGraph, Annotation } from "@langchain/langgraph"; const GameStateAnnotation = Annotation.Root({ // note that we're defining a reducer (operator.add) here. // This will allow all agents to write their updates for resources concurrently. wood: Annotation<number>({ default: () => 0, reducer: (a, b) => a + b, }), food: Annotation<number>({ default: () => 0, reducer: (a, b) => a + b, }), gold: Annotation<number>({ default: () => 0, reducer: (a, b) => a + b, }), guardOnDuty: Annotation<boolean>,
}); /** Villager NPC that gathers wood and food. */
const villager = async (state: typeof GameStateAnnotation.State) => { const currentResources = state.wood + state.food; // Continue gathering until we have enough resources if (currentResources < 15) { console.log("Villager gathering resources."); return new Command({ goto: "villager", update: { wood: 3, food: 1, }, }); } // NOTE: Returning Command({goto: "__end__"}) is not necessary for the graph to run correctly // but it's useful for visualization, to show that the agent actually halts return new Command({ goto: "__end__", });
} /** Guard NPC that protects gold and consumes food. */
const guard = async (state: typeof GameStateAnnotation.State) => { if (!state.guardOnDuty) { return new Command({ goto: "__end__", }); } // Guard needs food to keep patrolling if (state.food > 0) { console.log("Guard patrolling."); // Loop back to the 'guard' agent return new Command({ goto: "guard", update: { food: -1 }, }); } console.log("Guard leaving to get food."); return new Command({ goto: "__end__", update: { guardOnDuty: false, }, });
}; /** Merchant NPC that trades wood for gold. */
const merchant = async (state: typeof GameStateAnnotation.State) => { // Trade wood for gold when available if (state.wood >= 5) { console.log("Merchant trading wood for gold."); return new Command({ goto: "merchant", update: { wood: -5, gold: 1 } }); } return new Command({ goto: "__end__" });
}; /** Thief NPC that steals gold if the guard leaves to get food. */
const thief = async (state: typeof GameStateAnnotation.State) => { if (!state.guardOnDuty) { console.log("Thief stealing gold."); return new Command({ goto: "__end__", update: { gold: -state.gold } }); } // keep thief on standby (loop back to the 'thief' agent) return new Command({ goto: "thief" });
}; const gameGraph = new StateGraph(GameStateAnnotation) .addNode("villager", villager, { ends: ["villager", "__end__"], }) .addNode("guard", guard, { ends: ["guard", "__end__"], }) .addNode("merchant", merchant, { ends: ["merchant", "__end__"], }) .addNode("thief", thief, { ends: ["thief", "__end__"], }) .addEdge("__start__", "villager") .addEdge("__start__", "guard") .addEdge("__start__", "merchant") .addEdge("__start__", "thief") .compile();
``` ---------------------------------------- TITLE: Define LangChain Replanner Step (JS)
DESCRIPTION: Sets up the components for a replanning step in a LangGraph workflow. It includes a tool definition for the final response, a prompt template for guiding the replanning process based on the original plan and past steps, and a chain that invokes an OpenAI model (gpt-4o) with the prompt and tools, parsing the output.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/plan-and-execute/plan-and-execute.ipynb#_snippet_10 LANGUAGE: JavaScript
CODE:
```
import { JsonOutputToolsParser } from "@langchain/core/output_parsers/openai_tools"; const response = zodToJsonSchema( z.object({ response: z.string().describe("Response to user."), }),
); const responseTool = { type: "function", function: { name: "response", description: "Response to user.", parameters: response, },
}; const replannerPrompt = ChatPromptTemplate.fromTemplate( `For the given objective, come up with a simple step by step plan. This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps.
The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps. Your objective was this:
{input} Your original plan was this:
{plan} You have currently done the follow steps:
{pastSteps} Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that and use the 'response' function.
Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.`,
); const parser = new JsonOutputToolsParser();
const replanner = replannerPrompt .pipe( new ChatOpenAI({ model: "gpt-4o" }).bindTools([ planTool, responseTool, ]), ) .pipe(parser); ``` ---------------------------------------- TITLE: Defining Human Assistance Tool (Langchain.js)
DESCRIPTION: Defines a 'humanAssistance' tool using '@langchain/langgraph's 'interrupt' function. This tool allows the agent to request human intervention, taking a query string as input and returning the human's response data. It also defines the 'tools' array combining 'getWeather' and 'humanAssistance'.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/wait-user-input-functional.ipynb#_snippet_7 LANGUAGE: javascript
CODE:
```
import { interrupt } from "@langchain/langgraph";
import { z } from "zod"; const humanAssistance = tool(async ({ query }) => { const humanResponse = interrupt({ query }); return humanResponse.data;
}, { name: "humanAssistance", description: "Request assistance from a human.", schema: z.object({ query: z.string().describe("Human readable question for the human") })
}); const tools = [getWeather, humanAssistance];
``` ---------------------------------------- TITLE: Adding Edges for Technical and Billing Support Nodes (TypeScript)
DESCRIPTION: Adds a direct edge from "technical_support" to "__end__". Also adds conditional edges from "billing_support" that route to "handle_refund" if `state.nextRepresentative` includes "REFUND", otherwise routes to "__end__".
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/chatbots/customer_support_small_model.ipynb#_snippet_10 LANGUAGE: typescript
CODE:
```
builder = builder .addEdge("technical_support", "__end__") .addConditionalEdges("billing_support", async (state) => { if (state.nextRepresentative.includes("REFUND")) { return "refund"; } else { return "__end__"; } }, { refund: "handle_refund", __end__: "__end__", }) .addEdge("handle_refund", "__end__"); console.log("Added edges!");
``` ---------------------------------------- TITLE: Enable LangSmith Tracing (TypeScript)
DESCRIPTION: Configures environment variables to enable LangSmith tracing for observability, setting the tracing version and the LangSmith API key.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/add-summary-conversation-history.ipynb#_snippet_2 LANGUAGE: TypeScript
CODE:
```
process.env.LANGCHAIN_TRACING_V2 = 'true'\nprocess.env.LANGCHAIN_API_KEY = 'YOUR_API_KEY'
``` ---------------------------------------- TITLE: Using MongoDBSaver for LangGraph Checkpoints in TypeScript
DESCRIPTION: This snippet shows how to connect to MongoDB, instantiate the MongoDBSaver, and perform basic checkpoint operations like saving (put), loading (get), and listing checkpoints for a specific thread.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/libs/checkpoint-mongodb/README.md#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { MongoClient } from "mongodb";
import { MongoDBSaver } from "@langchain/langgraph-checkpoint-mongodb"; const writeConfig = { configurable: { thread_id: "1", checkpoint_ns: "" }
};
const readConfig = { configurable: { thread_id: "1" }
}; const client = new MongoClient(process.env.MONGODB_URL); const checkpointer = new MongoDBSaver({ client });
const checkpoint = { v: 1, ts: "2024-07-31T20:14:19.804150+00:00", id: "1ef4f797-8335-6428-8001-8a1503f9b875", channel_values: { my_key: "meow", node: "node" }, channel_versions: { __start__: 2, my_key: 3, "start:node": 3, node: 3 }, versions_seen: { __input__: {}, __start__: { __start__: 1 }, node: { "start:node": 2 } }, pending_sends: [],
} // store checkpoint
await checkpointer.put(writeConfig, checkpoint, {}, {}); // load checkpoint
await checkpointer.get(readConfig); // list checkpoints
for await (const checkpoint of checkpointer.list(readConfig)) { console.log(checkpoint);
} await client.close();
``` ---------------------------------------- TITLE: Creating and Running Trajectory Match Evaluator (TypeScript)
DESCRIPTION: Shows how to import and use `createTrajectoryMatchEvaluator` from `agentevals`. It defines sample outputs and reference outputs, configures the evaluator with a `trajectoryMatchMode` (e.g., "superset"), and demonstrates running the evaluator with the defined outputs.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/evals.md#_snippet_2 LANGUAGE: ts
CODE:
```
// highlight-next-line
import { createTrajectoryMatchEvaluator } from "agentevals"; const outputs = [ { role: "assistant", tool_calls: [ { function: { name: "get_weather", arguments: JSON.stringify({ city: "san francisco" }), }, }, { function: { name: "get_directions", arguments: JSON.stringify({ destination: "presidio" }), }, }, ], },
]; const referenceOutputs = [ { role: "assistant", tool_calls: [ { function: { name: "get_weather", arguments: JSON.stringify({ city: "san francisco" }), }, }, ], },
]; // Create the evaluator
const evaluator = createTrajectoryMatchEvaluator({ // highlight-next-line trajectoryMatchMode: "superset", // (1)!
}) // Run the evaluator
const result = await evaluator({ outputs, referenceOutputs,
});
``` ---------------------------------------- TITLE: Implementing Parallel LLM Calls using LangGraph Functional API (TypeScript)
DESCRIPTION: This snippet uses the LangGraph Functional API to define tasks for individual LLM calls and an aggregator. The main workflow uses `Promise.all` to execute the three LLM tasks concurrently before passing their results to the aggregator task.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { task, entrypoint } from "@langchain/langgraph"; // Tasks // First LLM call to generate initial joke
const callLlm1 = task("generateJoke", async (topic: string) => { const msg = await llm.invoke(`Write a joke about ${topic}`); return msg.content;
}); // Second LLM call to generate story
const callLlm2 = task("generateStory", async (topic: string) => { const msg = await llm.invoke(`Write a story about ${topic}`); return msg.content;
}); // Third LLM call to generate poem
const callLlm3 = task("generatePoem", async (topic: string) => { const msg = await llm.invoke(`Write a poem about ${topic}`); return msg.content;
}); // Combine outputs
const aggregator = task("aggregator", async (params: { topic: string; joke: string; story: string; poem: string;
}) => { const { topic, joke, story, poem } = params; return `Here's a story, joke, and poem about ${topic}!\n\n` + `STORY:\n${story}\n\n` + `JOKE:\n${joke}\n\n` + `POEM:\n${poem}`;
}); // Build workflow
const workflow = entrypoint( "parallelWorkflow", async (topic: string) => { const [joke, story, poem] = await Promise.all([ callLlm1(topic), callLlm2(topic), callLlm3(topic), ]); return aggregator({ topic, joke, story, poem }); }
); // Invoke
const stream = await workflow.stream("cats", { streamMode: "updates",
}); for await (const step of stream) { console.log(step);
} ``` ---------------------------------------- TITLE: Implementing Human Node and Resuming Graph with Interrupt in LangGraph (TypeScript)
DESCRIPTION: Defines a `humanNode` that uses `interrupt` to pause execution and collect human input. Shows how to compile the graph with a checkpointer, invoke it to trigger the interrupt, and later resume it using a `Command` object with the human-provided value.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/human_in_the_loop.md#_snippet_0 LANGUAGE: typescript
CODE:
```
import { interrupt } from "@langchain/langgraph"; function humanNode(state: typeof GraphAnnotation.State) { const value = interrupt( // Any JSON serializable value to surface to the human. // For example, a question or a piece of text or a set of keys in the state { text_to_revise: state.some_text, } ); // Update the state with the human's input or route the graph based on the input return { some_text: value, };
} const graph = workflow.compile({ checkpointer, // Required for `interrupt` to work
}); // Run the graph until the interrupt
const threadConfig = { configurable: { thread_id: "some_id" } };
await graph.invoke(someInput, threadConfig); // Below code can run some amount of time later and/or in a different process // Human input
const valueFromHuman = "..."; // Resume the graph with the human's input
await graph.invoke(new Command({ resume: valueFromHuman }), threadConfig);
``` ---------------------------------------- TITLE: Defining Arithmetic Tools with LangChain
DESCRIPTION: Defines simple arithmetic tools (add, multiply, divide) using `@langchain/core/tools` and `zod` for schema validation. These tools are intended to be used by an LLM within an agent workflow.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_25 LANGUAGE: TypeScript
CODE:
```
import { tool } from "@langchain/core/tools";
import { z } from "zod"; // Define tools
const multiply = tool( async ({ a, b }: { a: number; b: number }) => { return a * b; }, { name: "multiply", description: "Multiply two numbers together", schema: z.object({ a: z.number().describe("first number"), b: z.number().describe("second number"), }), }
); const add = tool( async ({ a, b }: { a: number; b: number }) => { return a + b; }, { name: "add", description: "Add two numbers together", schema: z.object({ a: z.number().describe("first number"), b: z.number().describe("second number"), }), }
); const divide = tool( async ({ a, b }: { a: number; b: number }) => { return a / b; }, { name: "divide", description: "Divide two numbers", schema: z.object({ a: z.number().describe("first number"), b: z.number().describe("second number"), }), }
); // Augment the LLM with tools
const tools = [add, multiply, divide];
const toolsByName = Object.fromEntries(tools.map((tool) => [tool.name, tool]));
const llmWithTools = llm.bindTools(tools);
``` ---------------------------------------- TITLE: Defining and Compiling LangGraph with Breakpoint (TypeScript)
DESCRIPTION: Defines a simple StateGraph with three steps, connects them sequentially, and compiles the graph. It configures a MemorySaver as a checkpointer and sets an interruption point before step3.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/breakpoints.ipynb#_snippet_3 LANGUAGE: typescript
CODE:
```
import { StateGraph, START, END, Annotation } from "@langchain/langgraph";
import { MemorySaver } from "@langchain/langgraph"; const GraphState = Annotation.Root({ input: Annotation<string>
}); const step1 = (state: typeof GraphState.State) => { console.log("---Step 1---"); return state;
} const step2 = (state: typeof GraphState.State) => { console.log("---Step 2---"); return state;
} const step3 = (state: typeof GraphState.State) => { console.log("---Step 3---"); return state;
} const builder = new StateGraph(GraphState) .addNode("step1", step1) .addNode("step2", step2) .addNode("step3", step3) .addEdge(START, "step1") .addEdge("step1", "step2") .addEdge("step2", "step3") .addEdge("step3", END); // Set up memory
const graphStateMemory = new MemorySaver() const graph = builder.compile({ checkpointer: graphStateMemory, interruptBefore: ["step3"]
});
``` ---------------------------------------- TITLE: Invoking LangGraph with User ID and Thread ID (TypeScript)
DESCRIPTION: Sets up the configuration object including both `thread_id` (for checkpointer) and `user_id` (for store namespacing). It then invokes the compiled graph using `stream`, passing the input messages and the configuration, and iterates through the resulting stream of updates.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/persistence.md#_snippet_14 LANGUAGE: TypeScript
CODE:
```
// Invoke the graph
const user_id = "1";
const config = { configurable: { thread_id: "1", user_id } }; // First let's just say hi to the AI
const stream = await graph.stream( { messages: [{ role: "user", content: "hi" }] }, { ...config, streamMode: "updates" },
); for await (const update of stream) { console.log(update);
}
``` ---------------------------------------- TITLE: Resume LangGraph with Tool Call Feedback Command (JS)
DESCRIPTION: Resumes the LangGraph execution by streaming a `Command` object. The command includes a `resume` action set to 'feedback' and provides user feedback data. This typically injects the feedback as a mock tool result and continues the graph flow, often leading back to the LLM.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/review-tool-calls.ipynb#_snippet_16 LANGUAGE: JavaScript
CODE:
```
for await (const event of await graph.stream( new Command({ resume: { action: "feedback", data: "User requested changes: use <city, country> format for location" } }), config
)) { const recentMsg = event.messages[event.messages.length - 1]; console.log(`================================ ${recentMsg._getType()} Message (1) =================================`) console.log(recentMsg.content);
}
``` ---------------------------------------- TITLE: Creating a Tool-Using Agent Runnable in LangChainJS
DESCRIPTION: This JavaScript function `createAgent` constructs a runnable agent using LangChain components. It combines a prompt template, an LLM (ChatOpenAI), and a list of StructuredTools, formatting the tools for the LLM and binding them to the prompt for execution.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/multi_agent/multi_agent_collaboration.ipynb#_snippet_1 LANGUAGE: javascript
CODE:
```
import { ChatPromptTemplate, MessagesPlaceholder,
} from "@langchain/core/prompts";
import { StructuredTool } from "@langchain/core/tools";
import { convertToOpenAITool } from "@langchain/core/utils/function_calling";
import { Runnable } from "@langchain/core/runnables";
import { ChatOpenAI } from "@langchain/openai"; /** * Create an agent that can run a set of tools. */
async function createAgent({ llm, tools, systemMessage,
}: { llm: ChatOpenAI; tools: StructuredTool[]; systemMessage: string;
}): Promise<Runnable> { const toolNames = tools.map((tool) => tool.name).join(", "); const formattedTools = tools.map((t) => convertToOpenAITool(t)); let prompt = ChatPromptTemplate.fromMessages([ [ "system", "You are a helpful AI assistant, collaborating with other assistants." + " Use the provided tools to progress towards answering the question." + " If you are unable to fully answer, that's OK, another assistant with different tools " + " will help where you left off. Execute what you can to make progress." + " If you or any of the other assistants have the final answer or deliverable," + " prefix your response with FINAL ANSWER so the team knows to stop." + " You have access to the following tools: {tool_names}.\n{system_message}", ], new MessagesPlaceholder("messages"), ]); prompt = await prompt.partial({ system_message: systemMessage, tool_names: toolNames, }); return prompt.pipe(llm.bind({ tools: formattedTools }));
}
``` ---------------------------------------- TITLE: Creating LangGraph ToolNode (TypeScript)
DESCRIPTION: Instantiates a `ToolNode` from `@langchain/langgraph/prebuilt`. This node is responsible for executing the tools defined in the `tools` array when called within the graph.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/stream-updates.ipynb#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { ToolNode } from "@langchain/langgraph/prebuilt"; const toolNode = new ToolNode(tools);
``` ---------------------------------------- TITLE: Saving User Info with a LangGraph Tool (TypeScript)
DESCRIPTION: This snippet defines a LangGraph tool `saveUserInfo` that takes user information as input and saves it to the store provided in the configuration. It demonstrates accessing the store and configurable parameters (`userId`) from the tool's execution context and uses Zod for schema validation.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/memory.md#_snippet_2 LANGUAGE: ts
CODE:
```
import { initChatModel } from "langchain/chat_models/universal";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { InMemoryStore } from "@langchain/langgraph-checkpoint";
import { getStore } from "@langchain/langgraph";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const store = new InMemoryStore(); // (1)! interface UserInfo { // (2)! name: string;
} // Save user info tool
const saveUserInfo = tool( // (3)! async (input: UserInfo, config: LangGraphRunnableConfig): Promise<string> => { // Same as that provided to `createReactAgent` // highlight-next-line const store = config.store; // (4)! if (!store) { throw new Error("store is required when compiling the graph"); } const userId = config.configurable?.userId; if (!userId) { throw new Error("userId is required in the config"); } await store.put(["users"], userId, input); // (5)! return "Successfully saved user info."; }, { name: "save_user_info", description: "Save user info.", schema: z.object({ name: z.string(), }), }
); const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest");
const agent = createReactAgent({ llm, tools: [saveUserInfo], // highlight-next-line store,
}); // Run the agent
await agent.invoke( { messages: [ { role: "user", content: "My name is John Smith" } ] }, { configurable: { userId: "user_123" } } // (6)!
); // You can access the store directly to get the value
const userInfo = await store.get(["users"], "user_123")
userInfo.value
``` ---------------------------------------- TITLE: Update LangGraph State Before Interrupted Node - JavaScript
DESCRIPTION: Updates the state of the LangGraph with a new input value *before* the node that caused the interrupt. This allows resuming execution past the interrupt condition.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/dynamic_breakpoints.ipynb#_snippet_8 LANGUAGE: JavaScript
CODE:
```
// NOTE: this update will be applied as of the last successful node before the interrupt,
// i.e. `step1`, right before the node with an interrupt
await graph.updateState(config2, { input: "short" });
``` ---------------------------------------- TITLE: Multi-Vector Indexing with LangGraph InMemoryStore (JavaScript)
DESCRIPTION: Configures and uses the InMemoryStore to store memories with multiple fields, specifying which fields should be used for indexing (embeddings). It demonstrates storing data with different content/emotion pairs and then searching based on a specific indexed field (emotional context).
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/semantic-search.ipynb#_snippet_14 LANGUAGE: javascript
CODE:
```
import { InMemoryStore } from "@langchain/langgraph"; // Configure store to embed both memory content and emotional context
const multiVectorStore = new InMemoryStore({ index: { embeddings: embeddings, dims: 1536, fields: ["memory", "emotional_context"], },
}); // Store memories with different content/emotion pairs
await multiVectorStore.put(["user_123", "memories"], "mem1", { memory: "Had pizza with friends at Mario's", emotional_context: "felt happy and connected", this_isnt_indexed: "I prefer ravioli though",
});
await multiVectorStore.put(["user_123", "memories"], "mem2", { memory: "Ate alone at home", emotional_context: "felt a bit lonely", this_isnt_indexed: "I like pie",
}); // Search focusing on emotional state - matches mem2
const results = await multiVectorStore.search(["user_123", "memories"], { query: "times they felt isolated", limit: 1,
}); console.log("Expect mem 2"); for (const r of results) { console.log(`Item: ${r.key}; Score(${r.score})`); console.log(`Memory: ${r.value.memory}`); console.log(`Emotion: ${r.value.emotional_context}`);
}
``` ---------------------------------------- TITLE: Defining Generate Node in LangGraph (JavaScript)
DESCRIPTION: Defines a LangGraph node function that generates an answer using a RAG chain. It takes the question and documents from the state, invokes the RAG chain, and updates the state with the generated answer.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rag/langgraph_adaptive_rag_local.ipynb#_snippet_14 LANGUAGE: JavaScript
CODE:
```
const generate = async (state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> => { console.log("---GENERATE---"); const generation = await ragChain.invoke({ context: formatDocs(state.documents), question: state.question, }); // Add generation to the state return { generation };
};
``` ---------------------------------------- TITLE: Streaming Events with LangGraphJS (JavaScript)
DESCRIPTION: This snippet shows how to call `streamEvents` on a LangGraphJS instance and iterate through the resulting event stream using `for await`. It includes logic to specifically identify and process a custom event named `my_custom_event`.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/streaming-content.ipynb#_snippet_5 LANGUAGE: JavaScript
CODE:
```
const eventStream = await graphWithDispatch.streamEvents( { messages: [{ role: "user", content: "What are you thinking about?", }] }, { version: "v2", }
); for await (const { event, name, data } of eventStream) { if (event === "on_custom_event" && name === "my_custom_event") { console.log(`${data.chunk}|`); }
}
``` ---------------------------------------- TITLE: Initialize LangGraphJS ReAct Agent with Human-in-the-Loop (JavaScript)
DESCRIPTION: Sets up the LangGraphJS ReAct agent using `createReactAgent`, configuring it with an OpenAI model, a custom weather tool, an in-memory checkpointer, and the `interruptBefore: ["tools"]` option to enable human intervention before tool execution.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/react-human-in-the-loop.ipynb#_snippet_2 LANGUAGE: javascript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { tool } from '@langchain/core/tools';
import { z } from 'zod';
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { MemorySaver } from "@langchain/langgraph"; const model = new ChatOpenAI({ model: "gpt-4o", }); const getWeather = tool((input) => { if (['sf', 'san francisco'].includes(input.location.toLowerCase())) { return 'It\'s always sunny in sf'; } else if (['nyc', 'new york city'].includes(input.location.toLowerCase())) { return 'It might be cloudy in nyc'; } else { throw new Error("Unknown Location"); }
}, { name: 'get_weather', description: 'Call to get the current weather in a given location.', schema: z.object({ location: z.string().describe("Location to get the weather for."), })
}) // Here we only save in-memory
const memory = new MemorySaver(); const agent = createReactAgent({ llm: model, tools: [getWeather], interruptBefore: ["tools"], checkpointSaver: memory }); ``` ---------------------------------------- TITLE: Handling LangGraph Recursion Limit (JS)
DESCRIPTION: Shows how to invoke a LangGraph with a recursion limit and catch the `GraphRecursionError` if the limit is reached, preventing the program from crashing.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/recursion-limit.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { GraphRecursionError } from "@langchain/langgraph"; try { await graph.invoke({ aggregate: [] }, { recursionLimit: 4 });
} catch (error) { if (error instanceof GraphRecursionError) { console.log("Recursion Error"); } else { throw error; }
}
``` ---------------------------------------- TITLE: Creating ReAct Execution Agent (TypeScript)
DESCRIPTION: Constructs the execution agent responsible for carrying out individual steps of the plan. It uses the `createReactAgent` prebuilt function with a specified OpenAI chat model (`gpt-4o`) and the previously defined list of tools.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/plan-and-execute/plan-and-execute.ipynb#_snippet_5 LANGUAGE: typescript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { createReactAgent } from "@langchain/langgraph/prebuilt"; const agentExecutor = createReactAgent({ llm: new ChatOpenAI({ model: "gpt-4o" }), tools: tools,
});
``` ---------------------------------------- TITLE: Define Simple LangGraph Tasks with Interrupt (TypeScript)
DESCRIPTION: Defines three tasks using LangGraph's `task` function. `step1` appends "bar", `humanFeedback` uses `interrupt` to pause and wait for user input before appending it, and `step3` appends "qux".
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/wait-user-input-functional.ipynb#_snippet_2 LANGUAGE: typescript
CODE:
```
import { task, interrupt } from "@langchain/langgraph"; const step1 = task("step1", async (inputQuery: string) => { return `${inputQuery} bar`;
}); const humanFeedback = task( "humanFeedback", async (inputQuery: string) => { const feedback = interrupt(`Please provide feedback: ${inputQuery}`); return `${inputQuery} ${feedback}`; }); const step3 = task("step3", async (inputQuery: string) => { return `${inputQuery} qux`;
});
``` ---------------------------------------- TITLE: Agent with Tool to Read from InMemoryStore
DESCRIPTION: This TypeScript code demonstrates how to integrate a data store (`InMemoryStore`) with a LangGraph agent. It shows how to initialize the store, populate it with data using `put`, define a tool (`getUserInfo`) that can access the store via the `config` object and retrieve data using `get`, and finally, how to pass the store to the agent during creation so the tool can utilize it. The example simulates looking up user information stored in memory.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/memory.md#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { initChatModel } from "langchain/chat_models/universal";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
// highlight-next-line
import { InMemoryStore } from "@langchain/langgraph-checkpoint";
// highlight-next-line
import { getStore } from "@langchain/langgraph";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const store = new InMemoryStore(); // (1)! await store.put( // (2)! ["users"], // (3)! "user_123", // (4)! { name: "John Smith", language: "English", } // (5)!
); // Look up user info tool
const getUserInfo = tool( async (input: Record<string, any>, config: LangGraphRunnableConfig): Promise<string> => { // Same as that provided to `createReactAgent` const store = config.store; // (6)! if (!store) { throw new Error("store is required when compiling the graph"); } const userId = config.configurable?.userId; if (!userId) { throw new Error("userId is required in the config"); } const userInfo = await store.get(["users"], userId); // (7)! return userInfo ? JSON.stringify(userInfo.value) : "Unknown user"; }, { name: "get_user_info", description: "Look up user info.", schema: z.object({}), }
); const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest");
const agent = createReactAgent({ llm, tools: [getUserInfo], store, // (8)!
}); // Run the agent
const response = await agent.invoke( { messages: [ { role: "user", content: "look up user information" } ] }, { configurable: { userId: "user_123" } }
);
``` ---------------------------------------- TITLE: Resuming LangGraph Stream by Updating State After NodeInterrupt (TypeScript)
DESCRIPTION: Shows how to resume a LangGraph stream after a `NodeInterrupt` by updating the graph state using `graph.updateState`. The state is modified (`input: "foo"`) so that the condition causing the interrupt is no longer met, allowing the graph execution to proceed past the previously interrupting node.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/breakpoints.md#_snippet_4 LANGUAGE: TypeScript
CODE:
```
// Update the state to pass the dynamic breakpoint
await graph.updateState({ input: "foo" }, threadConfig); for await (const event of await graph.stream(null, threadConfig)) { console.log(event);
}
``` ---------------------------------------- TITLE: Defining LangGraph Workflow Nodes (TypeScript)
DESCRIPTION: This code snippet initializes a LangGraph StateGraph with a defined state type and adds the core nodes that represent the steps in the agent's workflow: 'agent', 'retrieve', 'gradeDocuments', 'rewrite', and 'generate'.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rag/langgraph_agentic_rag.ipynb#_snippet_12 LANGUAGE: TypeScript
CODE:
```
import { StateGraph } from "@langchain/langgraph"; // Define the graph
const workflow = new StateGraph(GraphState) // Define the nodes which we'll cycle between. .addNode("agent", agent) .addNode("retrieve", toolNode) .addNode("gradeDocuments", gradeDocuments) .addNode("rewrite", rewrite) .addNode("generate", generate);
``` ---------------------------------------- TITLE: Initialize Memory Store with Semantic Search
DESCRIPTION: Creates an InMemoryStore instance configured with an index for semantic search. It uses OpenAIEmbeddings to generate vectors and specifies the embedding dimension.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/semantic-search.ipynb#_snippet_3 LANGUAGE: typescript
CODE:
```
import { OpenAIEmbeddings } from "@langchain/openai";
import { InMemoryStore } from "@langchain/langgraph"; const embeddings = new OpenAIEmbeddings({ model: "text-embedding-3-small",
}); const store = new InMemoryStore({ index: { embeddings, dims: 1536, }
});
``` ---------------------------------------- TITLE: Define Graph State - LangGraph/JS
DESCRIPTION: Defines the state structure for the LangGraph agent. It uses LangGraph's Annotation to specify a 'messages' field which is an array of BaseMessage, with a reducer to concatenate messages.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rag/langgraph_agentic_rag.ipynb#_snippet_3 LANGUAGE: JavaScript
CODE:
```
import { Annotation } from "@langchain/langgraph";
import { BaseMessage } from "@langchain/core/messages"; const GraphState = Annotation.Root({ messages: Annotation<BaseMessage[]> ({ reducer: (x, y) => x.concat(y), default: () => [], })
})
``` ---------------------------------------- TITLE: Handling Recursion Limit Error in LangGraph (TypeScript)
DESCRIPTION: Illustrates invoking a compiled LangGraph with a `recursionLimit` option in the config. It wraps the invocation in a try-catch block to specifically handle `GraphRecursionError` if the recursion limit is reached during execution.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/recursion-limit.ipynb#_snippet_1 LANGUAGE: ts
CODE:
```
import { GraphRecursionError } from "@langchain/langgraph"; try { await graph.invoke(inputs, { recursionLimit: 3 });
} catch (error) { if (error instanceof GraphRecursionError) { console.log("Recursion Error"); } else { throw error; }
}
``` ---------------------------------------- TITLE: Implementing Custom Tool Calling Node with Error Handling
DESCRIPTION: Provides an alternative approach to the default ToolNode by implementing a custom asynchronous function (`callTool2`) to execute tool calls. This custom node iterates through tool calls from the last AI message, invokes the corresponding tools, and explicitly handles potential errors during tool execution, returning ToolMessage objects.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/tool-calling-errors.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { AIMessage, ToolMessage, RemoveMessage, isAIMessage } from "@langchain/core/messages";
import { tool } from "@langchain/core/tools";
import { ChatAnthropic } from "@langchain/anthropic";
import { z } from "zod";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { MessagesAnnotation } from "@langchain/langgraph/prebuilt"; const haikuRequestSchema2 = z.object({ topic: z.array(z.string()).length(3),
}); const masterHaikuGenerator2 = tool(async ({ topic }) => { const model = new ChatAnthropic({ model: "claude-3-haiku-20240307", temperature: 0, }); const chain = model.pipe(new StringOutputParser()); const topics = topic.join(", "); const haiku = await chain.invoke(`Write a haiku about ${topics}`); return haiku;
}, { name: "master_haiku_generator", description: "Generates a haiku based on the provided topics.", schema: haikuRequestSchema2,
}); const callTool2 = async (state: typeof MessagesAnnotation.State) => { const { messages } = state; const toolsByName: Record<string, typeof masterHaikuGenerator2> = { master_haiku_generator: masterHaikuGenerator2 }; const lastMessage = messages[messages.length - 1] as AIMessage; const outputMessages: ToolMessage[] = []; for (const toolCall of lastMessage.tool_calls) { try { const toolResult = await toolsByName[toolCall.name].invoke(toolCall); outputMessages.push(toolResult); } catch (error: any) { // Return the error if the tool call fails outputMessages.push( new ToolMessage({ content: error.message, name: toolCall.name, tool_call_id: toolCall.id!, additional_kwargs: { error } }) ); } } return { messages: outputMessages };
}; const model = new ChatAnthropic({ model: "claude-3-haiku-20240307", temperature: 0,
});
const modelWithTools2 = model.bindTools([masterHaikuGenerator2]); const betterModel = new ChatAnthropic({ model: "claude-3-5-sonnet-20240620", temperature: 0,
});
const betterModelWithTools = betterModel.bindTools([masterHaikuGenerator2]); const shouldContinue2 = async (state: typeof MessagesAnnotation.State) => { const { messages } = state; const lastMessage = messages[messages.length - 1]; if (isAIMessage(lastMessage) && lastMessage.tool_calls?.length) { return "tools"; } return "__end__";
}
``` ---------------------------------------- TITLE: Helper Functions for LangGraph Agent Teams - TypeScript
DESCRIPTION: This code block defines three utility functions: `agentStateModifier` which prepares the state for an agent node by adding system messages; `runAgentNode` which executes an agent with the current state and formats the output; and `createTeamSupervisor` which builds a runnable that acts as a supervisor, using an LLM to decide the next agent to route to or if the process should finish.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { z } from "zod";
import { HumanMessage, BaseMessage, SystemMessage } from "@langchain/core/messages";
import { ChatPromptTemplate, MessagesPlaceholder,
} from "@langchain/core/prompts";
import { JsonOutputToolsParser } from "langchain/output_parsers";
import { ChatOpenAI } from "@langchain/openai";
import { Runnable } from "@langchain/core/runnables";
import { StructuredToolInterface } from "@langchain/core/tools";
import { MessagesAnnotation } from "@langchain/langgraph"; const agentStateModifier = ( systemPrompt: string, tools: StructuredToolInterface[], teamMembers: string[],
): ((state: typeof MessagesAnnotation.State) => BaseMessage[]) => { const toolNames = tools.map((t) => t.name).join(", "); const systemMsgStart = new SystemMessage(systemPrompt + "\nWork autonomously according to your specialty, using the tools available to you." + " Do not ask for clarification." + " Your other team members (and other teams) will collaborate with you with their own specialties." + ` You are chosen for a reason! You are one of the following team members: ${teamMembers.join(", ")}.`) const systemMsgEnd = new SystemMessage(`Supervisor instructions: ${systemPrompt}\n` + `Remember, you individually can only use these tools: ${toolNames}` + "\n\nEnd if you have already completed the requested task. Communicate the work completed."); return (state: typeof MessagesAnnotation.State): any[] => [systemMsgStart, ...state.messages, systemMsgEnd];
} async function runAgentNode(params: { state: any; agent: Runnable; name: string;
}) { const { state, agent, name } = params; const result = await agent.invoke({ messages: state.messages, }); const lastMessage = result.messages[result.messages.length - 1]; return { messages: [new HumanMessage({ content: lastMessage.content, name })], };
} async function createTeamSupervisor( llm: ChatOpenAI, systemPrompt: string, members: string[],
): Promise<Runnable> { const options = ["FINISH", ...members]; const routeTool = { name: "route", description: "Select the next role.", schema: z.object({ reasoning: z.string(), next: z.enum(["FINISH", ...members]), instructions: z.string().describe("The specific instructions of the sub-task the next role should accomplish."), }) } let prompt = ChatPromptTemplate.fromMessages([ ["system", systemPrompt], new MessagesPlaceholder("messages"), [ "system", "Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}", ], ]); prompt = await prompt.partial({ options: options.join(", "), team_members: members.join(", "), }); const supervisor = prompt .pipe( llm.bindTools([routeTool], { tool_choice: "route", }), ) .pipe(new JsonOutputToolsParser()) // select the first one .pipe((x) => ({ next: x[0].args.next, instructions: x[0].args.instructions, })); return supervisor;
}
``` ---------------------------------------- TITLE: Defining a LangGraph with Dynamic Breakpoint (TypeScript)
DESCRIPTION: This snippet defines a simple LangGraph with three nodes (`step1`, `step2`, `step3`). It demonstrates how to use `NodeInterrupt` within `step2` to dynamically pause the graph's execution if the input string's length exceeds 5 characters. The graph is compiled with a `MemorySaver` checkpointer.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/dynamic_breakpoints.ipynb#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { Annotation, MemorySaver, NodeInterrupt, StateGraph,
} from "@langchain/langgraph"; const StateAnnotation = Annotation.Root({ input: Annotation<string>,
}); const step1 = async (state: typeof StateAnnotation.State) => { console.log("---Step 1---"); return state;
}; const step2 = async (state: typeof StateAnnotation.State) => { // Let's optionally raise a NodeInterrupt // if the length of the input is longer than 5 characters if (state.input?.length > 5) { throw new NodeInterrupt(`Received input that is longer than 5 characters: ${state.input}`); } console.log("---Step 2---"); return state;
}; const step3 = async (state: typeof StateAnnotation.State) => { console.log("---Step 3---"); return state;
}; const checkpointer = new MemorySaver(); const graph = new StateGraph(StateAnnotation) .addNode("step1", step1) .addNode("step2", step2) .addNode("step3", step3) .addEdge("__start__", "step1") .addEdge("step1", "step2") .addEdge("step2", "step3") .addEdge("step3", "__end__") .compile({ checkpointer });
``` ---------------------------------------- TITLE: Placing Side Effects After Interrupt in LangGraphJS (OK)
DESCRIPTION: This example shows the recommended pattern of placing side effects (apiCall) after the `interrupt` function. This ensures that the side effect is only executed once after the node has been successfully resumed and the interrupt has completed.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/human_in_the_loop.md#_snippet_11 LANGUAGE: TypeScript
CODE:
```
import { interrupt } from "@langchain/langgraph"; function humanNode(state: typeof GraphAnnotation.State) { /** * Human node with validation. */ const answer = interrupt(question); apiCall(answer); // OK as it's after the interrupt
}
``` ---------------------------------------- TITLE: Define and Compile LangGraph Workflow - LangGraph JS
DESCRIPTION: Initializes a `StateGraph` with the `AgentState`. Adds the "agent" node (`callModel`) and "tools" node (`toolNode`). Defines edges: starts at "agent", conditionally moves from "agent" based on `shouldContinue`, and always returns from "tools" to "agent". Finally, compiles the workflow into a runnable application.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/managing-agent-steps.ipynb#_snippet_9 LANGUAGE: TypeScript
CODE:
```
import { START, StateGraph } from "@langchain/langgraph"; // Define a new graph
const workflow = new StateGraph(AgentState) .addNode("agent", callModel) .addNode("tools", toolNode) .addEdge(START, "agent") .addConditionalEdges( "agent", shouldContinue, ) .addEdge("tools", "agent"); // Finally, we compile it!
// This compiles it into a LangChain Runnable,
// meaning you can use it as you would any other runnable
const app = workflow.compile();
``` ---------------------------------------- TITLE: Define LangGraph Structure (JavaScript)
DESCRIPTION: Sets up the state graph using `MessagesAnnotation`, defines nodes for calling the main agent model, executing tools, and calling the final model. It establishes the flow with conditional edges based on the agent's response and connects the tool execution back to the agent.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/streaming-from-final-node.ipynb#_snippet_2 LANGUAGE: javascript
CODE:
```
import { StateGraph, MessagesAnnotation } from "@langchain/langgraph";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import { AIMessage, HumanMessage, SystemMessage } from "@langchain/core/messages"; const shouldContinue = async (state: typeof MessagesAnnotation.State) => { const messages = state.messages; const lastMessage: AIMessage = messages[messages.length - 1]; // If the LLM makes a tool call, then we route to the "tools" node if (lastMessage.tool_calls?.length) { return "tools"; } // Otherwise, we stop (reply to the user) return "final";
}; const callModel = async (state: typeof MessagesAnnotation.State) => { const messages = state.messages; const response = await model.invoke(messages); // We return a list, because this will get added to the existing list return { messages: [response] };
}; const callFinalModel = async (state: typeof MessagesAnnotation.State) => { const messages = state.messages; const lastAIMessage = messages[messages.length - 1]; const response = await finalModel.invoke([ new SystemMessage("Rewrite this in the voice of Al Roker"), new HumanMessage({ content: lastAIMessage.content }) ]); // MessagesAnnotation allows you to overwrite messages from the agent // by returning a message with the same id response.id = lastAIMessage.id; return { messages: [response] };
} const toolNode = new ToolNode<typeof MessagesAnnotation.State>(tools); const graph = new StateGraph(MessagesAnnotation) .addNode("agent", callModel) .addNode("tools", toolNode) // add a separate final node .addNode("final", callFinalModel) .addEdge("__start__", "agent") // Third parameter is optional and only here to draw a diagram of the graph .addConditionalEdges("agent", shouldContinue, { tools: "tools", final: "final", }) .addEdge("tools", "agent") .addEdge("final", "__end__") .compile();
``` ---------------------------------------- TITLE: Updating Agent State from a Tool in LangGraph (TypeScript)
DESCRIPTION: This snippet illustrates how a LangChain tool can modify the agent's state during execution. The tool returns a Command object with an update property. This update includes setting a userName field in the custom state and adding a ToolMessage to the message history, demonstrating how tools can persist results or provide feedback.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/context.md#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { Annotation, MessagesAnnotation, LangGraphRunnableConfig, Command } from "@langchain/langgraph";
import { tool } from "@langchain/core/tools";
import { z } from "zod";
import { ToolMessage } from "@langchain/core/messages";
import { initChatModel } from "langchain/chat_models/universal";
import { createReactAgent } from "@langchain/langgraph/prebuilt"; const CustomState = Annotation.Root({ ...MessagesAnnotation.spec, // highlight-next-line userName: Annotation<string>(), // Will be updated by the tool
}); const getUserInfo = tool( async ( _input: Record<string, never>, config: LangGraphRunnableConfig ): Promise<Command> => { const userId = config.configurable?.userId; if (!userId) { throw new Error("Please provide a user id in config.configurable"); } const toolCallId = config.toolCall?.id; const name = userId === "user_123" ? "John Smith" : "Unknown user"; // Return command to update state return new Command({ update: { // highlight-next-line userName: name, // Update the message history // highlight-next-line messages: [ new ToolMessage({ content: "Successfully looked up user information", tool_call_id: toolCallId, }), ], }, }); }, { name: "get_user_info", description: "Look up user information.", schema: z.object({}), }
); const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest");
const agent = createReactAgent({ llm, tools: [getUserInfo], // highlight-next-line stateSchema: CustomState,
}); await agent.invoke( { messages: "look up user information" }, // highlight-next-line { configurable: { userId: "user_123" } }
);
``` ---------------------------------------- TITLE: Defining Network Agent Graph with LangGraph.js (TypeScript)
DESCRIPTION: This snippet demonstrates how to define a multi-agent network architecture using LangGraph.js. Agents are represented as nodes that can transition to any other agent node or terminate execution based on their internal logic, typically driven by an LLM's output. It uses `StateGraph` and `Command` for state updates and routing.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/multi_agent.md#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { StateGraph, Annotation, MessagesAnnotation, Command
} from "@langchain/langgraph";
import { ChatOpenAI } from "@langchain/openai"; const model = new ChatOpenAI({ model: "gpt-4o-mini",
}); const agent1 = async (state: typeof MessagesAnnotation.State) => { // you can pass relevant parts of the state to the LLM (e.g., state.messages) // to determine which agent to call next. a common pattern is to call the model // with a structured output (e.g. force it to return an output with a "next_agent" field) const response = await model.withStructuredOutput(...).invoke(...); return new Command({ update: { messages: [response.content], }, goto: response.next_agent, });
}; const agent2 = async (state: typeof MessagesAnnotation.State) => { const response = await model.withStructuredOutput(...).invoke(...); return new Command({ update: { messages: [response.content], }, goto: response.next_agent, });
}; const agent3 = async (state: typeof MessagesAnnotation.State) => { ... return new Command({ update: { messages: [response.content], }, goto: response.next_agent, });
}; const graph = new StateGraph(MessagesAnnotation) .addNode("agent1", agent1, { ends: ["agent2", "agent3" "__end__"], }) .addNode("agent2", agent2, { ends: ["agent1", "agent3", "__end__"], }) .addNode("agent3", agent3, { ends: ["agent1", "agent2", "__end__"], }) .addEdge("__start__", "agent1") .compile();
``` ---------------------------------------- TITLE: Defining ReAct Agent Workflow with LangGraph (TypeScript)
DESCRIPTION: This snippet defines the core LangGraph workflow for a ReAct agent. It sets up nodes for calling the language model (`agent`) and executing tools (`tools`), and defines the conditional logic (`shouldContinue`) to transition between them based on the model's response. The graph starts by calling the agent and cycles between the agent and tools until the agent's response does not contain tool calls.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/tool-calling.ipynb#_snippet_9 LANGUAGE: TypeScript
CODE:
```
import { StateGraph, MessagesAnnotation, END, START
} from "@langchain/langgraph"; const toolNodeForGraph = new ToolNode(tools) const shouldContinue = (state: typeof MessagesAnnotation.State) => { const { messages } = state; const lastMessage = messages[messages.length - 1]; if ("tool_calls" in lastMessage && Array.isArray(lastMessage.tool_calls) && lastMessage.tool_calls?.length) { return "tools"; } return END;
} const callModel = async (state: typeof MessagesAnnotation.State) => { const { messages } = state; const response = await modelWithTools.invoke(messages); return { messages: response };
} const workflow = new StateGraph(MessagesAnnotation) // Define the two nodes we will cycle between .addNode("agent", callModel) .addNode("tools", toolNodeForGraph) .addEdge(START, "agent") .addConditionalEdges("agent", shouldContinue, ["tools", END]) .addEdge("tools", "agent"); const app = workflow.compile()
``` ---------------------------------------- TITLE: Reviewing Tool Calls with Interrupt (TypeScript)
DESCRIPTION: This TypeScript function demonstrates how to use the `interrupt` function to pause the application flow and allow a human to review a generated tool call. It handles different human actions like continuing, updating arguments, or providing feedback, returning the original tool call, an updated tool call, or a tool message based on the action.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/review-tool-calls-functional.ipynb#_snippet_0 LANGUAGE: typescript
CODE:
```
function reviewToolCall(toolCall: ToolCall): ToolCall | ToolMessage { // Interrupt for human review const humanReview = interrupt({ question: "Is this correct?", tool_call: toolCall, }); const { action, data } = humanReview; if (action === "continue") { return toolCall; } else if (action === "update") { return { ...toolCall, args: data, }; } else if (action === "feedback") { return new ToolMessage({ content: data, name: toolCall.name, tool_call_id: toolCall.id, }); } throw new Error(`Unsupported review action: ${action}`);
}
``` ---------------------------------------- TITLE: Resume LangGraph Stream with Feedback Command - TypeScript
DESCRIPTION: Creates a `Command` object with the `resume` action set to `feedback` and provides a string message ("Please format as <City>, <State>.") to guide the agent on how to reformat a tool call, then streams the result.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/review-tool-calls-functional.ipynb#_snippet_13 LANGUAGE: TypeScript
CODE:
```
// highlight-next-line
const humanInput3 = new Command({ // highlight-next-line resume: { // highlight-next-line action: "feedback", // highlight-next-line data: "Please format as <City>, <State>.", // highlight-next-line }, // highlight-next-line
}); const resumedStream3 = await agent.stream(humanInput3, config3) for await (const step of resumedStream3) { printStep(step);
}
``` ---------------------------------------- TITLE: Define and Test RAG Generation Chain (JavaScript)
DESCRIPTION: Pulls a RAG prompt from LangChain Hub, defines a helper function to format documents, initializes an Ollama LLM (without JSON mode), creates an LCEL chain for RAG generation, and invokes it with formatted documents and a question.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rag/langgraph_adaptive_rag_local.ipynb#_snippet_7 LANGUAGE: JavaScript
CODE:
```
import * as hub from "langchain/hub";
import { StringOutputParser } from "@langchain/core/output_parsers";
import type { Document } from "@langchain/core/documents"; // https://smith.langchain.com/hub/rlm/rag-prompt
const ragPrompt = await hub.pull("rlm/rag-prompt"); // Post-processing
const formatDocs = (docs: Document[]) => { return docs.map((doc) => doc.pageContent).join("\n\n");
}; // Initialize a new model without JSON mode active
const llm = new ChatOllama({ model: "llama3", temperature: 0,
}); // Chain
const ragChain = ragPrompt.pipe(llm).pipe(new StringOutputParser()); // Test run
const testQuestion2 = "agent memory";
const docs3 = await retriever.invoke(testQuestion2); await ragChain.invoke({ context: formatDocs(docs3), question: testQuestion2 });
``` ---------------------------------------- TITLE: Implementing User-Scoped Authorization with LangGraph JS/TS SDK
DESCRIPTION: This snippet shows how to combine authentication with a simple authorization handler using `Auth().on("*", ...)`. The handler adds the authenticated user's identity to the resource metadata during creation and returns filters to restrict read/search/update operations to resources owned by that user.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/auth.md#_snippet_1 LANGUAGE: typescript
CODE:
```
import { Auth, HTTPException } from "@langchain/langgraph-sdk/auth"; export const auth = new Auth() .authenticate(async (request: Request) => ({ identity: "user-123", permissions: [], })) .on("*", ({ value, user }) => { // Create filter to restrict access to just this user's resources const filters = { owner: user.identity }; // If the operation supports metadata, add the user identity // as metadata to the resource. if ("metadata" in value) { value.metadata ??= {}; value.metadata.owner = user.identity; } // Return filters to restrict access // These filters are applied to ALL operations (create, read, update, search, etc.) // to ensure users can only access their own resources return filters; });
``` ---------------------------------------- TITLE: Defining LangGraph State Schema and User Lookup Tool (TypeScript)
DESCRIPTION: Defines the `StateAnnotation` schema for the LangGraph state, including message history, an optional `lastName`, and `userInfo`. It also defines the `lookupUserInfo` tool using `@langchain/core/tools`, which takes a user ID from the configuration, retrieves user data from `USER_ID_TO_USER_INFO`, and returns a `Command` to update the graph state with the user info and add a tool message.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/update-state-from-tools.ipynb#_snippet_4 LANGUAGE: TypeScript
CODE:
```
import { Annotation, Command, MessagesAnnotation } from "@langchain/langgraph";
import { tool } from "@langchain/core/tools"; import { z } from "zod"; const StateAnnotation = Annotation.Root({ ...MessagesAnnotation.spec, // user provided lastName: Annotation<string>, // updated by the tool userInfo: Annotation<Record<string, any>>,
}); const lookupUserInfo = tool(async (_, config) => { const userId = config.configurable?.user_id; if (userId === undefined) { throw new Error("Please provide a user id in config.configurable"); } if (USER_ID_TO_USER_INFO[userId] === undefined) { throw new Error(`User "${userId}" not found`); } // Populated when a tool is called with a tool call from a model as input const toolCallId = config.toolCall.id; return new Command({ update: { // update the state keys userInfo: USER_ID_TO_USER_INFO[userId], // update the message history messages: [ { role: "tool", content: "Successfully looked up user information", tool_call_id: toolCallId, }, ], }, })
}, { name: "lookup_user_info", description: "Always use this to look up information about the user to better assist them with their questions.", schema: z.object({}),
});
``` ---------------------------------------- TITLE: Define Node Returning Command (goto Parent)
DESCRIPTION: Shows how a node within a subgraph can use Command with 'graph: Command.PARENT' to navigate to a node in the parent graph, combining state updates and cross-graph control flow.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/command.ipynb#_snippet_1 LANGUAGE: typescript
CODE:
```
const myNode = (state: typeof StateAnnotation.State) => { return new Command({ update: { foo: "bar" }, goto: "other_subgraph", // where `other_subgraph` is a node in the parent graph graph: Command.PARENT });
};
``` ---------------------------------------- TITLE: Adding Conditional Edges from Initial Support Node (TypeScript)
DESCRIPTION: Configures the graph builder to add conditional edges from the "initial_support" node. It checks the `state.nextRepresentative` property to route execution to "billing_support", "technical_support", or "__end__" based on whether it includes "BILLING", "TECHNICAL", or neither.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/chatbots/customer_support_small_model.ipynb#_snippet_9 LANGUAGE: typescript
CODE:
```
builder = builder.addConditionalEdges("initial_support", async (state: typeof StateAnnotation.State) => { if (state.nextRepresentative.includes("BILLING")) { return "billing"; } else if (state.nextRepresentative.includes("TECHNICAL")) { return "technical"; } else { return "conversational"; }
}, { billing: "billing_support", technical: "technical_support", conversational: "__end__",
}); console.log("Added edges!");
``` ---------------------------------------- TITLE: Handling Refunds with Human Authorization in LangGraph
DESCRIPTION: Implements a LangGraph node for handling refunds. It checks if a refund is authorized in the state. If not authorized, it throws a NodeInterrupt to signal that human intervention is required before proceeding. If authorized, it returns a success message.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/chatbots/customer_support_small_model.ipynb#_snippet_7 LANGUAGE: JavaScript
CODE:
```
import { NodeInterrupt } from "@langchain/langgraph"; const handleRefund = async (state: typeof StateAnnotation.State) => { if (!state.refundAuthorized) { console.log("--- HUMAN AUTHORIZATION REQUIRED FOR REFUND ---"); throw new NodeInterrupt("Human authorization required.") } return { messages: { role: "assistant", content: "Refund processed!", }, };
};
``` ---------------------------------------- TITLE: Define and Compile Subgraph and Parent Graph
DESCRIPTION: This snippet demonstrates how to define state annotations for both a subgraph and a parent graph, create nodes for each, build the graphs using StateGraph, and compile them. It highlights how state keys can be shared between the parent and subgraph.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/subgraph.ipynb#_snippet_1 LANGUAGE: JavaScript
CODE:
```
import { StateGraph, Annotation } from "@langchain/langgraph"; const SubgraphStateAnnotation = Annotation.Root({ foo: Annotation<string>, // note that this key is shared with the parent graph state bar: Annotation<string>,
}); const subgraphNode1 = async (state: typeof SubgraphStateAnnotation.State) => { return { bar: "bar" };
}; const subgraphNode2 = async (state: typeof SubgraphStateAnnotation.State) => { // note that this node is using a state key ('bar') that is only available in the subgraph // and is sending update on the shared state key ('foo') return { foo: state.foo + state.bar };
}; const subgraphBuilder = new StateGraph(SubgraphStateAnnotation) .addNode("subgraphNode1", subgraphNode1) .addNode("subgraphNode2", subgraphNode2) .addEdge("__start__", "subgraphNode1") .addEdge("subgraphNode1", "subgraphNode2"); const subgraph = subgraphBuilder.compile(); // Define parent graph
const ParentStateAnnotation = Annotation.Root({ foo: Annotation<string>,
}); const node1 = async (state: typeof ParentStateAnnotation.State) => { return { foo: "hi! " + state.foo, };
}; const builder = new StateGraph(ParentStateAnnotation) .addNode("node1", node1) // note that we're adding the compiled subgraph as a node to the parent graph .addNode("node2", subgraph) .addEdge("__start__", "node1") .addEdge("node1", "node2"); const graph = builder.compile();
``` ---------------------------------------- TITLE: Calling Agent Model with Tools (TypeScript)
DESCRIPTION: This function acts as the agent node in the graph. It filters messages to remove the relevance score tool call, initializes a ChatOpenAI model bound with tools, invokes the model with the filtered messages, and returns the model's response.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rag/langgraph_agentic_rag.ipynb#_snippet_9 LANGUAGE: TypeScript
CODE:
``` const filteredMessages = messages.filter((message) => { if ("tool_calls" in message && Array.isArray(message.tool_calls) && message.tool_calls.length > 0) { return message.tool_calls[0].name !== "give_relevance_score"; } return true; }); const model = new ChatOpenAI({ model: "gpt-4o", temperature: 0, streaming: true, }).bindTools(tools); const response = await model.invoke(filteredMessages); return { messages: [response], };
``` ---------------------------------------- TITLE: Defining and Integrating a Compiled Subgraph with Shared State in LangGraph (TypeScript)
DESCRIPTION: This comprehensive example demonstrates defining both a parent graph and a subgraph with shared state keys (`foo`). It shows how the subgraph processes its state and returns values that can be consumed by the parent graph, highlighting the importance of shared state for direct integration of compiled subgraphs.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/low_level.md#_snippet_30 LANGUAGE: TypeScript
CODE:
```
import { StateGraph, Annotation } from "@langchain/langgraph"; const StateAnnotation = Annotation.Root({ foo: Annotation<string>,
}); const SubgraphStateAnnotation = Annotation.Root({ foo: Annotation<string>, // note that this key is shared with the parent graph state bar: Annotation<string>,
}); // Define subgraph
const subgraphNode = async (state: typeof SubgraphStateAnnotation.State) => { // note that this subgraph node can communicate with // the parent graph via the shared "foo" key return { foo: state.foo + "bar" };
}; const subgraph = new StateGraph(SubgraphStateAnnotation) .addNode("subgraph", subgraphNode); ... .compile(); // Define parent graph
const parentGraph = new StateGraph(StateAnnotation) .addNode("subgraph", subgraph) ... .compile();
``` ---------------------------------------- TITLE: Streaming LLM Tokens with LangGraph (TypeScript)
DESCRIPTION: Shows how to stream tokens directly from the language model during LangGraph execution using the `stream` method with `streamMode: "messages"`. It iterates over streamed tokens and associated metadata.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/streaming.md#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { initChatModel } from "langchain/chat_models/universal"; const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest");
const agent = createReactAgent({ llm, tools: [getWeather],
});
// highlight-next-line
for await (const [token, metadata] of await agent.stream( { messages: "what is the weather in sf" }, // highlight-next-line { streamMode: "messages" }
)) { console.log("Token", token); console.log("Metadata", metadata); console.log("\n");
}
``` ---------------------------------------- TITLE: Adding In-Memory Checkpointer to LangGraph.js (TypeScript)
DESCRIPTION: Introduces conversational memory by initializing a `MemorySaver` checkpointer. It then recompiles the previously defined workflow, passing the `memory` object to the `compile` method to enable state persistence.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/persistence.ipynb#_snippet_10 LANGUAGE: TypeScript
CODE:
```
import { MemorySaver } from "@langchain/langgraph"; // Here we only save in-memory
const memory = new MemorySaver();
const persistentGraph = workflow.compile({ checkpointer: memory });
``` ---------------------------------------- TITLE: Bind Tools to Model
DESCRIPTION: Binds the previously defined tools to the chat model using the `bindTools` method. This makes the model aware of the available tools and allows it to generate tool call requests.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/stream-values.ipynb#_snippet_5 LANGUAGE: TypeScript
CODE:
```
const boundModel = model.bindTools(tools);
``` ---------------------------------------- TITLE: Bind Tools to Model (JS)
DESCRIPTION: Binds a list of available tools to the initialized chat model. This allows the model to understand and suggest calling these tools during its response generation.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/time-travel.ipynb#_snippet_5 LANGUAGE: JavaScript
CODE:
```
const boundModel = model.bindTools(tools);
``` ---------------------------------------- TITLE: Implementing a Basic Supervisor Workflow (TypeScript)
DESCRIPTION: Demonstrates creating and running a multi-agent supervisor using `@langchain/langgraph-supervisor`. It defines specialized agents with tools, configures the supervisor to route tasks based on prompts, compiles the workflow, and invokes it with a user message. This example showcases the core functionality of orchestrating agents with a supervisor.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/libs/langgraph-supervisor/README.md#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { createSupervisor } from "@langchain/langgraph-supervisor";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const model = new ChatOpenAI({ modelName: "gpt-4o" }); // Create specialized agents
const add = tool( async (args) => args.a + args.b, { name: "add", description: "Add two numbers.", schema: z.object({ a: z.number(), b: z.number() }) }
); const multiply = tool( async (args) => args.a * args.b, { name: "multiply", description: "Multiply two numbers.", schema: z.object({ a: z.number(), b: z.number() }) }
); const webSearch = tool( async (args) => { return ( "Here are the headcounts for each of the FAANG companies in 2024:\n" + "1. **Facebook (Meta)**: 67,317 employees.\n" + "2. **Apple**: 164,000 employees.\n" + "3. **Amazon**: 1,551,000 employees.\n" + "4. **Netflix**: 14,000 employees.\n" + "5. **Google (Alphabet)**: 181,269 employees." ); }, { name: "web_search", description: "Search the web for information.", schema: z.object({ query: z.string() }) }
); const mathAgent = createReactAgent({ llm: model, tools: [add, multiply], name: "math_expert", prompt: "You are a math expert. Always use one tool at a time."
}); const researchAgent = createReactAgent({ llm: model, tools: [webSearch], name: "research_expert", prompt: "You are a world class researcher with access to web search. Do not do any math."
}); // Create supervisor workflow
const workflow = createSupervisor({ agents: [researchAgent, mathAgent], llm: model, prompt: "You are a team supervisor managing a research expert and a math expert. " + "For current events, use research_agent. " + "For math problems, use math_agent."
}); // Compile and run
const app = workflow.compile();
const result = await app.invoke({ messages: [ { role: "user", content: "what's the combined headcount of the FAANG companies in 2024??" } ]
});
``` ---------------------------------------- TITLE: Defining OpenAI Model and Weather Tool (Langchain.js)
DESCRIPTION: Initializes a ChatOpenAI model and defines a placeholder 'getWeather' tool using '@langchain/core/tools'. The tool simulates weather lookup based on location input and uses Zod for schema validation, returning a simple weather description.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/wait-user-input-functional.ipynb#_snippet_6 LANGUAGE: javascript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const model = new ChatOpenAI({ model: "gpt-4o-mini",
}); const getWeather = tool(async ({ location }) => { // This is a placeholder for the actual implementation const lowercaseLocation = location.toLowerCase(); if (lowercaseLocation.includes("sf") || lowercaseLocation.includes("san francisco")) { return "It's sunny!"; } else if (lowercaseLocation.includes("boston")) { return "It's rainy!"; } else { return `I am not sure what the weather is in ${location}`; }
}, { name: "getWeather", schema: z.object({ location: z.string().describe("Location to get the weather for"), }), description: "Call to get the weather from a specific location.",
});
``` ---------------------------------------- TITLE: Stream Results from Parent Graph
DESCRIPTION: This snippet shows how to invoke the compiled parent graph with an initial state and iterate over the streamed output chunks using a for await...of loop.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/subgraph.ipynb#_snippet_2 LANGUAGE: JavaScript
CODE:
```
const stream = await graph.stream({ foo: "foo" }); for await (const chunk of stream) { console.log(chunk);
}
``` ---------------------------------------- TITLE: Annotating LangGraph State with `messagesStateReducer` (TypeScript)
DESCRIPTION: This snippet demonstrates how to annotate a `messages` key in the LangGraph state to use the prebuilt `messagesStateReducer`. This reducer is crucial for managing conversational history, as it handles appending new messages and updating existing ones based on message IDs, while also deserializing messages into LangChain `BaseMessage` objects upon state updates.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/low_level.md#_snippet_5 LANGUAGE: typescript
CODE:
```
import type { BaseMessage } from "@langchain/core/messages";
import { Annotation, type Messages } from "@langchain/langgraph"; const StateAnnotation = Annotation.Root({ messages: Annotation<BaseMessage[], Messages>({ reducer: messagesStateReducer, }),
});
``` ---------------------------------------- TITLE: Defining Graph State with Annotation.Root for Messages
DESCRIPTION: This snippet defines a `GraphAnnotation` using `Annotation.Root` to create a top-level state object. It includes a `messages` channel configured to store an array of `BaseMessage` objects, with a `reducer` to concatenate new messages and a `default` function to initialize the channel as an empty array.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/define-state.ipynb#_snippet_1 LANGUAGE: typescript
CODE:
```
import { BaseMessage } from "@langchain/core/messages";
import { Annotation } from "@langchain/langgraph"; const GraphAnnotation = Annotation.Root({ // Define a 'messages' channel to store an array of BaseMessage objects messages: Annotation<BaseMessage[]>({ // Reducer function: Combines the current state with new messages reducer: (currentState, updateValue) => currentState.concat(updateValue), // Default function: Initialize the channel with an empty array default: () => [], })
});
``` ---------------------------------------- TITLE: Defining LangGraph Workflow (JavaScript)
DESCRIPTION: Imports necessary components from `@langchain/langgraph` and defines the workflow structure using `StateGraph`. It adds nodes for planning, tool execution, and solving, defines static edges, and uses the `_route` function to conditionally route from the tool execution node back to itself or to the solver node. Finally, it compiles the workflow with a `MemorySaver` checkpointer. Requires `StateGraph`, `MemorySaver`, `GraphState`, `getPlan`, `toolExecution`, and `solve` functions/types.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rewoo/rewoo.ipynb#_snippet_9 LANGUAGE: JavaScript
CODE:
```
import { END, START, StateGraph } from "@langchain/langgraph";
import { MemorySaver } from "@langchain/langgraph"; const _route = (state: typeof GraphState.State) => { console.log("---ROUTE TASK---"); const _step = _getCurrentTask(state); if (_step === null) { // We have executed all tasks return "solve"; } // We are still executing tasks, loop back to the "tool" node return "tool";
}; const workflow = new StateGraph(GraphState) .addNode("plan", getPlan) .addNode("tool", toolExecution) .addNode("solve", solve) .addEdge("plan", "tool") .addEdge("solve", END) .addConditionalEdges("tool", _route) .addEdge(START, "plan"); // Compile
const app = workflow.compile({ checkpointer: new MemorySaver() });
``` ---------------------------------------- TITLE: Invoking LangGraph for Research Query (JS/TS)
DESCRIPTION: This snippet demonstrates how to invoke the compiled graph using the `stream` method with an initial `HumanMessage` asking about popular TV shows. It sets a recursion limit and then asynchronously iterates through the streamed output, printing each step's result to the console until the `__end__` marker is encountered.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/multi_agent/agent_supervisor.ipynb#_snippet_8 LANGUAGE: javascript
CODE:
```
let streamResults = graph.stream( { messages: [ new HumanMessage({ content: "What were the 3 most popular tv shows in 2023?", }), ], }, { recursionLimit: 100 },
); for await (const output of await streamResults) { if (!output?.__end__) { console.log(output); console.log("----"); }
}
``` ---------------------------------------- TITLE: Initializing LangGraphJS ReAct Agent with Memory and Tool
DESCRIPTION: Imports necessary classes and functions from LangChain and LangGraph, defines a ChatOpenAI model and a simple weather tool, creates an in-memory checkpoint saver, and initializes the ReAct agent using the model, tool, and the memory saver.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/react-memory.ipynb#_snippet_2 LANGUAGE: javascript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { tool } from '@langchain/core/tools';
import { z } from 'zod';
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { MemorySaver } from "@langchain/langgraph"; const model = new ChatOpenAI({ model: "gpt-4o", }); const getWeather = tool((input) => { if (input.location === 'sf') { return 'It\'s always sunny in sf'; } else { return 'It might be cloudy in nyc'; }
}, { name: 'get_weather', description: 'Call to get the current weather.', schema: z.object({ location: z.enum(['sf','nyc']).describe("Location to get the weather for."), })
}) // Here we only save in-memory
const memory = new MemorySaver(); const agent = createReactAgent({ llm: model, tools: [getWeather], checkpointSaver: memory });
``` ---------------------------------------- TITLE: Stream Custom Data using .stream (TypeScript)
DESCRIPTION: Demonstrates how to invoke the compiled graph using the `.stream` method with `streamMode: "custom"`. This configuration allows iterating over the custom chunks written by the node function.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/streaming-content.ipynb#_snippet_2 LANGUAGE: typescript
CODE:
```
const inputs = [{ role: "user", content: "What are you thinking about?",
}]; const stream = await graph.stream( { messages: inputs }, { streamMode: "custom" }
); for await (const chunk of stream) { console.log(chunk);
}
``` ---------------------------------------- TITLE: Defining LangGraph State Schema (JavaScript)
DESCRIPTION: Defines the schema for the shared graph state using LangGraph's Annotation.Root. It includes fields for the task, plan string, execution steps, results from tools, and the final result, specifying their types, reducers, and default values.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rewoo/rewoo.ipynb#_snippet_2 LANGUAGE: JavaScript
CODE:
```
import { Annotation } from "@langchain/langgraph"; const GraphState = Annotation.Root({ task: Annotation<string>({ reducer: (x, y) => (y ?? x), default: () => "", }), planString: Annotation<string>({ reducer: (x, y) => (y ?? x), default: () => "", }), steps: Annotation<string[][]>({ reducer: (x, y) => x.concat(y), default: () => [], }), results: Annotation<Record<string, any>>({ reducer: (x, y) => ({ ...x, ...y }), default: () => ({}), }), result: Annotation<string>({ reducer: (x, y) => (y ?? x), default: () => "", }),
})
``` ---------------------------------------- TITLE: Define State, Node, and Compile Graph
DESCRIPTION: This snippet defines the state structure for the graph, creates a node function that interacts with a store for user-specific memory and invokes a language model, and compiles the graph with a memory saver and an in-memory store.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/cross-thread-persistence.ipynb#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { v4 as uuidv4 } from "uuid";
import { ChatAnthropic } from "@langchain/anthropic";
import { BaseMessage } from "@langchain/core/messages";
import { Annotation, StateGraph, START, MemorySaver, LangGraphRunnableConfig, messagesStateReducer,
} from "@langchain/langgraph"; const StateAnnotation = Annotation.Root({ messages: Annotation<BaseMessage[]> ({ reducer: messagesStateReducer, default: () => [], }),
}); const model = new ChatAnthropic({ modelName: "claude-3-5-sonnet-20240620" }); // NOTE: we're passing the Store param to the node --
// this is the Store we compile the graph with
const callModel = async ( state: typeof StateAnnotation.State, config: LangGraphRunnableConfig
): Promise<{ messages: any }> => { const store = config.store; if (!store) { if (!store) { throw new Error("store is required when compiling the graph"); } } if (!config.configurable?.userId) { throw new Error("userId is required in the config"); } const namespace = ["memories", config.configurable?.userId]; const memories = await store.search(namespace); const info = memories.map((d) => d.value.data).join("\n"); const systemMsg = `You are a helpful assistant talking to the user. User info: ${info}`; // Store new memories if the user asks the model to remember const lastMessage = state.messages[state.messages.length - 1]; if ( typeof lastMessage.content === "string" && lastMessage.content.toLowerCase().includes("remember") ) { await store.put(namespace, uuidv4(), { data: lastMessage.content }); } const response = await model.invoke([ { type: "system", content: systemMsg }, ...state.messages, ]); return { messages: response };
}; const builder = new StateGraph(StateAnnotation) .addNode("call_model", callModel) .addEdge(START, "call_model"); // NOTE: we're passing the store object here when compiling the graph
const graph = builder.compile({ checkpointer: new MemorySaver(), store: inMemoryStore,
});
// If you're using LangGraph Cloud or LangGraph Studio, you don't need to pass the store or checkpointer when compiling the graph, since it's done automatically. ``` ---------------------------------------- TITLE: Invoking LangGraph with Configurable LLM (TypeScript)
DESCRIPTION: This snippet demonstrates how to invoke a LangGraph instance with a custom configuration. The 'configurable' field allows specifying runtime parameters, such as the LLM type, which can then be accessed within the graph's nodes to dynamically select models or prompts.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/low_level.md#_snippet_24 LANGUAGE: typescript
CODE:
```
const config = { configurable: { llm: "anthropic" } }; await graph.invoke(inputs, config);
``` ---------------------------------------- TITLE: Streaming LangGraph Execution (Requires Tool/Review) - TypeScript
DESCRIPTION: Initializes inputs and configuration for a query that is expected to trigger a tool call and subsequent human review. It streams the graph execution and logs the latest message content from each event until the graph pauses for review.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/review-tool-calls.ipynb#_snippet_8 LANGUAGE: TypeScript
CODE:
```
inputs = { messages: [{ role: "user", content: "what's the weather in SF?" }] };
config = { configurable: { thread_id: "2" }, streamMode: "values" as const }; stream = await graph.stream(inputs, config); for await (const event of stream) { const recentMsg = event.messages[event.messages.length - 1]; console.log(`================================ ${recentMsg._getType()} Message (1) =================================`) console.log(recentMsg.content);
}
``` ---------------------------------------- TITLE: Implementing Streaming with Different Modes in TypeScript
DESCRIPTION: Provides the foundation for streaming execution data. The `StreamMode` enum defines output types (values, updates, messages, debug). The `_emit` function dispatches data to registered handlers based on the stream mode. Prototype methods on `PregelLoop` integrate streaming into the execution flow for specific data types like full state snapshots (`_streamValues`) and state changes (`_streamUpdates`).
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/libs/langgraph/spec/pregel-execution-model.md#_snippet_8 LANGUAGE: typescript
CODE:
```
enum StreamMode { VALUES = "values", // Complete state after each step UPDATES = "updates", // Only state changes MESSAGES = "messages", // Internal node communication DEBUG = "debug" // Detailed execution events
} // Stream handler function
function _emit( config: RunnableConfig, mode: StreamMode, values: any
): void { const handlers = config.configurable?.callbacks?.[mode]; if (handlers) { for (const handler of handlers) { handler(values); } }
} // Stream values from PregelLoop
PregelLoop.prototype._streamValues = function( config: RunnableConfig, snapshot: StateSnapshot
): void { _emit(config, StreamMode.VALUES, snapshot);
}; // Stream updates from PregelLoop
PregelLoop.prototype._streamUpdates = function( config: RunnableConfig, updates: Record<string, any>
): void { _emit(config, StreamMode.UPDATES, updates);
};
``` ---------------------------------------- TITLE: Define Sightseeing Advisor Agent Node (TypeScript)
DESCRIPTION: Implements the sightseeingAdvisor agent function for the LangGraph state machine. Similar to travelAdvisor, it sets up a system prompt, calls callLlm with its potential targets (travelAdvisor, hotelAdvisor), processes the response, and returns a Command object for routing and state updates. It also routes 'finish' to 'human'.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/multi-agent-multi-turn-convo.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
async function sightseeingAdvisor( state: typeof MessagesAnnotation.State
): Promise<Command> { const systemPrompt = "You are a travel expert that can provide specific sightseeing recommendations for a given destination. " + "If you need general travel help, go to 'travelAdvisor' for help. " + "If you need hotel recommendations, go to 'hotelAdvisor' for help. " + "If you have enough information to respond to the user, return 'finish'. " + "Never mention other agents by name."; const messages = [{"role": "system", "content": systemPrompt}, ...state.messages] as BaseMessage[]; const targetAgentNodes = ["travelAdvisor", "hotelAdvisor"]; const response = await callLlm(messages, targetAgentNodes); const aiMsg = {"role": "ai", "content": response.response, "name": "sightseeingAdvisor"}; let goto = response.goto; if (goto === "finish") { goto = "human"; } return new Command({ goto, update: {"messages": [aiMsg] } });
}
``` ---------------------------------------- TITLE: Streaming Multiple Output Modes from LangGraph (JavaScript)
DESCRIPTION: Demonstrates how to stream multiple types of output chunks from the LangGraph agent simultaneously. It calls the stream method with an array of desired streamMode values ("updates", "debug") and iterates through the resulting stream, logging the type and content of each chunk.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/stream-multiple.ipynb#_snippet_4 LANGUAGE: javascript
CODE:
```
let inputs = { messages: [{ role: "user", content: "what's the weather in sf?" }] }; let stream = await graph.stream(inputs, { streamMode: ["updates", "debug"],
}); for await (const chunk of stream) { console.log(`Receiving new event of type: ${chunk[0]}`); console.log(chunk[1]); console.log("\n====\n");
}
``` ---------------------------------------- TITLE: Resume Stream After Updating Subgraph State - LangGraphJS - Javascript
DESCRIPTION: Resumes the graph execution stream from the point where it paused, using the thread ID. This time, the execution will proceed with the state that was previously modified (e.g., the updated city), demonstrating the effect of the state update.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/subgraphs-manage-state.ipynb#_snippet_18 LANGUAGE: javascript
CODE:
```
const resumedStreamWithUpdatedState = await graph.stream(null, { configurable: { thread_id: "4" }, streamMode: "updates", subgraphs: true
}) for await (const update of resumedStreamWithUpdatedState) { console.log(JSON.stringify(update, null, 2));
}
``` ---------------------------------------- TITLE: Setting Up Search Tool and Executor - TypeScript
DESCRIPTION: Defines the tools to be used by the agent, specifically a Tavily search tool, and wraps them in a ToolExecutor for execution within the LangGraph. Requires @langchain/langgraph and @langchain/community/tools/tavily_search.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/chat_agent_executor_with_function_calling/base.ipynb#_snippet_3 LANGUAGE: typescript
CODE:
```
import { ToolExecutor } from "@langchain/langgraph/prebuilt";
import { TavilySearchResults } from "@langchain/community/tools/tavily_search"; const tools = [new TavilySearchResults({ maxResults: 1 })]; const toolExecutor = new ToolExecutor({ tools,
});
``` ---------------------------------------- TITLE: Adding Memory to LangGraph Supervisor Workflow (TypeScript)
DESCRIPTION: Demonstrates how to compile a LangGraph supervisor workflow with a MemorySaver (checkpointer) or an InMemoryStore to enable persistence. It shows importing the necessary classes and passing instances to the workflow's .compile() method.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/libs/langgraph-supervisor/README.md#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { MemorySaver, InMemoryStore } from "@langchain/langgraph"; const checkpointer = new MemorySaver()
const store = new InMemoryStore() const model = ...
const researchAgent = ...
const mathAgent = ... const workflow = createSupervisor({ agents: [researchAgent, mathAgent], llm: model, prompt: "You are a team supervisor managing a research expert and a math expert.",
}) // Compile with checkpointer/store
const app = workflow.compile({ checkpointer, store
})
``` ---------------------------------------- TITLE: Wrapping Tools in a ToolNode (Python)
DESCRIPTION: Instantiates a ToolNode from @langchain/langgraph/prebuilt, passing the list of defined tools. This node is responsible for executing the tools when invoked by the language model within the graph.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/persistence.ipynb#_snippet_4 LANGUAGE: python
CODE:
```
import { ToolNode } from "@langchain/langgraph/prebuilt"; const toolNode = new ToolNode(tools);
``` ---------------------------------------- TITLE: Updating LangGraph State for Forking (TypeScript)
DESCRIPTION: Explains how to modify the state of a specific historical checkpoint to create a fork. The `updateState` method is used, providing the `threadConfig` with the target `checkpoint_id` and the new state data. This action creates a new checkpoint derived from the specified historical one.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/time-travel.md#_snippet_3 LANGUAGE: typescript
CODE:
```
const threadConfig = { configurable: { thread_id: "1", checkpoint_id: "xyz" } }; graph.updateState(threadConfig, { state: "updated state" });
``` ---------------------------------------- TITLE: Defining LangGraph Workflow with Message Filtering (TypeScript)
DESCRIPTION: This snippet defines the state structure, tools, language model, and the core nodes (`agent`, `action`) and edges for a LangGraph workflow. It includes a `filterMessages` function to process the message history before invoking the model and a `shouldContinueMessageFiltering` function to determine the next step based on the model's response.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/manage-conversation-history.ipynb#_snippet_5 LANGUAGE: TypeScript
CODE:
```
import { ChatAnthropic } from "@langchain/anthropic";
import { tool } from "@langchain/core/tools";
import { BaseMessage, AIMessage } from "@langchain/core/messages";
import { StateGraph, Annotation, START, END } from "@langchain/langgraph";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import { MemorySaver } from "@langchain/langgraph";
import { z } from "zod"; const MessageFilteringAgentState = Annotation.Root({ messages: Annotation<BaseMessage[]> ({ reducer: (x, y) => x.concat(y), }),
}); const messageFilteringMemory = new MemorySaver(); const messageFilteringSearchTool = tool((_): string => { // This is a placeholder for the actual implementation // Don't let the LLM know this though 😊 return "It's sunny in San Francisco, but you better look out if you're a Gemini 😈."
}, { name: "search", description: "Call to surf the web.", schema: z.object({ query: z.string() })
}) // We can re-use the same search tool as above as we don't need to change it for this example.
const messageFilteringTools = [messageFilteringSearchTool]
const messageFilteringToolNode = new ToolNode<typeof MessageFilteringAgentState.State>(messageFilteringTools)
const messageFilteringModel = new ChatAnthropic({ model: "claude-3-haiku-20240307" })
const boundMessageFilteringModel = messageFilteringModel.bindTools(messageFilteringTools) async function shouldContinueMessageFiltering(state: typeof MessageFilteringAgentState.State): Promise<"action" | typeof END> { const lastMessage = state.messages[state.messages.length - 1]; // If there is no function call, then we finish if (lastMessage && !(lastMessage as AIMessage).tool_calls?.length) { return END; } // Otherwise if there is, we continue return "action";
} const filterMessages = (messages: BaseMessage[]): BaseMessage[] => { // This is very simple helper function which only ever uses the last message return messages.slice(-1);
} // Define the function that calls the model
async function callModelMessageFiltering(state: typeof MessageFilteringAgentState.State) { const response = await boundMessageFilteringModel.invoke(filterMessages(state.messages)); // We return an object, because this will get merged with the existing state return { messages: [response] };
} // Define a new graph
const messageFilteringWorkflow = new StateGraph(MessageFilteringAgentState) // Define the two nodes we will cycle between .addNode("agent", callModelMessageFiltering) .addNode("action", messageFilteringToolNode) // We now add a conditional edge .addConditionalEdges( // First, we define the start node. We use `agent`. // This means these are the edges taken after the `agent` node is called. "agent", // Next, we pass in the function that will determine which node is called next. shouldContinueMessageFiltering ) // We now add a normal edge from `action` to `agent`. // This means that after `action` is called, `agent` node is called next. .addEdge("action", "agent") // Set the entrypoint as `agent` // This means that this node is the first one called .addEdge(START, "agent"); // Finally, we compile it!
// This compiles it into a LangChain Runnable,
// meaning you can use it as you would any other runnable
const messageFilteringApp = messageFilteringWorkflow.compile({ checkpointer: messageFilteringMemory,
});
``` ---------------------------------------- TITLE: Managing Long-Term Memory with InMemoryStore in LangGraph (TypeScript)
DESCRIPTION: This snippet demonstrates basic operations for managing long-term memory using LangGraph's InMemoryStore. It shows how to initialize the store, put a JSON document under a specific namespace and key, retrieve the document by its key, and search for documents within a namespace using content filters. Note that InMemoryStore is suitable for testing, and a database-backed store should be used for production environments.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/memory.md#_snippet_5 LANGUAGE: typescript
CODE:
```
import { InMemoryStore } from "@langchain/langgraph"; // InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.
const store = new InMemoryStore();
const userId = "my-user";
const applicationContext = "chitchat";
const namespace = [userId, applicationContext];
await store.put(namespace, "a-memory", { rules: [ "User likes short, direct language", "User only speaks English & TypeScript" ], "my-key": "my-value"
});
// get the "memory" by ID
const item = await store.get(namespace, "a-memory");
// list "memories" within this namespace, filtering on content equivalence
const items = await store.search(namespace, { filter: { "my-key": "my-value" }
});
``` ---------------------------------------- TITLE: Initialize LangGraph Agent with MemorySaver Checkpointer
DESCRIPTION: Initializes a LangGraph agent with a MemorySaver checkpointer to enable thread-level persistence. The entrypoint function is modified to accept and process previous state, and the final output is structured to save the full message history.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/react-agent-from-scratch-functional.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
import { MemorySaver, getPreviousState,
} from "@langchain/langgraph"; // highlight-next-line
const checkpointer = new MemorySaver(); const agentWithMemory = entrypoint({ name: "agentWithMemory", // highlight-next-line checkpointer,
}, async (messages: BaseMessageLike[]) => { const previous = getPreviousState<BaseMessage>() ?? []; let currentMessages = addMessages(previous, messages); let llmResponse = await callModel(currentMessages); while (true) { if (!llmResponse.tool_calls?.length) { break; } // Execute tools const toolResults = await Promise.all( llmResponse.tool_calls.map((toolCall) => { return callTool(toolCall); }) ); // Append to message list currentMessages = addMessages(currentMessages, [llmResponse, ...toolResults]); // Call model again llmResponse = await callModel(currentMessages); } // Append final response for storage currentMessages = addMessages(currentMessages, llmResponse); // highlight-next-line return entrypoint.final({ // highlight-next-line value: llmResponse, // highlight-next-line save: currentMessages, // highlight-next-line });
});
``` ---------------------------------------- TITLE: Getting the Latest LangGraph State by Thread ID (TypeScript)
DESCRIPTION: Demonstrates how to retrieve the most recent state snapshot for a specific thread using the getState method of a compiled graph, providing the thread_id in the configuration. Requires a compiled graph with a checkpointer.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/persistence.md#_snippet_2 LANGUAGE: TypeScript
CODE:
```
// Get the latest state snapshot
const config = { configurable: { thread_id: "1" } };
const state = await graph.getState(config);
``` ---------------------------------------- TITLE: Retrieve Final State from LangGraph Checkpoint
DESCRIPTION: Shows how to retrieve the final state of the graph after execution using the same checkpoint configuration used for streaming. It then extracts and logs the content of all messages stored in the state.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/reflection/reflection.ipynb#_snippet_8 LANGUAGE: JavaScript
CODE:
```
const snapshot = await app.getState(checkpointConfig);
console.log( snapshot.values.messages .map((msg: BaseMessage) => msg.content) .join("\n\n\n------------------\n\n\n"),
);
``` ---------------------------------------- TITLE: Retrieving Final LangGraph State (JavaScript)
DESCRIPTION: Shows how to retrieve the final state of a completed workflow execution using the `getState` method and the thread configuration. It then logs the final result stored in the state. Requires a compiled `app` instance and the thread configuration used for execution.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rewoo/rewoo.ipynb#_snippet_11 LANGUAGE: JavaScript
CODE:
```
const snapshot = await app.getState(threadConfig);
console.log(snapshot.values.result);
``` ---------------------------------------- TITLE: Define Generic LangGraph Entrypoint Workflow (TypeScript)
DESCRIPTION: Illustrates the basic structure for defining a LangGraph workflow using the `entrypoint` function. It shows how to configure the entrypoint with a `checkpointer` (like `MemorySaver`) and a name, and defines the signature for the main workflow function, emphasizing the single positional argument requirement and JSON serializability.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/functional_api.md#_snippet_2 LANGUAGE: typescript
CODE:
```
import { entrypoint, MemorySaver } from "@langchain/langgraph"; const checkpointer = new MemorySaver(); const myWorkflow = entrypoint( { checkpointer, name: "myWorkflow" }, async (someInput: Record<string, any>): Promise<number> => { // some logic that may involve long-running tasks like API calls, // and may be interrupted for human-in-the-loop. return result; }
);
``` ---------------------------------------- TITLE: Defining and Invoking a Subgraph with State Transformation in LangGraph (TypeScript)
DESCRIPTION: This snippet demonstrates how to define and integrate a subgraph with a distinct state schema into a parent LangGraph. It shows the necessary state transformation logic to convert parent graph state to subgraph state before invocation and then transform the subgraph's output back to the parent's state, enabling modular graph design.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/low_level.md#_snippet_31 LANGUAGE: TypeScript
CODE:
```
import { StateGraph, Annotation } from "@langchain/langgraph"; const StateAnnotation = Annotation.Root({ foo: Annotation<string>,
}); const SubgraphStateAnnotation = Annotation.Root({ // note that none of these keys are shared with the parent graph state bar: Annotation<string>, baz: Annotation<string>,
}); // Define subgraph
const subgraphNode = async (state: typeof SubgraphStateAnnotation.State) => { return { bar: state.bar + "baz" };
}; const subgraph = new StateGraph(SubgraphStateAnnotation) .addNode("subgraph", subgraphNode); ... .compile(); // Define parent graph
const subgraphWrapperNode = async (state: typeof StateAnnotation.State) => { // transform the state to the subgraph state const response = await subgraph.invoke({ bar: state.foo, }); // transform response back to the parent state return { foo: response.bar, };
} const parentGraph = new StateGraph(StateAnnotation) .addNode("subgraph", subgraphWrapperNode) ... .compile();
``` ---------------------------------------- TITLE: Streaming Filtered Events from LangGraph Agent (TypeScript)
DESCRIPTION: Demonstrates how to stream events from the agent runnable using streamEvents. It iterates through the streamed events, specifically including those with the tool_llm tag using the includeTags option. It logs the type and content of chunk events.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/streaming-events-from-within-tools.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
let finalEvent; for await (const event of agent.streamEvents( { messages: [ [ "human", "what items are on the shelf? You should call the get_items tool.", ], ], }, { version: "v2", }, { includeTags: ["tool_llm"], }
)) { if ("chunk" in event.data) { console.dir({ type: event.data.chunk._getType(), content: event.data.chunk.content, }) } finalEvent = event;
}
``` ---------------------------------------- TITLE: Implementing Human Approval Workflow with Breakpoints (TypeScript)
DESCRIPTION: This TypeScript code illustrates a human-in-the-loop approval pattern using LangGraph breakpoints. It compiles the graph with a `checkpointer` and sets an `interruptBefore` breakpoint before a specific node ("node_2"). The graph is streamed up to this breakpoint, allowing for external human review. After approval (represented by the comment), the graph execution is resumed from the last checkpoint using `graph.stream` with `null` input, continuing the workflow.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/v0-human-in-the-loop.md#_snippet_4 LANGUAGE: typescript
CODE:
```
// Compile our graph with a checkpointer and a breakpoint before the step to approve
const graph = builder.compile({ checkpointer, interruptBefore: ["node_2"] }); // Run the graph up to the breakpoint
for await (const event of await graph.stream(inputs, threadConfig)) { console.log(event);
} // ... Get human approval ... // If approved, continue the graph execution from the last saved checkpoint
for await (const event of await graph.stream(null, threadConfig)) { console.log(event);
}
``` ---------------------------------------- TITLE: Streaming Graph Events - LangGraphJS - TypeScript
DESCRIPTION: Demonstrates how to stream events from the LangGraph execution using the '.streamEvents' method. It initiates the graph with a user message and iterates through the event stream, logging custom events.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/streaming-tokens-without-langchain.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
const eventStream = await graph.streamEvents( { messages: [{ role: "user", content: "what's in the bedroom?" }] }, { version: "v2" },
); for await (const { event, name, data } of eventStream) { if (event === "on_custom_event") { console.log(name, data); }
}
``` ---------------------------------------- TITLE: Define and Compile Authoring Graph in LangGraphJS
DESCRIPTION: Defines a LangGraph StateGraph for an authoring process, including nodes for different roles (DocWriter, NoteTaker, ChartGenerator) and a supervisor. It sets up both unconditional and conditional edges for routing based on the supervisor's output, and compiles the graph into a runnable chain.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb#_snippet_14 LANGUAGE: JavaScript
CODE:
```
// Create the graph here:
const authoringGraph = new StateGraph(DocWritingState) .addNode("DocWriter", docWritingNode) .addNode("NoteTaker", noteTakingNode) .addNode("ChartGenerator", chartGeneratingNode) .addNode("supervisor", docWritingSupervisor) // Add the edges that always occur .addEdge("DocWriter", "supervisor") .addEdge("NoteTaker", "supervisor") .addEdge("ChartGenerator", "supervisor") // Add the edges where routing applies .addConditionalEdges("supervisor", (x) => x.next, { DocWriter: "DocWriter", NoteTaker: "NoteTaker", ChartGenerator: "ChartGenerator", FINISH: END, }) .addEdge(START, "supervisor"); const enterAuthoringChain = RunnableLambda.from( ({ messages }: { messages: BaseMessage[] }) => { return { messages: messages, team_members: ["Doc Writer", "Note Taker", "Chart Generator"], }; },
);
const authoringChain = enterAuthoringChain.pipe(authoringGraph.compile());
``` ---------------------------------------- TITLE: Creating and Running Supervisor Multi-Agent System (TypeScript)
DESCRIPTION: Defines tools for booking hotels and flights, creates specialized React agents for each, and then uses `createSupervisor` to coordinate them. The compiled supervisor graph is then invoked with a user query, and the output stream is logged.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/multi-agent.md#_snippet_1 LANGUAGE: typescript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
// highlight-next-line
import { createSupervisor } from "@langchain/langgraph-supervisor";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const bookHotel = tool( async (input: { hotel_name: string }) => { return `Successfully booked a stay at ${input.hotel_name}.`; }, { name: "book_hotel", description: "Book a hotel", schema: z.object({ hotel_name: z.string().describe("The name of the hotel to book"), }), }
); const bookFlight = tool( async (input: { from_airport: string; to_airport: string }) => { return `Successfully booked a flight from ${input.from_airport} to ${input.to_airport}.`; }, { name: "book_flight", description: "Book a flight", schema: z.object({ from_airport: z.string().describe("The departure airport code"), to_airport: z.string().describe("The arrival airport code"), }), }
); const llm = new ChatOpenAI({ modelName: "gpt-4o" }); // Create specialized agents
const flightAssistant = createReactAgent({ llm, tools: [bookFlight], prompt: "You are a flight booking assistant", // highlight-next-line name: "flight_assistant",
}); const hotelAssistant = createReactAgent({ llm, tools: [bookHotel], prompt: "You are a hotel booking assistant", // highlight-next-line name: "hotel_assistant",
}); // highlight-next-line
const supervisor = createSupervisor({ agents: [flightAssistant, hotelAssistant], llm, prompt: "You manage a hotel booking assistant and a flight booking assistant. Assign work to them, one at a time.",
}).compile(); const stream = await supervisor.stream({ messages: [{ role: "user", content: "first book a flight from BOS to JFK and then book a stay at McKittrick Hotel" }]
}); for await (const chunk of stream) { console.log(chunk); console.log("\n");
}
``` ---------------------------------------- TITLE: Define Example LangGraph Agent (TypeScript)
DESCRIPTION: Example TypeScript code defining a LangGraph agent using `createReactAgent`. It includes importing necessary modules, defining a custom tool (`getWeather`) with a Zod schema, initializing a chat model, and exporting the `graph` instance for use by the API server.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/deployment.md#_snippet_1 LANGUAGE: typescript
CODE:
```
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { initChatModel } from "langchain/chat_models/universal";
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const getWeather = tool( async (input: { city: string }) => { return `It's always sunny in ${input.city}!`; }, { name: "getWeather", schema: z.object({ city: z.string().describe("The city to get the weather for") }), description: "Get weather for a given city." }
); const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest");
// make sure to export the graph that will be used in the LangGraph API server
// highlight-next-line
export const graph = createReactAgent({ llm, tools: [getWeather], prompt: "You are a helpful assistant"
})
``` ---------------------------------------- TITLE: Defining Model and Tools (TypeScript)
DESCRIPTION: Initializes the ChatOpenAI model and defines a placeholder 'getWeather' tool using @langchain/core/tools and zod for schema validation. This tool simulates fetching weather information based on location.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/react-agent-from-scratch-functional.ipynb#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const model = new ChatOpenAI({ model: "gpt-4o-mini",
}); const getWeather = tool(async ({ location }) => { const lowercaseLocation = location.toLowerCase(); if (lowercaseLocation.includes("sf") || lowercaseLocation.includes("san francisco")) { return "It's sunny!"; } else if (lowercaseLocation.includes("boston")) { return "It's rainy!"; } else { return `I am not sure what the weather is in ${location}`; }
}, { name: "getWeather", schema: z.object({ location: z.string().describe("location to get the weather for"), }), description: "Call to get the weather from a specific location."
}); const tools = [getWeather];
``` ---------------------------------------- TITLE: Defining Tool for Memory Upsert (TypeScript)
DESCRIPTION: Defines a LangChain tool `upsert_memory` that takes `content` as input. It uses a provided `InMemoryStore` from the LangGraph configuration to store the content with a unique UUID key under a specific user path. Requires `@langchain/core/tools`, `@langchain/langgraph`, `zod`, and `uuid`. Returns "Stored memory." on success or throws an error if the store is missing.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/semantic-search.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { tool } from "@langchain/core/tools";
import { LangGraphRunnableConfig } from "@langchain/langgraph"; import { z } from "zod";
import { v4 as uuidv4 } from "uuid"; const upsertMemoryTool = tool(async ( { content }, config: LangGraphRunnableConfig
): Promise<string> => { const store = config.store as InMemoryStore; if (!store) { throw new Error("No store provided to tool."); } await store.put( ["user_123", "memories"], uuidv4(), // give each memory its own unique ID { text: content } ); return "Stored memory.";
}, { name: "upsert_memory", schema: z.object({ content: z.string().describe("The content of the memory to store."), }), description: "Upsert long-term memories.",
});
``` ---------------------------------------- TITLE: Run LangGraph Workflow (JS)
DESCRIPTION: Executes the compiled LangGraph application (app) with a specific input query and configuration (including a recursion limit). It uses the stream method to asynchronously iterate over and print the events generated during the graph's execution.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/plan-and-execute/plan-and-execute.ipynb#_snippet_12 LANGUAGE: JavaScript
CODE:
```
const config = { recursionLimit: 50 };
const inputs = { input: "what is the hometown of the 2024 Australian open winner?",
}; for await (const event of await app.stream(inputs, config)) { console.log(event);
} ``` ---------------------------------------- TITLE: Invoke LangGraph Agent with Initial Input (JS)
DESCRIPTION: Defines input messages and configuration for the graph, includes a helper function `printMessages` to format and display the conversation history, and invokes the compiled graph with the initial user input.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/pass-run-time-values-to-tools.ipynb#_snippet_11 LANGUAGE: JavaScript
CODE:
```
import { BaseMessage, isAIMessage, isHumanMessage, isToolMessage, HumanMessage, ToolMessage,
} from "@langchain/core/messages"; let inputs = { messages: [ new HumanMessage({ content: "My favorite pet is a terrier. I saw a cute one on Twitter." }) ],
}; let config = { configurable: { thread_id: "1", userId: "a-user", },
}; function printMessages(messages: BaseMessage[]) { for (const message of messages) { if (isHumanMessage(message)) { console.log(`User: ${message.content}`); } else if (isAIMessage(message)) { const aiMessage = message as AIMessage; if (aiMessage.content) { console.log(`Assistant: ${aiMessage.content}`); } if (aiMessage.tool_calls) { for (const toolCall of aiMessage.tool_calls) { console.log(`Tool call: ${toolCall.name}(${JSON.stringify(toolCall.args)})`); } } } else if (isToolMessage(message)) { const toolMessage = message as ToolMessage; console.log(`${toolMessage.name} tool output: ${toolMessage.content}`); } }
} let { messages } = await graph.invoke(inputs, config); printMessages(messages);
``` ---------------------------------------- TITLE: Invoking LangGraph for Chart Generation Query (JS/TS)
DESCRIPTION: This snippet shows another invocation of the compiled graph using `graph.stream`, this time with an initial `HumanMessage` requesting a bar chart of US GDP growth. It sets a higher recursion limit and asynchronously iterates through the streamed output, logging each step's result until the `__end__` marker is reached.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/multi_agent/agent_supervisor.ipynb#_snippet_9 LANGUAGE: javascript
CODE:
```
streamResults = graph.stream( { messages: [ new HumanMessage({ content: "Generate a bar chart of the US GDP growth from 2021-2023.", }), ], }, { recursionLimit: 150 },
); for await (const output of await streamResults) { if (!output?.__end__) { console.log(output); console.log("----"); }
}
``` ---------------------------------------- TITLE: Generating Answer with RAG Chain (TypeScript)
DESCRIPTION: This function generates the final answer using a Retrieval Augmented Generation (RAG) chain. It extracts the question and the content of the last tool message (assumed to be retrieved documents) from the state, pulls a RAG prompt, initializes a ChatOpenAI model, creates a RAG chain by piping the prompt to the model, and invokes the chain with the context and question.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rag/langgraph_agentic_rag.ipynb#_snippet_11 LANGUAGE: TypeScript
CODE:
```
async function generate(state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> { console.log("---GENERATE---"); const { messages } = state; const question = messages[0].content as string; // Extract the most recent ToolMessage const lastToolMessage = messages.slice().reverse().find((msg) => msg._getType() === "tool"); if (!lastToolMessage) { throw new Error("No tool message found in the conversation history"); } const docs = lastToolMessage.content as string; const prompt = await pull<ChatPromptTemplate>("rlm/rag-prompt"); const llm = new ChatOpenAI({ model: "gpt-4o", temperature: 0, streaming: true, }); const ragChain = prompt.pipe(llm); const response = await ragChain.invoke({ context: docs, question, }); return { messages: [response], };
}
``` ---------------------------------------- TITLE: Define Research Team Tools (JS)
DESCRIPTION: Initializes tools for the research team, including a search tool using TavilySearchResults and a custom tool to scrape webpage content using CheerioWebBaseLoader. The scrape tool takes a URL and returns formatted document content.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb#_snippet_1 LANGUAGE: JavaScript
CODE:
```
import { TavilySearchResults } from "@langchain/community/tools/tavily_search";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const tavilyTool = new TavilySearchResults(); const scrapeWebpage = tool(async (input) => { const loader = new CheerioWebBaseLoader(input.url); const docs = await loader.load(); const formattedDocs = docs.map( (doc) => `<Document name=\"${doc.metadata?.title}\">\n${doc.pageContent}\n<\/Document>`, ); return formattedDocs.join("\n\n"); }, { name: "scrape_webpage", description: "Scrape the contents of a webpage.", schema: z.object({ url: z.string(), }), }
)
``` ---------------------------------------- TITLE: Build LangGraphJS Workflow with Routing (TypeScript)
DESCRIPTION: Constructs the main LangGraphJS workflow using 'entrypoint'. It first calls the 'router' task to determine the next step ('story', 'joke', or 'poem'), then selects and invokes the corresponding LLM task ('llmCall1', 'llmCall2', or 'llmCall3') based on the router's output.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_13 LANGUAGE: TypeScript
CODE:
```
const workflow = entrypoint( "routerWorkflow", async (input: string) => { const nextStep = await llmCallRouter(input); let llmCall; if (nextStep === "story") { llmCall = llmCall1; } else if (nextStep === "joke") { llmCall = llmCall2; } else if (nextStep === "poem") { llmCall = llmCall3; } const finalResult = await llmCall(input); return finalResult; }
);
``` ---------------------------------------- TITLE: Defining LangGraph Nodes and Building StateGraph (JS/TS)
DESCRIPTION: Defines the `hotelAdvisor` and `humanNode` functions used as nodes in the LangGraph state machine. It then constructs a `StateGraph` using these nodes and others, defining the possible transitions (`ends`) between them and setting the starting node.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/multi-agent-multi-turn-convo.ipynb#_snippet_8 LANGUAGE: javascript
CODE:
``` const response = await callLlm(messages, targetAgentNodes); const aiMsg = {"role": "ai", "content": response.response, "name": "hotelAdvisor"}; let goto = response.goto; if (goto === "finish") { goto = "human"; } return new Command({ goto, update: {"messages": [aiMsg] } });
} function humanNode( state: typeof MessagesAnnotation.State
): Command { const userInput: string = interrupt("Ready for user input."); let activeAgent: string | undefined = undefined; // Look up the active agent for (let i = state.messages.length - 1; i >= 0; i--) { if (state.messages[i].name) { activeAgent = state.messages[i].name; break; } } if (!activeAgent) { throw new Error("Could not determine the active agent."); } return new Command({ goto: activeAgent, update: { "messages": [ { "role": "human", "content": userInput, } ] } });
} const builder = new StateGraph(MessagesAnnotation) .addNode("travelAdvisor", travelAdvisor, { ends: ["sightseeingAdvisor", "hotelAdvisor"] }) .addNode("sightseeingAdvisor", sightseeingAdvisor, { ends: ["human", "travelAdvisor", "hotelAdvisor"] }) .addNode("hotelAdvisor", hotelAdvisor, { ends: ["human", "travelAdvisor", "sightseeingAdvisor"] }) // This adds a node to collect human input, which will route // back to the active agent. .addNode("human", humanNode, { ends: ["hotelAdvisor", "sightseeingAdvisor", "travelAdvisor", "human"] }) // We'll always start with a general travel advisor. .addEdge(START, "travelAdvisor") const checkpointer = new MemorySaver()
const graph = builder.compile({ checkpointer })
``` ---------------------------------------- TITLE: Define LangGraph Workflow for Joke Optimization (TypeScript)
DESCRIPTION: Defines the main LangGraph workflow named 'optimizerWorkflow'. It iteratively calls the joke generator and evaluator tasks until the evaluator grades the joke as 'funny', then returns the successful joke.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_23 LANGUAGE: TypeScript
CODE:
```
// Build workflow const workflow = entrypoint( "optimizerWorkflow", async (topic: string) => { let feedback: z.infer<typeof feedbackSchema> | undefined; let joke: string; while (true) { joke = await llmCallGenerator({ topic, feedback }); feedback = await llmCallEvaluator(joke); if (feedback.grade === "funny") { break; } } return joke; } );
``` ---------------------------------------- TITLE: Defining Solver Node Logic (JavaScript)
DESCRIPTION: Defines the prompt template for the final solving step using `ChatPromptTemplate` and implements the `solve` function. The `solve` function formats the plan and results from the state, invokes an LLM (`gpt-4o`) with the formatted prompt and task, and returns the final answer to be stored in the state. Requires `ChatPromptTemplate`, `ChatOpenAI`, and `GraphState` types.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rewoo/rewoo.ipynb#_snippet_8 LANGUAGE: JavaScript
CODE:
```
const solvePrompt = ChatPromptTemplate.fromTemplate( `Solve the following task or problem. To solve the problem, we have made step-by-step Plan and
retrieved corresponding Evidence to each Plan. Use them with caution since long evidence might
contain irrelevant information. {plan} Now solve the question or task according to provided Evidence above. Respond with the answer
directly with no extra words. Task: {task}
Response:`,
); async function solve(state: typeof GraphState.State, config?: RunnableConfig) { console.log("---SOLVE---"); let plan = ""; const _results = state.results || {}; for (let [_plan, stepName, tool, toolInput] of state.steps) { for (const [k, v] of Object.entries(_results)) { toolInput = toolInput.replace(k, v); } plan += `Plan: ${_plan}\n${stepName} = ${tool}[${toolInput}]\n`; } const model = new ChatOpenAI({ temperature: 0, model: "gpt-4o", }); const result = await solvePrompt .pipe(model) .invoke({ plan, task: state.task }, config); return { result: result.content.toString(), };
}
``` ---------------------------------------- TITLE: Implementing Short-Term Memory with LangGraph Agent (TypeScript)
DESCRIPTION: This snippet demonstrates how to configure and use short-term memory (thread-level memory) with a LangGraph agent in TypeScript. It shows how to initialize a `MemorySaver` checkpointer, pass it to `createReactAgent`, and invoke the agent with a `thread_id` in the config to maintain conversation history across multiple calls within the same session.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/agents/memory.md#_snippet_0 LANGUAGE: TypeScript
CODE:
```
// highlight-next-line
import { MemorySaver } from "@langchain/langgraph-checkpoint";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { initChatModel } from "langchain/chat_models/universal";
import { tool } from "@langchain/core/tools";
import { z } from "zod"; // highlight-next-line
const checkpointer = new MemorySaver(); // (1)! const getWeather = tool( async (input: { city: string }) => { return `It's always sunny in ${input.city}!`; }, { name: "getWeather", schema: z.object({ city: z.string().describe("The city to get the weather for") }), description: "Get weather for a given city." }
); const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest");
const agent = createReactAgent({ llm, tools: [getWeather], // highlight-next-line checkpointer // (2)!
}); // Run the agent
// highlight-next-line
const config = { configurable: { thread_id: "1" } }; // (3)!
const sfResponse = await agent.invoke( { messages: [ { role: "user", content: "what is the weather in sf" } ] }, config // (4)!
);
const nyResponse = await agent.invoke( { messages: [ { role: "user", content: "what about new york?" } ], config }
);
``` ---------------------------------------- TITLE: Thread-level persistence with RemoteGraph (JavaScript)
DESCRIPTION: Shows how to use `RemoteGraph` with a thread ID in the configuration to persist the graph's state across invocations in JavaScript. It covers creating a thread using the `Client`, invoking the remote graph with the thread config, and fetching the persisted state.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/how-tos/use-remote-graph.md#_snippet_8 LANGUAGE: JavaScript
CODE:
```
import { Client } from "@langchain/langgraph-sdk";
import { RemoteGraph } from "@langchain/langgraph/remote"; const url = `<DEPLOYMENT_URL>`;
const graphName = "agent";
const client = new Client({ apiUrl: url });
const remoteGraph = new RemoteGraph({ graphId: graphName, url }); // create a thread (or use an existing thread instead)
const thread = await client.threads.create(); // invoke the graph with the thread config
const config = { configurable: { thread_id: thread.thread_id }};
const result = await remoteGraph.invoke({ messages: [{ role: "user", content: "what's the weather in sf" }], config
}); // verify that the state was persisted to the thread
const threadState = await remoteGraph.getState(config);
console.log(threadState);
``` ---------------------------------------- TITLE: Compile and Run LangGraph with Breakpoint in TypeScript
DESCRIPTION: This snippet demonstrates how to compile a LangGraph builder with a checkpointer and a breakpoint set before a specific node. It then shows how to run the graph, which will pause execution at the breakpoint, and subsequently resume the execution from the current checkpoint by providing `null` as input.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/v0-human-in-the-loop.md#_snippet_0 LANGUAGE: typescript
CODE:
```
// Compile our graph with a checkpointer and a breakpoint before "step_for_human_in_the_loop"
const graph = builder.compile({ checkpointer, interruptBefore: ["step_for_human_in_the_loop"] }); // Run the graph up to the breakpoint
const threadConfig = { configurable: { thread_id: "1" }, streamMode: "values" as const };
for await (const event of await graph.stream(inputs, threadConfig)) { console.log(event);
} // Perform some action that requires human in the loop // Continue the graph execution from the current checkpoint for await (const event of await graph.stream(null, threadConfig)) { console.log(event);
}
``` ---------------------------------------- TITLE: Defining Tools and Tool-Calling Node - LangGraphJS - TypeScript
DESCRIPTION: Defines the 'getItems' function which simulates fetching items based on a place and the 'callTools' node function which processes tool calls from the model, specifically invoking 'getItems' based on the tool call name and arguments.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/streaming-tokens-without-langchain.ipynb#_snippet_3 LANGUAGE: TypeScript
CODE:
```
const getItems = async ({ place }: { place: string }) => { if (place.toLowerCase().includes("bed")) { // For under the bed return "socks, shoes and dust bunnies"; } else if (place.toLowerCase().includes("shelf")) { // For 'shelf' return "books, pencils and pictures"; } else { // if the agent decides to ask about a different place return "cat snacks"; }
}; const callTools = async (state: typeof StateAnnotation.State) => { const { messages } = state; const mostRecentMessage = messages[messages.length - 1]; const toolCalls = (mostRecentMessage as OpenAI.ChatCompletionAssistantMessageParam).tool_calls; if (toolCalls === undefined || toolCalls.length === 0) { throw new Error("No tool calls passed to node."); } const toolNameMap = { get_items: getItems, }; const functionName = toolCalls[0].function.name; const functionArguments = JSON.parse(toolCalls[0].function.arguments); const response = await toolNameMap[functionName](functionArguments); const toolMessage = { tool_call_id: toolCalls[0].id, role: "tool" as const, name: functionName, content: response, } return { messages: [toolMessage] };
}
``` ---------------------------------------- TITLE: Streaming LangGraph Workflow (TypeScript)
DESCRIPTION: Illustrates how to execute a LangGraph workflow using the `stream` method to receive results incrementally. It shows iterating over the streamed chunks and includes passing a configuration object with a `thread_id`.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/functional_api.md#_snippet_5 LANGUAGE: typescript
CODE:
```
const config = { configurable: { thread_id: "some_thread_id", },
}; for await (const chunk of await myWorkflow.stream(someInput, config)) { console.log(chunk);
}
``` ---------------------------------------- TITLE: Integrating PostgresSaver with StateGraph (TypeScript)
DESCRIPTION: This snippet shows the general approach to adding a PostgresSaver checkpointer when compiling a custom LangGraph StateGraph builder. It demonstrates passing an instantiated PostgresSaver to the compile method.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/persistence-postgres.ipynb#_snippet_0 LANGUAGE: ts
CODE:
```
import { StateGraph } from "@langchain/langgraph";
import { PostgresSaver } from "@langchain/langgraph-checkpoint-postgres"; const builder = new StateGraph(...); // ... define the graph const checkpointer = PostgresSaver.fromConnString(...); // postgres checkpointer (see examples below) const graph = builder.compile({ checkpointer });
...
``` ---------------------------------------- TITLE: Implement Approve or Reject Pattern with LangGraph.js (TypeScript)
DESCRIPTION: This snippet demonstrates how to pause a LangGraph.js workflow for human approval or rejection using the `interrupt` function. It checks the human's response and uses a `Command` to route the graph to a different node based on the decision. It requires the `@langchain/langgraph` library.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/concepts/human_in_the_loop.md#_snippet_4 LANGUAGE: typescript
CODE:
```
import { interrupt, Command } from "@langchain/langgraph"; function humanApproval(state: typeof GraphAnnotation.State): Command { const isApproved = interrupt({ question: "Is this correct?", // Surface the output that should be // reviewed and approved by the human. llm_output: state.llm_output, }); if (isApproved) { return new Command({ goto: "some_node" }); } else { return new Command({ goto: "another_node" }); }
} // Add the node to the graph in an appropriate location
// and connect it to the relevant nodes.
const graph = graphBuilder .addNode("human_approval", humanApproval) .compile({ checkpointer }); // After running the graph and hitting the interrupt, the graph will pause.
// Resume it with either an approval or rejection.
const threadConfig = { configurable: { thread_id: "some_id" } };
await graph.invoke(new Command({ resume: true }), threadConfig);
``` ---------------------------------------- TITLE: Defining Route Question Edge in LangGraph (JavaScript)
DESCRIPTION: Defines a LangGraph conditional edge function that decides whether to route the initial question to web search or RAG based on the question content. It invokes a question router and returns the name of the next node.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rag/langgraph_adaptive_rag_local.ipynb#_snippet_18 LANGUAGE: JavaScript
CODE:
```
const routeQuestion = async (state: typeof GraphState.State) => { const source: { datasource: string } = await questionRouter.invoke({ question: state.question, }); if (source.datasource === "web_search") { console.log(`---ROUTING QUESTION "${state.question} TO WEB SEARCH---`); return "web_search"; } else { console.log(`---ROUTING QUESTION "${state.question} TO RAG---`); return "retrieve"; }
};
``` ---------------------------------------- TITLE: Resuming LangGraph Agent Execution (JS)
DESCRIPTION: Resumes the execution of the LangGraph agent by calling `app.stream` with `null` input. It iterates through the subsequent events, logging the content of tool messages and AI messages. This demonstrates how the agent continues processing based on the potentially modified state. Requires a configured `app` instance and a `config` object.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/edit-graph-state.ipynb#_snippet_13 LANGUAGE: javascript
CODE:
```
for await (const event of await app.stream(null, config)) { console.log(event) const recentMsg = event.messages[event.messages.length - 1]; console.log(`================================ ${recentMsg._getType()} Message (1) =================================`) if (recentMsg._getType() === "tool") { console.log({ name: recentMsg.name, content: recentMsg.content }) } else if (recentMsg._getType() === "ai") { console.log(recentMsg.content) }
}
``` ---------------------------------------- TITLE: Invoke LangGraph Workflow and Stream Results (TypeScript)
DESCRIPTION: Invokes the defined 'optimizerWorkflow' with the topic 'Cats' and streams updates. It then iterates through the streamed steps and logs each one to the console.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/docs/docs/tutorials/workflows/index.md#_snippet_24 LANGUAGE: TypeScript
CODE:
```
// Invoke const stream = await workflow.stream("Cats", { streamMode: "updates", }); for await (const step of stream) { console.log(step); console.log("\n"); }
``` ---------------------------------------- TITLE: Define Agent State Schema
DESCRIPTION: Defines the state schema for the StateGraph using @langchain/langgraph's Annotation. The state includes a messages attribute, which is an array of BaseMessage objects, configured to be reduced by concatenation (x.concat(y)).
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/managing-agent-steps.ipynb#_snippet_3 LANGUAGE: JavaScript
CODE:
```
import { Annotation } from "@langchain/langgraph";
import { BaseMessage } from "@langchain/core/messages"; const AgentState = Annotation.Root({ messages: Annotation<BaseMessage[]> ({ reducer: (x, y) => x.concat(y), }),
});
``` ---------------------------------------- TITLE: Run Workflow for Another User (TypeScript)
DESCRIPTION: Configures and runs the LangGraph workflow for a different user ID ("2"). It sends the same message "what is my name?" to demonstrate that memories are isolated per user and the model should not recall the information stored for user "1". The output stream is logged.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/how-tos/cross-thread-persistence-functional.ipynb#_snippet_9 LANGUAGE: TypeScript
CODE:
```
const config3 = { configurable: { thread_id: "3", }, streamMode: "values" as const,
}; const otherUserStream = await workflow.stream({ messages: [{ role: "user", content: "what is my name?", }], userId: "2"
}, config3); for await (const chunk of otherUserStream) { console.log(chunk);
}
``` ---------------------------------------- TITLE: Defining Grade Documents Node in LangGraph (JavaScript)
DESCRIPTION: Defines a LangGraph node function that grades the relevance of retrieved documents to the question. It iterates through documents, uses a retrieval grader, and updates the state with only the relevant documents.
SOURCE: https://github.com/langchain-ai/langgraphjs/blob/main/examples/rag/langgraph_adaptive_rag_local.ipynb#_snippet_15 LANGUAGE: JavaScript
CODE:
```
const gradeDocuments = async (state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> => { console.log("---CHECK DOCUMENT RELEVANCE TO QUESTION---"); // Score each doc const relevantDocs: Document[] = []; for (const doc of state.documents) { const grade: { score: string } = await retrievalGrader.invoke({ question: state.question, content: doc.pageContent, }); if (grade.score === "yes") { console.log("---GRADE: DOCUMENT RELEVANT---"); relevantDocs.push(doc); } else { console.log("---GRADE: DOCUMENT NOT RELEVANT---"); } } return { documents: relevantDocs };
};
```