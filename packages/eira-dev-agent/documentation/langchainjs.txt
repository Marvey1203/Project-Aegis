TITLE: Installing LangChain.js with npm, yarn, or pnpm
DESCRIPTION: This snippet provides the commands to install the LangChain.js library using popular Node.js package managers: npm, yarn, or pnpm. It's the first step to integrate LangChain into a JavaScript/TypeScript project.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/README.md#_snippet_0 LANGUAGE: Shell
CODE:
```
npm install -S langchain
``` LANGUAGE: Shell
CODE:
```
yarn add langchain
``` LANGUAGE: Shell
CODE:
```
pnpm add langchain
``` ---------------------------------------- TITLE: Defining and Invoking a LangChain Query Transformation Chain (TypeScript)
DESCRIPTION: This snippet defines a ChatPromptTemplate and a queryTransformationChain to convert conversational follow-up questions into standalone search queries. It uses a MessagesPlaceholder to include the conversation history and instructs an LLM to generate a relevant search query. The example invocation demonstrates how the chain processes a multi-turn conversation to produce a transformed query.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#_snippet_11 LANGUAGE: TypeScript
CODE:
```
const queryTransformPrompt = ChatPromptTemplate.fromMessages([ new MessagesPlaceholder("messages"), [ "user", "Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.", ],
]); const queryTransformationChain = queryTransformPrompt.pipe(llm); await queryTransformationChain.invoke({ messages: [ new HumanMessage("Can LangSmith help test my LLM applications?"), new AIMessage( "Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise." ), new HumanMessage("Tell me more!"), ],
});
``` ---------------------------------------- TITLE: Storing Document Splits in a Vector Store with LangChain.js
DESCRIPTION: This snippet shows how to store the previously generated document splits (`allSplits`) into a `vectorStore`. This action typically involves embedding each document chunk and then indexing these embeddings for efficient semantic search and retrieval. It's a key step for preparing data for RAG applications.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#_snippet_10 LANGUAGE: JavaScript
CODE:
```
await vectorStore.addDocuments(allSplits)
``` ---------------------------------------- TITLE: Importing ChatOpenAI for Chat Models - TypeScript
DESCRIPTION: Imports the `ChatOpenAI` class from the `@langchain/openai` package. This class is specifically designed to interact with OpenAI's chat-optimized models, such as GPT-3.5 Turbo and GPT-4, within LangChain applications. It serves as the primary interface for building conversational AI features.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/openai.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
``` ---------------------------------------- TITLE: Setting Up Environment Variables for OpenAI and LangSmith (JavaScript)
DESCRIPTION: This snippet demonstrates how to set environment variables for the OpenAI API key and optional LangSmith tracing. It includes configurations for enabling background callbacks, tracing, and specifying a LangSmith project, which are crucial for monitoring and debugging agent workflows.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/migrate_agent.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
// process.env.OPENAI_API_KEY = "..."; // Optional, add tracing in LangSmith
// process.env.LANGSMITH_API_KEY = "ls...";
// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = "true";
// process.env.LANGSMITH_TRACING = "true";
// process.env.LANGSMITH_PROJECT = "How to migrate: LangGraphJS"; // Reduce tracing latency if you are not in a serverless environment
// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = "true";
``` ---------------------------------------- TITLE: Defining LangGraph Workflow with Message History (JavaScript)
DESCRIPTION: This code defines a LangGraph workflow to manage conversational state. It uses `MessagesAnnotation` for state, adds a `callModel` node to invoke the LLM and update messages, and compiles the graph with `MemorySaver` for in-memory persistence of message history between runs.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/message_history.ipynb#_snippet_1 LANGUAGE: JavaScript
CODE:
```
import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from "@langchain/langgraph"; // Define the function that calls the model
const callModel = async (state: typeof MessagesAnnotation.State) => { const response = await llm.invoke(state.messages); // Update message history with response: return { messages: response };
}; // Define a new graph
const workflow = new StateGraph(MessagesAnnotation) // Define the (single) node in the graph .addNode("model", callModel) .addEdge(START, "model") .addEdge("model", END); // Add memory
const memory = new MemorySaver();
const app = workflow.compile({ checkpointer: memory });
``` ---------------------------------------- TITLE: Creating and Invoking a ChatPromptTemplate in LangChain.js (TypeScript)
DESCRIPTION: This snippet illustrates how to construct and use a `ChatPromptTemplate` to format an array of messages for a chat model in LangChain.js. It imports `ChatPromptTemplate`, defines a template with a system message and a user message containing a variable, and then invokes it with an object to populate the variable, generating a structured chat prompt.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#_snippet_21 LANGUAGE: TypeScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts"; const promptTemplate = ChatPromptTemplate.fromMessages([ ["system", "You are a helpful assistant"], ["user", "Tell me a joke about {topic}"],
]); await promptTemplate.invoke({ topic: "cats" });
``` ---------------------------------------- TITLE: Building a Conditional Retrieval Chain
DESCRIPTION: This code constructs a custom LangChain `RunnableLambda` that conditionally executes a retriever. It first invokes the `queryAnalyzer`. If a tool call (search query) is present, it parses the query and invokes the `retriever`; otherwise, it returns the LLM's direct response.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_no_queries.ipynb#_snippet_9 LANGUAGE: TypeScript
CODE:
```
import { RunnableConfig, RunnableLambda } from "@langchain/core/runnables"; const chain = async (question: string, config?: RunnableConfig) => { const response = await queryAnalyzer.invoke(question, config); if ("tool_calls" in response.additional_kwargs && response.additional_kwargs.tool_calls !== undefined) { const query = await outputParser.invoke(response, config); return retriever.invoke(query[0].query, config); } else { return response; }
} const customChain = new RunnableLambda({ func: chain });
``` ---------------------------------------- TITLE: Logging ChatOpenAI Response Content (JavaScript)
DESCRIPTION: Following an invocation, this code shows how to access and print the actual content of the AI's response. The `aiMsg` object contains the model's output, and its `content` property holds the generated text.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#_snippet_4 LANGUAGE: javascript
CODE:
```
console.log(aiMsg.content)
``` ---------------------------------------- TITLE: Defining a Retrieval Tool for LangChain (JS)
DESCRIPTION: This snippet defines a `retrieve` tool using `@langchain/core/tools` and `zod` for schema validation. The tool performs a similarity search on a `vectorStore` based on a given query, serializes the retrieved documents, and returns both the serialized content and the raw documents. This tool is designed to be integrated into a LangGraph workflow for context retrieval.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#_snippet_7 LANGUAGE: javascript
CODE:
```
import { z } from "zod";
import { tool } from "@langchain/core/tools"; const retrieveSchema = z.object({query: z.string()}); const retrieve = tool( async ({ query }) => { const retrievedDocs = await vectorStore.similaritySearch(query, 2); const serialized = retrievedDocs.map( doc => `Source: ${doc.metadata.source}\nContent: ${doc.pageContent}` ).join("\n"); return [ serialized, retrievedDocs, ]; }, { name: "retrieve", description: "Retrieve information related to a query.", schema: retrieveSchema, responseFormat: "content_and_artifact", }
);
``` ---------------------------------------- TITLE: Setting OpenAI API Key Environment Variable in Bash
DESCRIPTION: This command sets the `OPENAI_API_KEY` environment variable, which is required for authenticating with the OpenAI API. Replace `your-api-key` with your actual OpenAI API key before running.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-openai/README.md#_snippet_2 LANGUAGE: bash
CODE:
```
export OPENAI_API_KEY=your-api-key
``` ---------------------------------------- TITLE: Defining Zod Schema for Multiple Entity Extraction (JavaScript)
DESCRIPTION: This code defines a Zod schema for extracting multiple entities. It first defines a `person` object schema, then nests it within a `dataSchema` as an array of `people`, enabling the extraction of a list of person objects from text.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/extraction.ipynb#_snippet_7 LANGUAGE: javascript
CODE:
```
import { z } from "zod"; const person = z.object({ name: z.optional(z.string()).describe('The name of the person'), hair_color: z.optional(z.string()).describe("The color of the person's hair if known"), height_in_meters: z.number().nullish().describe('Height measured in meters'),
}); const dataSchema = z.object({ people: z.array(person).describe('Extracted data about people'),
});
``` ---------------------------------------- TITLE: Defining an Image Search Tool and Agent Executor in LangChain.js (TSX)
DESCRIPTION: This snippet defines a server-side tool named "Images" for searching and displaying images, integrated with LangChain.js. It uses `createRunnableUI` to stream UI updates (e.g., "Searching..." message and image results) during the tool's execution. The tool's schema specifies `query` and `limit` parameters. Finally, it sets up an `AgentExecutor` using `createToolCallingAgent` with the defined tool.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/generative_ui.mdx#_snippet_0 LANGUAGE: tsx
CODE:
```
"use server"; const tool = tool( async (input, config) => { const stream = await createRunnableUI(config); stream.update(<div>Searching...</div>); const result = await images(input); stream.done( <Images images={result.images_results .map((image) => image.thumbnail) .slice(0, input.limit)} /> ); return `[Returned ${result.images_results.length} images]`; }, { name: "Images", description: "A tool to search for images. input should be a search query.", schema: z.object({ query: z.string().describe("The search query used to search for cats"), limit: z.number().describe("The number of pictures shown to the user"), }), }
); // add LLM, prompt, etc... const tools = [tool]; export const agentExecutor = new AgentExecutor({ agent: createToolCallingAgent({ llm, tools, prompt }), tools,
});
``` ---------------------------------------- TITLE: Creating and Streaming a Chat Chain with OpenAI (TypeScript)
DESCRIPTION: This TypeScript example demonstrates how to construct a LangChain Expression Language (LCEL) chain using `@langchain/core` components (prompt, output parser) and the `@langchain/openai` model. It shows how to define a prompt, initialize a chat model, parse the output, and then stream responses for a given question, illustrating the modularity and streaming capabilities of LangChain.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/langchain-core/README.md#_snippet_2 LANGUAGE: typescript
CODE:
```
import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai"; const prompt = ChatPromptTemplate.fromTemplate( `Answer the following question to the best of your ability:\n{question}`
); const model = new ChatOpenAI({ temperature: 0.8,
}); const outputParser = new StringOutputParser(); const chain = prompt.pipe(model).pipe(outputParser); const stream = await chain.stream({ question: "Why is the sky blue?",
}); for await (const chunk of stream) { console.log(chunk);
} /*
The sky appears blue because of a phenomenon known as Ray
leigh scattering
*/
``` ---------------------------------------- TITLE: Processing Audio Input with Google Gemini 1.5 Pro using LangChain.js
DESCRIPTION: This snippet demonstrates how to send an audio file to Google's Gemini 1.5 Pro model via LangChain.js. It defines a 'summary_tool' to process the model's output, converts an audio file from a URL to a base64 string, and then invokes the model with a system message and the base64-encoded audio content. The expected output is a tool call containing the summary.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calls_multimodal.ipynb#_snippet_4 LANGUAGE: javascript
CODE:
```
import { SystemMessage } from "@langchain/core/messages";
import { tool } from "@langchain/core/tools"; const summaryTool = tool((input) => { return input.summary;
}, { name: "summary_tool", description: "Log the summary of the content", schema: z.object({ summary: z.string().describe("The summary of the content to log") }),
}); const audioUrl = "https://www.pacdv.com/sounds/people_sound_effects/applause-1.wav"; const axiosRes = await axios.get(audioUrl, { responseType: "arraybuffer" });
const base64 = btoa( new Uint8Array(axiosRes.data).reduce( (data, byte) => data + String.fromCharCode(byte), '' )
); const model = new ChatGoogleGenerativeAI({ model: "gemini-1.5-pro-latest" }).bindTools([summaryTool]); const response = await model.invoke([ new SystemMessage("Summarize this content. always use the summary_tool in your response"), new HumanMessage({ content: [{ type: "media", mimeType: "audio/wav", data: base64, }]
})]); console.log(response.tool_calls);
``` ---------------------------------------- TITLE: Invoking a LangChain Retriever (TypeScript)
DESCRIPTION: This snippet demonstrates how to invoke a LangChain retriever. Retrievers provide a standard interface to connect to various data services or databases, such as vector stores. The `invoke` method is used to query the retriever, which then returns a list of `Document` objects containing relevant `pageContent` and `metadata`.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/why_langchain.mdx#_snippet_2 LANGUAGE: typescript
CODE:
```
const documents = await myRetriever.invoke("What is the meaning of life?");
``` ---------------------------------------- TITLE: Indexing and Retrieving Data with MemoryVectorStore in LangChain (JavaScript)
DESCRIPTION: This snippet demonstrates how to index a sample document into a `MemoryVectorStore` using an `embeddings` object and then retrieve the most similar text using the vector store configured as a retriever. It shows the full RAG flow from document ingestion to retrieval.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ibm.ipynb#_snippet_8 LANGUAGE: javascript
CODE:
```
// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory"; const text = "LangChain is the framework for building context-aware reasoning applications"; const vectorstore = await MemoryVectorStore.fromDocuments( [{ pageContent: text, metadata: {} }], embeddings,
); // Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1); // Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?"); retrievedDocuments[0].pageContent;
``` ---------------------------------------- TITLE: Fetching RAG Prompt Template with LangChain Hub (TypeScript)
DESCRIPTION: This snippet demonstrates how to fetch a pre-defined RAG prompt template from the LangChain Hub. It shows how to import necessary modules, pull the template, and then invoke it with example context and question to see the generated messages. This template is crucial for structuring the input to the language model during the generation phase.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#_snippet_11 LANGUAGE: TypeScript
CODE:
```
import { pull } from "langchain/hub";
import { ChatPromptTemplate } from "@langchain/core/prompts"; const promptTemplate = await pull<ChatPromptTemplate>("rlm/rag-prompt"); // Example:
const example_prompt = await promptTemplate.invoke( { context: "(context goes here)", question: "(question goes here)" }
)
const example_messages = example_prompt.messages console.assert(example_messages.length === 1);
example_messages[0].content
``` ---------------------------------------- TITLE: Assembling ReAct Agent with LangGraph (TypeScript)
DESCRIPTION: This snippet uses LangGraph's `createReactAgent` function to assemble the conversational agent. It integrates the initialized language model (`llm`), the defined tools, and the conversational prompt (`messageModifier`) to create a tool-calling agent capable of reasoning and action.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_tools.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
import { createReactAgent } from "@langchain/langgraph/prebuilt" // messageModifier allows you to preprocess the inputs to the model inside ReAct agent
// in this case, since we're passing a prompt string, we'll just always add a SystemMessage
// with this prompt string before any other messages sent to the model
const agent = createReactAgent({ llm, tools, messageModifier: prompt })
``` ---------------------------------------- TITLE: Building a Full RAG Chain with Chat History - LangChain.js
DESCRIPTION: This snippet constructs the complete RAG chain. It first defines a `systemPrompt` for the QA model and a `qaPrompt` using `ChatPromptTemplate` with `MessagesPlaceholder` for chat history. Then, it creates a `questionAnswerChain` using `createStuffDocumentsChain` to combine retrieved documents with the prompt. Finally, it assembles the `ragChain` using `createRetrievalChain`, integrating the `historyAwareRetriever` and the `questionAnswerChain` to provide context-aware answers.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#_snippet_7 LANGUAGE: JavaScript
CODE:
```
import { createStuffDocumentsChain } from "langchain/chains/combine_documents";
import { createRetrievalChain } from "langchain/chains/retrieval"; const systemPrompt = "You are an assistant for question-answering tasks. " + "Use the following pieces of retrieved context to answer " + "the question. If you don't know the answer, say that you " + "don't know. Use three sentences maximum and keep the " + "answer concise." + "\n\n" + "{context}"; const qaPrompt = ChatPromptTemplate.fromMessages([ ["system", systemPrompt], new MessagesPlaceholder("chat_history"), ["human", "{input}"],
]); const questionAnswerChain = await createStuffDocumentsChain({ llm, prompt: qaPrompt,
}); const ragChain = await createRetrievalChain({ retriever: historyAwareRetriever, combineDocsChain: questionAnswerChain,
});
``` ---------------------------------------- TITLE: Chaining a LangChain Tool with a Language Model
DESCRIPTION: This comprehensive example demonstrates how to integrate the tool into a LangChain runnable chain. It defines a prompt, binds the tool to the language model, and creates a `RunnableLambda` to handle user input, model responses, and tool invocations in a conversational flow.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/tools.ipynb#_snippet_6 LANGUAGE: typescript
CODE:
```
import { HumanMessage } from "@langchain/core/messages";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableLambda } from "@langchain/core/runnables"; const prompt = ChatPromptTemplate.fromMessages( [ ["system", "You are a helpful assistant."], ["placeholder", "{messages}"] ]
); const llmWithTools = llm.bindTools([tool]); const chain = prompt.pipe(llmWithTools); const toolChain = RunnableLambda.from( async (userInput: string, config) => { const humanMessage = new HumanMessage(userInput,); const aiMsg = await chain.invoke({ messages: [new HumanMessage(userInput)] }, config); const toolMsgs = await tool.batch(aiMsg.tool_calls, config); return chain.invoke({ messages: [humanMessage, aiMsg, ...toolMsgs] }, config); }
); const toolChainResult = await toolChain.invoke("what is the current weather in sf?");
``` ---------------------------------------- TITLE: Complete Conversational RAG Agent Setup in LangChain.js
DESCRIPTION: This comprehensive snippet ties together all components for a conversational RAG agent. It includes initializing an LLM, loading and splitting documents, creating a vector store and retriever, defining a retriever tool, and finally constructing a stateful agent executor with memory.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#_snippet_20 LANGUAGE: JavaScript
CODE:
```
import { createRetrieverTool } from "langchain/tools/retriever";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { MemorySaver } from "@langchain/langgraph";
import { ChatOpenAI } from "@langchain/openai";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { MemoryVectorStore } from "langchain/vectorstores/memory"
import { OpenAIEmbeddings } from "@langchain/openai"; const llm3 = new ChatOpenAI({ model: "gpt-4o" }); const loader3 = new CheerioWebBaseLoader( "https://lilianweng.github.io/posts/2023-06-23-agent/"
); const docs3 = await loader3.load(); const textSplitter3 = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });
const splits3 = await textSplitter3.splitDocuments(docs3);
const vectorStore3 = await MemoryVectorStore.fromDocuments(splits3, new OpenAIEmbeddings()); // Retrieve and generate using the relevant snippets of the blog.
const retriever3 = vectorStore3.asRetriever(); const tool2 = createRetrieverTool( retriever3, { name: "blog_post_retriever", description: "Searches and returns excerpts from the Autonomous Agents blog post.", }
)
const tools2 = [tool2]
const memory4 = new MemorySaver(); const agentExecutor3 = createReactAgent({ llm: llm3, tools: tools2, checkpointSaver: memory4 })
``` ---------------------------------------- TITLE: Installing LangChain Core Package (npm)
DESCRIPTION: This command installs the main 'langchain' package along with '@langchain/core', which serves as a foundational dependency. Note that this installation does not include dependencies for specific integrations, which must be installed separately.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/installation.mdx#_snippet_0 LANGUAGE: bash
CODE:
```
npm install langchain @langchain/core
``` ---------------------------------------- TITLE: Implementing Conversation Pre-processing with LangGraph in TypeScript
DESCRIPTION: This snippet demonstrates how to build a conversational AI application using LangGraph, incorporating message trimming for conversation pre-processing. It defines a `callModel` function to interact with `ChatOpenAI` and trim messages based on a simple count, then constructs a `StateGraph` with a `MemorySaver` to maintain conversation history across turns. The example shows how to stream responses for initial and follow-up messages, ensuring the model remembers context.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/conversation_buffer_window_memory.ipynb#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { v4 as uuidv4 } from 'uuid';
import { ChatOpenAI } from "@langchain/openai";
import { StateGraph, MessagesAnnotation, END, START, MemorySaver } from "@langchain/langgraph";
import { trimMessages } from "@langchain/core/messages"; // Define a chat model
const model = new ChatOpenAI({ model: "gpt-4o" }); // Define the function that calls the model
const callModel = async (state: typeof MessagesAnnotation.State): Promise<Partial<typeof MessagesAnnotation.State>> => { // highlight-start const selectedMessages = await trimMessages( state.messages, { tokenCounter: (messages) => messages.length, // Simple message count instead of token count maxTokens: 5, // Allow up to 5 messages strategy: "last", startOn: "human", includeSystem: true, allowPartial: false, } ); // highlight-end const response = await model.invoke(selectedMessages); // With LangGraph, we're able to return a single message, and LangGraph will concatenate // it to the existing list return { messages: [response] };
}; // Define a new graph
const workflow = new StateGraph(MessagesAnnotation)
// Define the two nodes we will cycle between .addNode("model", callModel) .addEdge(START, "model") .addEdge("model", END) const app = workflow.compile({ // Adding memory is straightforward in LangGraph! // Just pass a checkpointer to the compile method. checkpointer: new MemorySaver()
}); // The thread id is a unique key that identifies this particular conversation
// ---
// NOTE: this must be `thread_id` and not `threadId` as the LangGraph internals expect `thread_id`
// ---
const thread_id = uuidv4();
const config = { configurable: { thread_id }, streamMode: "values" as const }; const inputMessage = { role: "user", content: "hi! I'm bob",
}
for await (const event of await app.stream({ messages: [inputMessage] }, config)) { const lastMessage = event.messages[event.messages.length - 1]; console.log(lastMessage.content);
} // Here, let's confirm that the AI remembers our name!
const followUpMessage = { role: "user", content: "what was my name?",
} // ---
// NOTE: You must pass the same thread id to continue the conversation
// we do that here by passing the same `config` object to the `.stream` call.
// ---
for await (const event of await app.stream({ messages: [followUpMessage] }, config)) { const lastMessage = event.messages[event.messages.length - 1]; console.log(lastMessage.content);
}
``` ---------------------------------------- TITLE: Implementing RAG with LangChain, Anthropic, and XML Output Parsing
DESCRIPTION: This comprehensive JavaScript snippet demonstrates how to build a Retrieval-Augmented Generation (RAG) system using LangChain with Anthropic's Claude model. It defines a chat prompt for a Q&A system with citations, formats retrieved documents into XML, and chains these components to generate answers with verifiable quotes and source IDs. The `XMLOutputParser` is used to structure the LLM's response.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#_snippet_10 LANGUAGE: javascript
CODE:
```
import { ChatAnthropic } from "@langchain/anthropic";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { XMLOutputParser } from "@langchain/core/output_parsers";
import { Document } from "@langchain/core/documents";
import { RunnableLambda, RunnablePassthrough, RunnableMap } from "@langchain/core/runnables"; const anthropic = new ChatAnthropic({ model: "claude-instant-1.2", temperature: 0,
});
const system = `You're a helpful AI assistant. Given a user question and some web article snippets,
answer the user question and provide citations. If none of the articles answer the question, just say you don't know. Remember, you must return both an answer and citations. A citation consists of a VERBATIM quote that
justifies the answer and the ID of the quote article. Return a citation for every quote across all articles
that justify the answer. Use the following format for your final output: <cited_answer> <answer></answer> <citations> <citation><source_id></source_id><quote></quote></citation> <citation><source_id></source_id><quote></quote></citation> ... </citations>
</cited_answer> Here are the web articles:{context}`; const anthropicPrompt = ChatPromptTemplate.fromMessages([ ["system", system], ["human", "{question}"]
]); const formatDocsToXML = (docs: Array<Document>): string => { const formatted: Array<string> = []; docs.forEach((doc, idx) => { const docStr = `<source id="${idx}"> <title>${doc.metadata.title}</title> <article_snippet>${doc.pageContent}</article_snippet>
</source>` formatted.push(docStr); }); return `\n\n<sources>${formatted.join("\n")}</sources>`;
} const format3 = new RunnableLambda({ func: (input: { docs: Array<Document> }) => formatDocsToXML(input.docs)
})
const answerChain = anthropicPrompt .pipe(anthropic) .pipe(new XMLOutputParser()) .pipe( new RunnableLambda({ func: (input: { cited_answer: any }) => input.cited_answer }) );
const map3 = RunnableMap.from({ question: new RunnablePassthrough(), docs: retriever,
});
const chain3 = map3.assign({ context: format3 }).assign({ cited_answer: answerChain }).pick(["cited_answer", "docs"]) const res = await chain3.invoke("How fast are cheetahs?"); console.log(JSON.stringify(res, null, 2));
``` ---------------------------------------- TITLE: Handling Recursion Limit in LangGraph (JavaScript)
DESCRIPTION: This snippet shows how to control the execution depth in LangGraph using the `recursionLimit` parameter during invocation. It also demonstrates how to catch a `GraphRecursionError` specifically when the limit is reached, providing a robust way to manage agent execution and prevent infinite loops.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/migrate_agent.ipynb#_snippet_15 LANGUAGE: JavaScript
CODE:
```
import { GraphRecursionError } from "@langchain/langgraph"; const RECURSION_LIMIT = 2 * 2 + 1; const appWithBadTools = createReactAgent({ llm, tools: badTools }); try { await appWithBadTools.invoke({ messages: [ { role: "user", content: query } ] }, { recursionLimit: RECURSION_LIMIT, });
} catch (e) { if (e instanceof GraphRecursionError) { console.log("Recursion limit reached."); } else { throw e; }
}
``` ---------------------------------------- TITLE: Batching a Runnable in LangChain (TypeScript)
DESCRIPTION: Illustrates how to process multiple inputs concurrently using the `batch()` method of a `Runnable`. This is efficient for handling arrays of data, applying the runnable's logic to each element in parallel and returning an array of results.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { RunnableLambda } from "@langchain/core/runnables"; const runnable = RunnableLambda.from((x: number) => x.toString()); await runnable.batch([7, 8, 9]);
``` ---------------------------------------- TITLE: Setting Up Document Loading, Splitting, and Vector Store (JavaScript)
DESCRIPTION: This comprehensive snippet sets up the data ingestion pipeline for the RAG application. It uses CheerioWebBaseLoader to load content from a specified URL, RecursiveCharacterTextSplitter to divide the document into manageable chunks, and MemoryVectorStore with OpenAIEmbeddings to store and index these chunks in memory, finally creating a retriever for efficient document lookup.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#_snippet_4 LANGUAGE: javascript
CODE:
```
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "@langchain/openai"; const loader = new CheerioWebBaseLoader( "https://lilianweng.github.io/posts/2023-06-23-agent/"
); const docs = await loader.load(); const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });
const splits = await textSplitter.splitDocuments(docs);
const vectorStore = await MemoryVectorStore.fromDocuments(splits, new OpenAIEmbeddings()); // Retrieve and generate using the relevant snippets of the blog.
const retriever = vectorStore.asRetriever();
``` ---------------------------------------- TITLE: Implementing Tool Selection and Execution Chain in LangChain
DESCRIPTION: This snippet constructs a LangChain runnable chain that enables a model to select and execute a tool. It defines a `toolChain` that maps model output to the correct tool and its arguments, then integrates this with a chat prompt, a language model, and a JSON output parser. The chain processes user input, selects a tool, and invokes it, demonstrating dynamic tool usage.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_prompting.ipynb#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { StructuredToolInterface } from "@langchain/core/tools" const tools = [addTool, exponentiateTool, multiplyTool] const toolChain = (modelOutput) => { const toolMap: Record<string, StructuredToolInterface> = Object.fromEntries(tools.map(tool => [tool.name, tool])) const chosenTool = toolMap[modelOutput.name] return new RunnablePick("arguments").pipe(new RunnableLambda({ func: (input) => chosenTool.invoke({ first_int: input[0], second_int: input[1] }) }))
}
const toolChainRunnable = new RunnableLambda({ func: toolChain
}) const renderedTools = renderTextDescription(tools)
const systemPrompt = `You are an assistant that has access to the following set of tools. Here are the names and descriptions for each tool: {rendered_tools} Given the user input, return the name and input of the tool to use. Return your response as a JSON blob with 'name' and 'arguments' keys.` const prompt = ChatPromptTemplate.fromMessages( [["system", systemPrompt], ["user", "{input}"]]
)
const chain = prompt.pipe(model).pipe(new JsonOutputParser()).pipe(toolChainRunnable)
await chain.invoke({ input: "what's 3 plus 1132", rendered_tools: renderedTools })
``` ---------------------------------------- TITLE: Calling Tools with Anthropic Chat Model (JavaScript)
DESCRIPTION: This snippet illustrates tool calling with `ChatAnthropic` using a base64-encoded image. It reads an image file, initializes a `claude-3-sonnet-20240229` model, and binds the `weatherTool`. A `HumanMessage` is created with text and the base64-encoded image data as an `image_url` type. The model is invoked, and its tool calls are printed, showcasing Anthropic's multimodal tool-calling capability.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calls_multimodal.ipynb#_snippet_2 LANGUAGE: javascript
CODE:
```
import * as fs from "node:fs/promises"; import { ChatAnthropic } from "@langchain/anthropic";
import { HumanMessage } from "@langchain/core/messages"; const imageData = await fs.readFile("../../data/sunny_day.jpeg"); const model = new ChatAnthropic({ model: "claude-3-sonnet-20240229",
}).bindTools([weatherTool]); const message = new HumanMessage({ content: [ { type: "text", text: "describe the weather in this image", }, { type: "image_url", image_url: { url: `data:image/jpeg;base64,${imageData.toString("base64")}`, }, }, ],
}); const response = await model.invoke([message]); console.log(response.tool_calls);
``` ---------------------------------------- TITLE: Setting OpenAI API Key (TypeScript)
DESCRIPTION: This snippet shows how to set the OpenAI API key as an environment variable. This key is necessary if you are using OpenAI embeddings with your vector store, allowing the application to authenticate with the OpenAI API. Replace "YOUR_API_KEY" with your actual OpenAI API key.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/vectorstores.ipynb#_snippet_1 LANGUAGE: typescript
CODE:
```
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
``` ---------------------------------------- TITLE: Setting OpenAI API Key in TypeScript
DESCRIPTION: This snippet demonstrates how to set the OpenAI API key as an environment variable. This key is essential for authenticating requests when using OpenAI embeddings with HNSWLib.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hnswlib.ipynb#_snippet_0 LANGUAGE: typescript
CODE:
```
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
``` ---------------------------------------- TITLE: Implementing Reciprocal Rank Fusion Function (TypeScript)
DESCRIPTION: This snippet defines the `reciprocalRankFusion` function, which combines and re-ranks search results from multiple queries. It calculates a fused score for each unique document based on its rank across all result sets, using the formula `1 / (index + k)`. The function then sorts documents by their fused scores in descending order and returns them as an array of `Document` objects, effectively merging and re-ranking the search outcomes.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rag_fusion.ipynb#_snippet_8 LANGUAGE: TypeScript
CODE:
```
const reciprocalRankFusion = (results: Document[][], k = 60) => { const fusedScores: Record<string, number> = {}; for (const result of results) { // Assumes the docs are returned in sorted order of relevance result.forEach((item, index) => { const docString = item.pageContent; if (!(docString in fusedScores)) { fusedScores[docString] = 0; } fusedScores[docString] += 1 / (index + k); }); } const rerankedResults = Object.entries(fusedScores) .sort((a, b) => b[1] - a[1]) .map( ([doc, score]) => new Document({ pageContent: doc, metadata: { score } }) ); return rerankedResults;
};
``` ---------------------------------------- TITLE: Implementing a Conversational Retrieval Chain with LangChain.js
DESCRIPTION: This comprehensive code snippet demonstrates the end-to-end process of building a conversational retrieval-augmented generation (RAG) system using LangChain.js. It covers loading documents from a URL, splitting them into chunks, embedding them into a vector store, setting up a history-aware retriever, defining a question-answering chain, and integrating these components into a LangGraph workflow for stateful conversational interactions. It showcases how to handle chat history to contextualize follow-up questions and retrieve relevant information.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#_snippet_12 LANGUAGE: JavaScript
CODE:
```
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings, ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
import { createHistoryAwareRetriever } from "langchain/chains/history_aware_retriever";
import { createStuffDocumentsChain } from "langchain/chains/combine_documents";
import { createRetrievalChain } from "langchain/chains/retrieval";
import { AIMessage, BaseMessage, HumanMessage } from "@langchain/core/messages";
import { StateGraph, START, END, MemorySaver, messagesStateReducer, Annotation } from "@langchain/langgraph";
import { v4 as uuidv4 } from "uuid"; const llm2 = new ChatOpenAI({ model: "gpt-4o" }); const loader2 = new CheerioWebBaseLoader( "https://lilianweng.github.io/posts/2023-06-23-agent/"
); const docs2 = await loader2.load(); const textSplitter2 = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });
const splits2 = await textSplitter2.splitDocuments(docs2);
const vectorStore2 = await MemoryVectorStore.fromDocuments(splits2, new OpenAIEmbeddings()); // Retrieve and generate using the relevant snippets of the blog.
const retriever2 = vectorStore2.asRetriever(); const contextualizeQSystemPrompt2 = "Given a chat history and the latest user question " + "which might reference context in the chat history, " + "formulate a standalone question which can be understood " + "without the chat history. Do NOT answer the question, " + "just reformulate it if needed and otherwise return it as is."; const contextualizeQPrompt2 = ChatPromptTemplate.fromMessages( [ ["system", contextualizeQSystemPrompt2], new MessagesPlaceholder("chat_history"), ["human", "{input}"], ]
); const historyAwareRetriever2 = await createHistoryAwareRetriever({ llm: llm2, retriever: retriever2, rephrasePrompt: contextualizeQPrompt2
}); const systemPrompt2 = "You are an assistant for question-answering tasks. " + "Use the following pieces of retrieved context to answer " + "the question. If you don't know the answer, say that you " + "don't know. Use three sentences maximum and keep the " + "answer concise." + "\n\n" + "{context}"; const qaPrompt2 = ChatPromptTemplate.fromMessages([ ["system", systemPrompt2], new MessagesPlaceholder("chat_history"), ["human", "{input}"],
]); const questionAnswerChain2 = await createStuffDocumentsChain({ llm: llm2, prompt: qaPrompt2,
}); const ragChain2 = await createRetrievalChain({ retriever: historyAwareRetriever2, combineDocsChain: questionAnswerChain2,
}); // Define the State interface
const GraphAnnotation2 = Annotation.Root({ input: Annotation<string>(), chat_history: Annotation<BaseMessage[]>({ reducer: messagesStateReducer, default: () => [], }), context: Annotation<string>(), answer: Annotation<string>()
}); // Define the call_model function
async function callModel2(state: typeof GraphAnnotation2.State) { const response = await ragChain2.invoke(state); return { chat_history: [ new HumanMessage(state.input), new AIMessage(response.answer), ], context: response.context, answer: response.answer, };
} // Create the workflow
const workflow2 = new StateGraph(GraphAnnotation2) .addNode("model", callModel2) .addEdge(START, "model") .addEdge("model", END); // Compile the graph with a checkpointer object
const memory2 = new MemorySaver();
const app2 = workflow2.compile({ checkpointer: memory2 }); const threadId2 = uuidv4();
const config2 = { configurable: { thread_id: threadId2 } }; const result3 = await app2.invoke( { input: "What is Task Decomposition?" }, config2,
);
console.log(result3.answer); const result4 = await app2.invoke( { input: "What is one way of doing it?" }, config2,
);
console.log(result4.answer);
``` ---------------------------------------- TITLE: Initializing and Invoking ChatOpenAI Model in TypeScript
DESCRIPTION: This TypeScript snippet demonstrates how to initialize `ChatOpenAI` with an API key and a specific model name, then invoke it with a `HumanMessage`. It's the standard way to interact with OpenAI chat models via LangChain.js.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-openai/README.md#_snippet_3 LANGUAGE: typescript
CODE:
```
import { ChatOpenAI } from "@langchain/openai"; const model = new ChatOpenAI({ apiKey: process.env.OPENAI_API_KEY, modelName: "gpt-4-1106-preview",
});
const response = await model.invoke(new HumanMessage("Hello world!"));
``` ---------------------------------------- TITLE: Full Azure Cosmos DB Vector Store Usage Example (TypeScript)
DESCRIPTION: Provides a comprehensive example demonstrating the full lifecycle of using Azure Cosmos DB as a LangChain vector store. This includes initializing embeddings and an LLM, loading and splitting documents, adding them to the vector store, performing a similarity search, and finally using a retrieval chain to answer a question based on the retrieved context.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/azure_cosmosdb_nosql.mdx#_snippet_3 LANGUAGE: typescript
CODE:
```
import { AzureCosmosDBVectorStore } from "@langchain/azure-cosmosdb";
import { OpenAIEmbeddings, ChatOpenAI } from "@langchain/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { PromptTemplate } from "@langchain/core/prompts";
import { RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { Document } from "@langchain/core/documents"; // Ensure environment variables are set:
// AZURE_COSMOSDB_CONNECTION_STRING, AZURE_COSMOSDB_DATABASE_ID, AZURE_COSMOSDB_CONTAINER_ID
// OPENAI_API_KEY async function runExample() { // 1. Initialize Embeddings and LLM const embeddings = new OpenAIEmbeddings(); const llm = new ChatOpenAI({ temperature: 0, modelName: "gpt-3.5-turbo" }); // 2. Initialize Vector Store const vectorStore = new AzureCosmosDBVectorStore({ connectionString: process.env.AZURE_COSMOSDB_CONNECTION_STRING, databaseName: process.env.AZURE_COSMOSDB_DATABASE_ID, containerName: process.env.AZURE_COSMOSDB_CONTAINER_ID, embeddings // Pass the embeddings instance }); // 3. Load and Split Documents const text = `LangChain is a framework for developing applications powered by language models. It enables applications that:\n\n1. Are data-aware: connect a language model to other sources of data\n2. Are agentic: allow a language model to interact with its environment\n\nThese types of applications are distinct from the traditional standalone LLM calls in that they require a combination of a language model with other sources of data or computation.`; const splitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 }); const docs = await splitter.createDocuments([text]); // 4. Add documents to vector store console.log("Adding documents to Cosmos DB..."); await vectorStore.addDocuments(docs); console.log("Documents added."); // 5. Perform a vector search const query = "What is LangChain?"; console.log(`Performing vector search for: "${query}"`); const retrievedDocs = await vectorStore.similaritySearch(query, 1); console.log("Retrieved documents:", retrievedDocs); // 6. Create a Q&A chain const prompt = PromptTemplate.fromTemplate( `Answer the question based only on the following context:\n{context}\n\nQuestion: {question}` ); const chain = RunnableSequence.from([ { context: async (input: { question: string }) => { const docs = await vectorStore.asRetriever().invoke(input.question); return docs.map((doc: Document) => doc.pageContent).join("\n\n"); }, question: (input: { question: string }) => input.question }, prompt, llm, new StringOutputParser() ]); // 7. Invoke the chain console.log(`Answering question: "${query}"`); const answer = await chain.invoke({ question: query }); console.log("Answer:", answer);
} runExample().catch(console.error);
``` ---------------------------------------- TITLE: Using Built-in Google Search Retrieval Tool (TypeScript)
DESCRIPTION: This snippet demonstrates how to integrate the built-in GoogleSearchRetrievalTool with a ChatGoogleGenerativeAI model. It configures the tool with DynamicRetrievalMode.MODE_DYNAMIC and a dynamicThreshold, then binds it to the model. The example shows invoking the model with a query that leverages the search tool to ground content generation in real-world information, and then logs the generated content.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_generativeai.ipynb#_snippet_8 LANGUAGE: TypeScript
CODE:
```
import { DynamicRetrievalMode, GoogleSearchRetrievalTool } from "@google/generative-ai";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai"; const searchRetrievalTool: GoogleSearchRetrievalTool = { googleSearchRetrieval: { dynamicRetrievalConfig: { mode: DynamicRetrievalMode.MODE_DYNAMIC, dynamicThreshold: 0.7, // default is 0.7 } }
};
const searchRetrievalModel = new ChatGoogleGenerativeAI({ model: "gemini-1.5-pro", temperature: 0, maxRetries: 0,
}).bindTools([searchRetrievalTool]); const searchRetrievalResult = await searchRetrievalModel.invoke("Who won the 2024 MLB World Series?"); console.log(searchRetrievalResult.content);
``` ---------------------------------------- TITLE: Invoking SelfQueryRetriever for Metadata-Filtered Query in TypeScript
DESCRIPTION: This snippet shows how to use the `selfQueryRetriever` to execute a natural language query that implicitly filters results based on document metadata. The query 'Which movies are rated higher than 8.5?' will be translated by the retriever into a structured filter, which is then applied to the vector store to retrieve relevant documents. The `invoke` method returns the filtered and retrieved documents.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/hnswlib.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
await selfQueryRetriever.invoke( "Which movies are rated higher than 8.5?"
);
``` ---------------------------------------- TITLE: Initializing and Invoking LangChain Chat Model
DESCRIPTION: This TypeScript snippet demonstrates how to import and initialize the chat model class from the package, using an API key from environment variables. It then shows how to invoke the model with a `HumanMessage` to get a response.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/create-langchain-integration/template/README.md#_snippet_3 LANGUAGE: typescript
CODE:
```
import { <ADD_CLASS_NAME_HERE> } from "@langchain/<ADD_PACKAGE_NAME_HERE>"; const model = new ExampleChatClass({ apiKey: process.env.EXAMPLE_API_KEY,
});
const response = await model.invoke(new HumanMessage("Hello world!"));
``` ---------------------------------------- TITLE: Creating and Invoking a String PromptTemplate in LangChain.js
DESCRIPTION: This snippet demonstrates how to create a `PromptTemplate` from a string template using `PromptTemplate.fromTemplate` and then invoke it with a variable. It shows how user input for a `topic` variable is formatted into a single string prompt for a language model.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/prompt_templates.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { PromptTemplate } from "@langchain/core/prompts"; const promptTemplate = PromptTemplate.fromTemplate( "Tell me a joke about {topic}"
); await promptTemplate.invoke({ topic: "cats" });
``` ---------------------------------------- TITLE: Initializing ChatOpenAI Chat Model in JavaScript
DESCRIPTION: This snippet initializes a ChatOpenAI instance, a chat model from LangChain, for use in conversational AI applications. It configures the model to 'gpt-4o-mini' and sets the temperature to 0 for deterministic outputs. This model serves as the core language understanding and generation component.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
import { ChatOpenAI } from '@langchain/openai'; const llm = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0,
})
``` ---------------------------------------- TITLE: Creating a Custom Tool with `tool` Function in TypeScript
DESCRIPTION: This snippet demonstrates how to define a custom tool using the `tool` function from `@langchain/core/tools`. It encapsulates a multiplication function, providing a name, description, and a Zod schema for its input arguments (`a` and `b`), which are both numbers. This is the recommended way to create tools in LangChain.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tools.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const multiply = tool( ({ a, b }: { a: number; b: number }): number => { /** * Multiply two numbers. */ return a * b; }, { name: "multiply", description: "Multiply two numbers", schema: z.object({ a: z.number(), b: z.number(), }), }
);
``` ---------------------------------------- TITLE: Implementing Retrieval and Generation Nodes for LangGraph (TypeScript)
DESCRIPTION: This snippet defines two asynchronous functions, 'retrieve' and 'generate', which serve as nodes in a LangGraph application. The 'retrieve' function performs a similarity search on a 'vectorStore' using the input question to get relevant documents. The 'generate' function takes the retrieved context and the original question, formats them using a 'promptTemplate', and then invokes a language model ('llm') to produce an answer. These functions represent the core logic of the RAG pipeline.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#_snippet_13 LANGUAGE: TypeScript
CODE:
```
import { concat } from "@langchain/core/utils/stream"; const retrieve = async (state: typeof InputStateAnnotation.State) => { const retrievedDocs = await vectorStore.similaritySearch(state.question) return { context: retrievedDocs };
}; const generate = async (state: typeof StateAnnotation.State) => { const docsContent = state.context.map(doc => doc.pageContent).join("\n"); const messages = await promptTemplate.invoke({ question: state.question, context: docsContent }); const response = await llm.invoke(messages); return { answer: response.content };
};
``` ---------------------------------------- TITLE: Building a Retrieval-Augmented Generation Chain with Document Post-processing (TypeScript)
DESCRIPTION: This snippet illustrates how to construct a LangChain expression language (LCEL) chain that integrates document retrieval and post-processing with an answer generation step. It uses `assign` to add `context` and `answer` to the chain's output and `pick` to select specific fields, demonstrating how to invoke the complete chain and log the result.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#_snippet_12 LANGUAGE: TypeScript
CODE:
```
const chain4 = retrieveMap .assign({ context: formatDocs }) .assign({ answer: answerChain }) .pick(["answer", "docs"]); // Note the documents have an article "summary" in the metadata that is now much longer than the
// actual document page content. This summary isn't actually passed to the model.
const res = await chain4.invoke("How fast are cheetahs?"); console.log(JSON.stringify(res, null, 2))
``` ---------------------------------------- TITLE: Installing LangChain with npm
DESCRIPTION: This command installs the main LangChain library using npm. The `-S` flag saves the package as a dependency in your `package.json` file, making it a required component for your project.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/index.mdx#_snippet_0 LANGUAGE: bash
CODE:
```
npm install -S langchain
``` ---------------------------------------- TITLE: Building LangChain Runnable Sequence for Query Analysis
DESCRIPTION: This snippet constructs a `RunnableSequence` for the query analyzer. It defines a system prompt instructing the LLM on its role, creates a `ChatPromptTemplate` incorporating system, examples, and human input, and then binds the `llm` with the `searchSchema` for structured output. The sequence orchestrates the flow from input question to prompt formatting and LLM invocation, preparing it for query generation.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_few_shot.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts"
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables" const system = `You are an expert at converting user questions into database queries.
You have access to a database of tutorial videos about a software library for building LLM-powered applications.
Given a question, return a list of database queries optimized to retrieve the most relevant results. If there are acronyms or words you are not familiar with, do not try to rephrase them.` const prompt = ChatPromptTemplate.fromMessages(
[ ["system", system], ["placeholder", "{examples}"], ["human", "{question}"],
]
)
const llmWithTools = llm.withStructuredOutput(searchSchema, { name: "Search",
})
const queryAnalyzer = RunnableSequence.from([ { question: new RunnablePassthrough(), }, prompt, llmWithTools
]);
``` ---------------------------------------- TITLE: Chaining TogetherAI LLM with PromptTemplate (JavaScript)
DESCRIPTION: This snippet illustrates how to create a chain by piping a `PromptTemplate` to the `TogetherAI` language model. It demonstrates invoking the chain with structured input to generate a language-specific translation.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/together.ipynb#_snippet_4 LANGUAGE: JavaScript
CODE:
```
import { PromptTemplate } from "@langchain/core/prompts" const prompt = PromptTemplate.fromTemplate("How to say {input} in {output_language}:\n") const chain = prompt.pipe(llm);
await chain.invoke( { output_language: "German", input: "I love programming.", }
)
``` ---------------------------------------- TITLE: Invoking Chat Model with Full Conversation History (TypeScript)
DESCRIPTION: This snippet demonstrates how to pass the complete conversation history, including both user and assistant messages, to the `llm`'s `.invoke` method. By providing the full context, the model can correctly answer follow-up questions that depend on previous turns, enabling a conversational chatbot experience.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
await llm.invoke([ { role: "user", content: "Hi! I'm Bob" }, { role: "assistant", content: "Hello Bob! How can I assist you today?" }, { role: "user", content: "What's my name?" }
]);
``` ---------------------------------------- TITLE: Constructing a RAG Chain with LangChain Runnables
DESCRIPTION: This snippet builds a RAG (Retrieval Augmented Generation) chain using RunnableSequence, ChatPromptTemplate, and StringOutputParser. It defines a system template for question-answering, incorporates the retriever for context, and structures the chain to process questions, retrieve context, format the prompt, invoke the LLM, and parse the output.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/convert_runnable_to_tool.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableSequence } from "@langchain/core/runnables"; const SYSTEM_TEMPLATE = `
YouYou are an assistant for question-answering tasks.
Use the below context to answer the question. If
you don't know the answer, say you don't know.
Use three sentences maximum and keep the answer
concise. Answer in the style of {answer_style}. Context: {context}`; const prompt = ChatPromptTemplate.fromMessages([ ["system", SYSTEM_TEMPLATE], ["human", "{question}"],
]); const ragChain = RunnableSequence.from([ { context: (input, config) => retriever.invoke(input.question, config), question: (input) => input.question, answer_style: (input) => input.answer_style, }, prompt, llm, new StringOutputParser(),
]);
``` ---------------------------------------- TITLE: Implementing a Basic RAG Workflow with LangChain and TypeScript
DESCRIPTION: This snippet demonstrates a fundamental Retrieval Augmented Generation (RAG) workflow using LangChain. It shows how to define a system prompt, retrieve relevant documents using a 'retriever' (assumed to be pre-configured), format the prompt with the retrieved context, and then use a 'ChatOpenAI' model to generate a concise answer based on the provided information. This example requires the '@langchain/openai' package and a functional 'retriever' instance.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/rag.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai"; // Define a system prompt that tells the model how to use the retrieved context
const systemPrompt = `You are an assistant for question-answering tasks.
Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say that you don't know.
Use three sentences maximum and keep the answer concise.
Context: {context}:`; // Define a question
const question = "What are the main components of an LLM-powered autonomous agent system?"; // Retrieve relevant documents
const docs = await retriever.invoke(question); // Combine the documents into a single string
const docsText = docs.map((d) => d.pageContent).join(""); // Populate the system prompt with the retrieved context
const systemPromptFmt = systemPrompt.replace("{context}", docsText); // Create a model
const model = new ChatOpenAI({ model: "gpt-4o", temperature: 0,
}); // Generate a response
const questions = await model.invoke([ { role: "system", content: systemPromptFmt, }, { role: "user", content: question, }
]);
``` ---------------------------------------- TITLE: Understanding RunnableSequence Execution Flow in TypeScript
DESCRIPTION: This snippet illustrates the underlying sequential execution logic of a `RunnableSequence`. It explicitly shows how the output of the first runnable (`runnable1`) becomes the input for the second runnable (`runnable2`), demonstrating the step-by-step data flow.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/lcel.mdx#_snippet_2 LANGUAGE: TypeScript
CODE:
```
const output1 = await runnable1.invoke(someInput);
const finalOutput = await runnable2.invoke(output1);
``` ---------------------------------------- TITLE: Extracting Structured Output with Zod Schema in LangChain.js
DESCRIPTION: This snippet demonstrates how to configure ChatOpenAI to produce structured output using the .withStructuredOutput() method. It utilizes a Zod schema to define the expected JSON structure for extracted traits and sets strict: true for precise output formatting, enabling reliable data extraction from model responses.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#_snippet_13 LANGUAGE: TypeScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai"; const traitSchema = z.object({ traits: z.array(z.string()).describe("A list of traits contained in the input")
}); const structuredLlm = new ChatOpenAI({ model: "gpt-4o-mini",
}).withStructuredOutput(traitSchema, { name: "extract_traits", strict: true,
}); await structuredLlm.invoke([{ role: "user", content: `I am 6'5" tall and love fruit.`
}]);
``` ---------------------------------------- TITLE: Defining and Binding a Calculator Tool with Zod Schema (JavaScript)
DESCRIPTION: This snippet demonstrates how to define a custom tool using `@langchain/core/tools` and `zod` for schema validation. It creates a 'calculator' tool that performs basic arithmetic operations and then binds it to a chat model using the `.bindTools()` method. The tool's description and schema are crucial for the LLM to understand its capabilities and arguments.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calling.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
import { tool } from "@langchain/core/tools";
import { z } from "zod"; /** * Note that the descriptions here are crucial, as they will be passed along * to the model along with the class name. */
const calculatorSchema = z.object({ operation: z .enum(["add", "subtract", "multiply", "divide"]) .describe("The type of operation to execute."), number1: z.number().describe("The first number to operate on."), number2: z.number().describe("The second number to operate on."),
}); const calculatorTool = tool(async ({ operation, number1, number2 }) => { // Functions must return strings if (operation === "add") { return `${number1 + number2}`; } else if (operation === "subtract") { return `${number1 - number2}`; } else if (operation === "multiply") { return `${number1 * number2}`; } else if (operation === "divide") { return `${number1 / number2}`; } else { throw new Error("Invalid operation."); }
}, { name: "calculator", description: "Can perform mathematical operations.", schema: calculatorSchema,
}); const llmWithTools = llm.bindTools([calculatorTool]);
``` ---------------------------------------- TITLE: Initializing RunnableSequence in TypeScript
DESCRIPTION: This snippet demonstrates how to create a `RunnableSequence` instance by providing `first` and `last` runnables. It allows chaining multiple runnables where the output of one serves as the input for the next, enabling sequential execution.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/lcel.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { RunnableSequence } from "@langchain/core/runnables";
const chain = new RunnableSequence({ first: runnable1, // Optional, use if you have more than two runnables // middle: [...], last: runnable2,
});
``` ---------------------------------------- TITLE: Performing Direct Similarity Search on MemoryVectorStore
DESCRIPTION: This snippet demonstrates how to perform a direct similarity search on the `MemoryVectorStore`. It defines an optional filter function, then queries the vector store for documents similar to 'biology', limiting results to 2, and logs the content and metadata of the retrieved documents.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/memory.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
const filter = (doc) => doc.metadata.source === "https://example.com"; const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, filter) for (const doc of similaritySearchResults) { console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
``` ---------------------------------------- TITLE: Creating a PipelinePromptTemplate for Reusable Prompts in LangChain (JS)
DESCRIPTION: This snippet demonstrates the creation of a `PipelinePromptTemplate` in LangChain, which enables the reuse of prompt parts. It defines several sub-prompts (`introductionPrompt`, `examplePrompt`, `startPrompt`) and then combines them into a `fullPrompt`. The `PipelinePromptTemplate` orchestrates the formatting of these sub-prompts and passes their outputs as variables to the final prompt.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/prompts_composition.ipynb#_snippet_5 LANGUAGE: JavaScript
CODE:
```
import { PromptTemplate, PipelinePromptTemplate, } from "@langchain/core/prompts"; const fullPrompt = PromptTemplate.fromTemplate(`{introduction} {example} {start}`); const introductionPrompt = PromptTemplate.fromTemplate(
`You are impersonating {person}.`
); const examplePrompt =
PromptTemplate.fromTemplate(`Here's an example of an interaction:
Q: {example_q}
A: {example_a}`); const startPrompt = PromptTemplate.fromTemplate(`Now, do this for real!
Q: {input}
A:`); const composedPrompt = new PipelinePromptTemplate({
pipelinePrompts: [ { name: "introduction", prompt: introductionPrompt, }, { name: "example", prompt: examplePrompt, }, { name: "start", prompt: startPrompt, },
],
finalPrompt: fullPrompt,
});
``` ---------------------------------------- TITLE: Defining Zod Schema for Structured Output in TypeScript
DESCRIPTION: This snippet illustrates how to define a structured output schema using Zod, a TypeScript-first schema declaration and validation library. The `ResponseFormatter` schema defines an object with two string properties: `answer` and `followup_question`, each with a description. Zod schemas are a common and recommended format for structured output in LangChain.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/structured_outputs.mdx#_snippet_1 LANGUAGE: typescript
CODE:
```
import { z } from "zod"; const ResponseFormatter = z.object({ answer: z.string().describe("The answer to the user's question"), followup_question: z .string() .describe("A followup question the user could ask")
});
``` ---------------------------------------- TITLE: Using withStructuredOutput() for Structured Output in LangChain TypeScript
DESCRIPTION: This snippet demonstrates the recommended workflow for obtaining structured output using LangChain's `withStructuredOutput()` method. It shows how to define a simple schema, bind it to a model, and then invoke the model to get output conforming to that schema. This method automates schema binding and output parsing.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/structured_outputs.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
// Define schema
const schema = { foo: "bar" };
// Bind schema to model
const modelWithStructure = model.withStructuredOutput(schema);
// Invoke the model to produce structured output that matches the schema
const structuredOutput = await modelWithStructure.invoke(userInput);
``` ---------------------------------------- TITLE: Creating a Basic Chat Prompt Template with System Message (LangChain.js)
DESCRIPTION: This snippet demonstrates how to create a `ChatPromptTemplate` in LangChain.js. It defines a system message that instructs the LLM to respond like a pirate and uses a `MessagesPlaceholder` to dynamically insert user and AI messages into the prompt. This template prepares raw user input for LLM processing by adding a consistent conversational context.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#_snippet_11 LANGUAGE: JavaScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts"; const promptTemplate = ChatPromptTemplate.fromMessages([ ["system", "You talk like a pirate. Answer all questions to the best of your ability."], ["placeholder", "{messages}"],
]);
``` ---------------------------------------- TITLE: Configuring LangChain for LLM Function Calling with OpenAI in TypeScript
DESCRIPTION: This code converts the Zod schema to a JSON schema and defines it as a function for an OpenAI model. It sets up a ChatOpenAI instance with a specific model and binds the function schema, ensuring the LLM always attempts to call this function. A PromptTemplate and a LangChain expression language chain are constructed to process user queries and parse the JSON output.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/basic_critique_revise.ipynb#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { zodToJsonSchema } from "npm:zod-to-json-schema"; import { JsonOutputFunctionsParser } from "npm:langchain@0.0.173/output_parsers";
import { ChatOpenAI } from "npm:langchain@0.0.173/chat_models/openai";
import { PromptTemplate } from "npm:langchain@0.0.173/prompts"; const functionSchema = { name: "task-scheduler", description: "Schedules tasks", parameters: zodToJsonSchema(zodSchema)
}; const template = `Respond to the following user query to the best of your ability: {query}`; const generatePrompt = PromptTemplate.fromTemplate(template); const taskFunctionCallModel = new ChatOpenAI({ temperature: 0, model: "gpt-3.5-turbo"
}).bind({ functions: [functionSchema], function_call: { name: "task-scheduler" }
}); const generateChain = generatePrompt .pipe(taskFunctionCallModel) .pipe(new JsonOutputFunctionsParser()) .withConfig({ runName: "GenerateChain" });
``` ---------------------------------------- TITLE: Invoking a Runnable in LangChain (TypeScript)
DESCRIPTION: Demonstrates how to synchronously invoke a `Runnable` with a single input using the `invoke()` method. This is suitable for processing individual data points and obtaining an immediate result. The example shows converting a number to a string.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { RunnableLambda } from "@langchain/core/runnables"; const runnable = RunnableLambda.from((x: number) => x.toString()); await runnable.invoke(5);
``` ---------------------------------------- TITLE: Composing Chains with Custom Input Formatting (JavaScript)
DESCRIPTION: This snippet shows how to compose the joke generation chain with an evaluation chain. It uses `RunnableLambda` to transform the output of the first chain (`result`) into the expected input format (`{ joke: result }`) for the `analysisPrompt`, then pipes it through the model and parser.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sequence.ipynb#_snippet_3 LANGUAGE: JavaScript
CODE:
```
import { RunnableLambda } from "@langchain/core/runnables"; const analysisPrompt = ChatPromptTemplate.fromTemplate("is this a funny joke? {joke}") const composedChain = new RunnableLambda({ func: async (input: { topic: string }) => { const result = await chain.invoke(input); return { joke: result }; }
}).pipe(analysisPrompt).pipe(model).pipe(new StringOutputParser()) await composedChain.invoke({ topic: "bears" })
``` ---------------------------------------- TITLE: Setting LangSmith Tracing Environment Variables (TypeScript)
DESCRIPTION: This TypeScript snippet illustrates how to configure environment variables for LangSmith tracing. Uncommenting these lines enables automated tracing of model calls, providing insights and debugging capabilities for LangChain applications by connecting to the LangSmith platform.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#_snippet_3 LANGUAGE: TypeScript
CODE:
```
// process.env.LANGSMITH_TRACING="true"
// process.env.LANGSMITH_API_KEY="your-api-key"
``` ---------------------------------------- TITLE: Configuring LangSmith Observability in TypeScript
DESCRIPTION: This code configures environment variables for LangSmith, enabling tracing and observability for LangChain applications. Setting LANGSMITH_TRACING to true activates tracing, and LANGSMITH_API_KEY provides the necessary authentication for sending data to LangSmith.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search_community.ipynb#_snippet_1 LANGUAGE: typescript
CODE:
```
process.env.LANGSMITH_TRACING="true"
process.env.LANGSMITH_API_KEY="your-api-key"
``` ---------------------------------------- TITLE: Instantiating ChatOpenAI LLM for LangChain (TypeScript)
DESCRIPTION: This snippet instantiates a ChatOpenAI Large Language Model (LLM) from LangChain. It configures the model to use "gpt-4o" with a temperature of 0, ensuring deterministic and focused responses. This LLM instance (llm) will be used as a core component for the SelfQueryRetriever to understand and process natural language queries.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/pinecone.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
// @lc-docs-hide-cell import { ChatOpenAI } from "@langchain/openai"; const llm = new ChatOpenAI({ model: "gpt-4o", temperature: 0,
});
``` ---------------------------------------- TITLE: Invoking SelfQueryRetriever for Metadata-Based Query (TypeScript)
DESCRIPTION: This snippet demonstrates the usage of the selfQueryRetriever by invoking it with a natural language question. The retriever processes the query, leverages the LLM and structured query translator to convert it into a filter, and then applies this filter to the Pinecone vector store to retrieve relevant documents based on their metadata. The example query asks for movies rated higher than 8.5.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/pinecone.ipynb#_snippet_6 LANGUAGE: typescript
CODE:
```
await selfQueryRetriever.invoke( "Which movies are rated higher than 8.5?"
);
``` ---------------------------------------- TITLE: Creating Self-Query Retriever with Weaviate (TypeScript)
DESCRIPTION: This snippet demonstrates how to instantiate a SelfQueryRetriever, linking it to the previously defined language model (llm) and Weaviate vector store. It requires a brief summary of the document contents and the attribute information, along with a WeaviateTranslator to convert natural language queries into Weaviate-compatible structured queries.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/weaviate.ipynb#_snippet_5 LANGUAGE: typescript
CODE:
```
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { WeaviateTranslator } from "@langchain/weaviate"; const selfQueryRetriever = SelfQueryRetriever.fromLLM({ llm: llm, vectorStore: vectorStore, /** A short summary of what the document contents represent. */ documentContents: "Brief summary of a movie", attributeInfo: attributeInfo, structuredQueryTranslator: new WeaviateTranslator()
});
``` ---------------------------------------- TITLE: Streaming Audio Output with ChatOpenAI in LangChain.js
DESCRIPTION: This snippet illustrates how to stream audio output from OpenAI models using `ChatOpenAI` in LangChain.js. It requires the `pcm16` audio format for streaming and demonstrates iterating over the `audioOutputStream` to concatenate `AIMessageChunk`s into a final audio message.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#_snippet_28 LANGUAGE: JavaScript
CODE:
```
import { AIMessageChunk } from "@langchain/core/messages";
import { concat } from "@langchain/core/utils/stream"
import { ChatOpenAI } from "@langchain/openai"; const modelWithStreamingAudioOutput = new ChatOpenAI({ model: "gpt-4o-audio-preview", modalities: ["text", "audio"], audio: { voice: "alloy", format: "pcm16", // Format must be `pcm16` for streaming },
}); const audioOutputStream = await modelWithStreamingAudioOutput.stream("Tell me a joke about cats.");
let finalAudioOutputMsg: AIMessageChunk | undefined;
for await (const chunk of audioOutputStream) { finalAudioOutputMsg = finalAudioOutputMsg ? concat(finalAudioOutputMsg, chunk) : chunk;
}
const castStreamedAudioContent = finalAudioOutputMsg?.additional_kwargs.audio as Record<string, any>; console.log({ ...castStreamedAudioContent, data: castStreamedAudioContent.data.slice(0, 100) // Sliced for brevity
})
``` ---------------------------------------- TITLE: Setting Up Custom JSON Parsing with LCEL in LangChain.js
DESCRIPTION: This snippet defines types for `Person` and `People` and sets up a `ChatPromptTemplate` to instruct the LLM to output JSON matching a given schema, wrapped in markdown code blocks. It also includes a `extractJson` function that acts as a custom parser, designed to extract and parse JSON content embedded within ````json` and ```` tags from the LLM's `AIMessage` output.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#_snippet_8 LANGUAGE: TypeScript
CODE:
```
import { AIMessage } from "@langchain/core/messages";
import { ChatPromptTemplate } from "@langchain/core/prompts"; type Person = { name: string; height_in_meters: number;
}; type People = { people: Person[];
}; const schema = `{{ people: [{{ name: "string", height_in_meters: "number" }}] }}` // Prompt
const prompt = await ChatPromptTemplate.fromMessages( [ [ "system", `Answer the user query. Output your answer as JSON that\nmatches the given schema: \`\`\`json\n{schema}\n\`\`\`.\nMake sure to wrap the answer in \`\`\`json and \`\`\` tags` ], [ "human", "{query}" ] ]
).partial({ schema
}); /** * Custom extractor * * Extracts JSON content from a string where * JSON is embedded between ```json and ``` tags. */
const extractJson = (output: AIMessage): Array<People> => { const text = output.content as string; // Define the regular expression pattern to match JSON blocks const pattern = /```json(.*?)```/gs; // Find all non-overlapping matches of the pattern in the string const matches = text.match(pattern); // Process each match, attempting to parse it as JSON try { return matches?.map(match => { // Remove the markdown code block syntax to isolate the JSON string const jsonStr = match.replace(/```json|```/g, '').trim(); return JSON.parse(jsonStr); }) ?? []; } catch (error) { throw new Error(`Failed to parse: ${output}`); }
}
``` ---------------------------------------- TITLE: Setting Up LangGraph with In-Memory Checkpointer (JavaScript)
DESCRIPTION: This snippet sets up a LangGraph workflow for automatic chat history management. It defines a `callModel` function to interact with the LLM, incorporating a system prompt and existing messages. A `StateGraph` is created with a 'model' node and edges from `START` to 'model' and 'model' to `END`. Crucially, an in-memory `MemorySaver` is used as a `checkpointer` when compiling the workflow, enabling persistence of conversation state.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_memory.ipynb#_snippet_2 LANGUAGE: JavaScript
CODE:
```
import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from "@langchain/langgraph"; // Define the function that calls the model
const callModel = async (state: typeof MessagesAnnotation.State) => { const systemPrompt = "You are a helpful assistant. " + "Answer all questions to the best of your ability."; const messages = [{ role: "system", content: systemPrompt }, ...state.messages]; const response = await llm.invoke(messages); return { messages: response };
}; const workflow = new StateGraph(MessagesAnnotation)
// Define the node and edge .addNode("model", callModel) .addEdge(START, "model") .addEdge("model", END); // Add simple in-memory checkpointer
// highlight-start
const memory = new MemorySaver();
const app = workflow.compile({ checkpointer: memory });
// highlight-end
``` ---------------------------------------- TITLE: Defining LangGraph State and Workflow with Object Inputs (TypeScript)
DESCRIPTION: This snippet defines the `GraphAnnotation` for the LangGraph state, including `language` and `messages` fields, allowing for structured object inputs. It then creates a single-node `StateGraph` (`workflow2`) where the 'model' node invokes the previously defined `runnable` and updates the message history. The workflow is compiled with a `MemorySaver` checkpointer for state persistence.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/message_history.ipynb#_snippet_7 LANGUAGE: typescript
CODE:
```
import { START, END, StateGraph, MemorySaver, MessagesAnnotation, Annotation } from "@langchain/langgraph"; // Define the State
const GraphAnnotation = Annotation.Root({ language: Annotation<string>(), // Spread `MessagesAnnotation` into the state to add the `messages` field. ...MessagesAnnotation.spec,
}) // Define the function that calls the model
const callModel2 = async (state: typeof GraphAnnotation.State) => { const response = await runnable.invoke(state); // Update message history with response: return { messages: [response] };
}; const workflow2 = new StateGraph(GraphAnnotation) .addNode("model", callModel2) .addEdge(START, "model") .addEdge("model", END); const app2 = workflow2.compile({ checkpointer: new MemorySaver() });
``` ---------------------------------------- TITLE: Chaining LLM with PromptTemplate (JavaScript/TypeScript)
DESCRIPTION: This snippet demonstrates how to create a chain by piping a `PromptTemplate` into the `llm` object. The `PromptTemplate` defines a structured input for the model, allowing dynamic insertion of variables like `input` and `output_language`. The `chain.invoke` method then processes the templated prompt and generates a completion based on the provided inputs.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/llms.ipynb#_snippet_4 LANGUAGE: javascript
CODE:
```
import { PromptTemplate } from "@langchain/core/prompts" const prompt = PromptTemplate.fromTemplate("How to say {input} in {output_language}:\n") const chain = prompt.pipe(llm);
await chain.invoke( { output_language: "German", input: "I love programming.", }
)
``` ---------------------------------------- TITLE: Handling Multimodal Inputs with ChatOllama in LangChain.js
DESCRIPTION: This snippet demonstrates how to use ChatOllama with multimodal models like LLaVA to process image inputs. It shows how to read an image file, convert it to a base64 string, and include it as part of a HumanMessage's content for visual analysis by the model.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama.ipynb#_snippet_9 LANGUAGE: JavaScript
CODE:
```
import { ChatOllama } from "@langchain/ollama";
import { HumanMessage } from "@langchain/core/messages";
import * as fs from "node:fs/promises"; const imageData = await fs.readFile("../../../../../examples/hotdog.jpg");
const llmForMultiModal = new ChatOllama({ model: "llava", baseUrl: "http://127.0.0.1:11434",
});
const multiModalRes = await llmForMultiModal.invoke([ new HumanMessage({ content: [ { type: "text", text: "What is in this image?", }, { type: "image_url", image_url: `data:image/jpeg;base64,${imageData.toString("base64")}`, }, ], }),
]);
console.log(multiModalRes);
``` ---------------------------------------- TITLE: Passing Audio Input to ChatOpenAI in LangChain.js
DESCRIPTION: This example demonstrates how to provide audio as input to an OpenAI model using `ChatOpenAI` in LangChain.js. It shows how to construct a `HumanMessage` with `input_audio` containing base64 audio data and its format, then invoke the model, and finally extract the transcribed text from the response.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#_snippet_29 LANGUAGE: JavaScript
CODE:
```
import { HumanMessage } from "@langchain/core/messages"; const userInput = new HumanMessage({ content: [{ type: "input_audio", input_audio: { data: castAudioContent.data, // Re-use the base64 data from the first example format: "wav", }, }]
}) // Re-use the same model instance
const userInputAudioRes = await modelWithAudioOutput.invoke([userInput]); console.log((userInputAudioRes.additional_kwargs.audio as Record<string, any>).transcript);
``` ---------------------------------------- TITLE: Invoking Chain with Custom Parser in LangChain.js
DESCRIPTION: This code illustrates how to construct and invoke an LCEL chain that incorporates a custom parsing function. The chain pipes the prompt, model, and the `extractJson` function (wrapped in `RunnableLambda`) to process the LLM's output, demonstrating a flexible approach to structured data extraction when built-in parsers are insufficient.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#_snippet_10 LANGUAGE: TypeScript
CODE:
```
import { RunnableLambda } from "@langchain/core/runnables"; const chain = prompt.pipe(model).pipe(new RunnableLambda({ func: extractJson })); await chain.invoke({ query })
``` ---------------------------------------- TITLE: Indexing and Querying Documents with Azure Cosmos DB Vector Search (TypeScript)
DESCRIPTION: Demonstrates how to index documents into Azure Cosmos DB for MongoDB vCore using LangChain's vector store, perform a vector search query, and then leverage a LangChain chain to answer questions based on the retrieved documents. This example showcases the end-to-end flow for building AI applications with Cosmos DB.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/azure_cosmosdb_mongodb.mdx#_snippet_2 LANGUAGE: typescript
CODE:
```
import { AzureCosmosDBMongoDBvCoreVectorStore } from "@langchain/azure-cosmosdb";
import { OpenAIEmbeddings } from "@langchain/openai";
import { RetrievalQAChain } from "langchain/chains";
import { ChatOpenAI } from "@langchain/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter"; // Ensure environment variables are set:
// AZURE_COSMOSDB_CONNECTION_STRING
// AZURE_COSMOSDB_ADMIN_KEY
// AZURE_COSMOSDB_DATABASE_NAME
// AZURE_COSMOSDB_COLLECTION_NAME
// OPENAI_API_KEY async function runExample() { // 1. Load documents const loader = new TextLoader("src/document.txt"); // Assuming a document.txt exists const rawDocs = await loader.load(); // 2. Split documents const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200, }); const docs = await textSplitter.splitDocuments(rawDocs); // 3. Initialize embeddings and vector store const embeddings = new OpenAIEmbeddings(); const vectorStore = new AzureCosmosDBMongoDBvCoreVectorStore(embeddings, { connectionString: process.env.AZURE_COSMOSDB_CONNECTION_STRING, adminKey: process.env.AZURE_COSMOSDB_ADMIN_KEY, databaseName: process.env.AZURE_COSMOSDB_DATABASE_NAME, collectionName: process.env.AZURE_COSMOSDB_COLLECTION_NAME, }); // 4. Add documents to vector store (indexing) console.log("Adding documents to vector store..."); await vectorStore.addDocuments(docs); console.log("Documents added."); // 5. Perform a vector search query const query = "What is Azure Cosmos DB?"; console.log(`Performing vector search for: "${query}"`); const results = await vectorStore.similaritySearch(query, 1); console.log("Search results:", results); // 6. Use a chain to answer a question const model = new ChatOpenAI(); const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever()); console.log(`Answering question with chain: "${query}"`); const response = await chain.call({ query }); console.log("Chain response:", response);
} runExample().catch(console.error);
``` ---------------------------------------- TITLE: Configuring Retries for a Runnable in LangChain.js
DESCRIPTION: Illustrates how to apply retries to a runnable using `withRetry()`. This method allows specifying retry logic, such as `stopAfterAttempt`, to handle transient errors gracefully and improve robustness.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#_snippet_11 LANGUAGE: TypeScript
CODE:
```
import { RunnableLambda } from "@langchain/core/runnables"; let counter = 0; const retryFn = (_: any) => { counter++; console.log(`attempt with counter ${counter}`); throw new Error("Expected error");
}; const chain = RunnableLambda.from(retryFn).withRetry({ stopAfterAttempt: 2,
}); await chain.invoke(2);
``` ---------------------------------------- TITLE: Adding Fallbacks to a Runnable in LangChain.js
DESCRIPTION: Demonstrates how to use `withFallbacks()` to provide alternative runnables that execute if the primary runnable throws an error. This ensures resilience in chain execution by allowing a graceful recovery path.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#_snippet_10 LANGUAGE: TypeScript
CODE:
```
import { RunnableLambda } from "@langchain/core/runnables"; const runnable = RunnableLambda.from((x: any) => { throw new Error("Error case")
}); const fallback = RunnableLambda.from((x: any) => x + x); const chain = runnable.withFallbacks([fallback]); await chain.invoke("foo");
``` ---------------------------------------- TITLE: Constructing a RAG Chain with ExaRetriever and LLM
DESCRIPTION: This snippet illustrates the creation of a Retrieval-Augmented Generation (RAG) chain. It combines the `ExaRetriever` for context retrieval, a `ChatPromptTemplate` for instruction, an LLM, and a `StringOutputParser` to generate a final answer based on the retrieved context and the user's question.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/exa.ipynb#_snippet_5 LANGUAGE: typescript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers"; import type { Document } from "@langchain/core/documents"; const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided. Context: {context} Question: {question}`); const formatDocs = (docs: Document[]) => { return docs.map((doc) => doc.pageContent).join("\n\n");
} // See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([ { context: retriever.pipe(formatDocs), question: new RunnablePassthrough(), }, prompt, llm, new StringOutputParser(),
]);
``` ---------------------------------------- TITLE: Defining a LangChain Conversational Retrieval Chain (TypeScript)
DESCRIPTION: This snippet defines the final conversationalRetrievalChain by integrating the queryTransformingRetrieverChain for context generation and the documentChain for answer generation. It uses RunnablePassthrough.assign to combine these components, creating a robust chain capable of handling both initial and follow-up conversational queries.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#_snippet_13 LANGUAGE: TypeScript
CODE:
```
const conversationalRetrievalChain = RunnablePassthrough.assign({ context: queryTransformingRetrieverChain,
}).assign({ answer: documentChain,
});
``` ---------------------------------------- TITLE: Initializing a System Message for Chat Prompt in LangChain (JS)
DESCRIPTION: This snippet initializes a `SystemMessage` instance, which sets the initial persona or context for a chat prompt in LangChain. This message acts as a foundational instruction for the AI, guiding its behavior and responses throughout the conversation.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/prompts_composition.ipynb#_snippet_2 LANGUAGE: JavaScript
CODE:
```
import { AIMessage, HumanMessage, SystemMessage} from "@langchain/core/messages" const prompt = new SystemMessage("You are a nice pirate")
``` ---------------------------------------- TITLE: Streaming Agent Response for Schema Description (JavaScript)
DESCRIPTION: This code illustrates the agent's ability to handle qualitative questions by streaming its response to a request to describe a specific table ("Describe the playlisttrack table"). Similar to the analytical query, it pretty-prints each step of the agent's reasoning and output.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#_snippet_26 LANGUAGE: JavaScript
CODE:
```
let inputs3 = { messages: [{ role: "user", content: "Describe the playlisttrack table" }] }; for await ( const step of await agent.stream(inputs3, { streamMode: "values", })
) { const lastMessage = step.messages[step.messages.length - 1]; prettyPrint(lastMessage); console.log("-----\n");
}
``` ---------------------------------------- TITLE: Initializing LangChain Chat Model in JavaScript
DESCRIPTION: This snippet imports necessary classes from `@langchain/core/prompts` and `@langchain/openai` and initializes a `ChatOpenAI` model. It sets up the `gpt-4o` model, which is capable of handling multimodal inputs, as the language model for subsequent operations.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multimodal_prompts.ipynb#_snippet_1 LANGUAGE: JavaScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai"; const model = new ChatOpenAI({ model: "gpt-4o" })
``` ---------------------------------------- TITLE: Invoking Gemini Vision Model with Image Input - Google GenAI - TypeScript
DESCRIPTION: Illustrates how to use a Gemini vision model (`gemini-2.0-flash`) with `ChatGoogleGenerativeAI` to process image inputs. It reads an image file using `fs.readFileSync`, converts it to base64, and includes it in a `HumanMessage` content array along with a text prompt for image description.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/google.mdx#_snippet_3 LANGUAGE: typescript
CODE:
```
const visionModel = new ChatGoogleGenerativeAI({ model: "gemini-2.0-flash", maxOutputTokens: 2048,
});
const image = fs.readFileSync("./hotdog.jpg").toString("base64");
const input2 = [ new HumanMessage({ content: [ { type: "text", text: "Describe the following image.", }, { type: "image_url", image_url: `data:image/png;base64,${image}`, }, ], }),
]; const res = await visionModel.invoke(input2);
``` ---------------------------------------- TITLE: Invoking SelfQueryRetriever for Metadata-Based Query (TypeScript)
DESCRIPTION: This snippet shows how to use the configured `SelfQueryRetriever` to query the vector store. It takes a natural language question, which the retriever then translates into a structured query based on the defined `attributeInfo` and executes against the Qdrant vector store to retrieve relevant documents.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/qdrant.ipynb#_snippet_6 LANGUAGE: typescript
CODE:
```
await selfQueryRetriever.invoke( "Which movies are rated higher than 8.5?"
);
``` ---------------------------------------- TITLE: Initializing OpenAI Chat Model with LangChain
DESCRIPTION: This snippet initializes a ChatOpenAI instance from the @langchain/openai package. It configures the model to use gpt-4o-mini and sets the temperature to 0 for deterministic outputs, preparing it for subsequent LLM invocations.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/llm_chain.ipynb#_snippet_2 LANGUAGE: javascript
CODE:
```
import { ChatOpenAI } from '@langchain/openai'; const model = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0,
})
``` ---------------------------------------- TITLE: Parsing JSON Output with LangChain's JsonOutputParser (TypeScript)
DESCRIPTION: This snippet demonstrates how to configure a LangChain chain to prompt a chat model for JSON output and parse it using `JsonOutputParser`. It defines a `Joke` interface for type safety, sets up a prompt template with format instructions, and pipes the prompt, model, and parser together to invoke a query.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_json.ipynb#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
const model = new ChatOpenAI({ model: "gpt-4o", temperature: 0,
}) import { JsonOutputParser } from "@langchain/core/output_parsers"
import { ChatPromptTemplate } from "@langchain/core/prompts" // Define your desired data structure. Only used for typing the parser output.
interface Joke { setup: string punchline: string
} // A query and format instructions used to prompt a language model.
const jokeQuery = "Tell me a joke.";
const formatInstructions = "Respond with a valid JSON object, containing two fields: 'setup' and 'punchline'." // Set up a parser + inject instructions into the prompt template.
const parser = new JsonOutputParser<Joke>() const prompt = ChatPromptTemplate.fromTemplate( "Answer the user query.\n{format_instructions}\n{query}\n"
); const partialedPrompt = await prompt.partial({ format_instructions: formatInstructions
}); const chain = partialedPrompt.pipe(model).pipe(parser); await chain.invoke({ query: jokeQuery });
``` ---------------------------------------- TITLE: Defining a Basic LangChain Retrieval Chain (TypeScript)
DESCRIPTION: This snippet defines a basic retrieval chain using LangChain's Runnable interface. It extracts the content of the last message from an input array to serve as the query for a retriever, then passes the retrieved documents as context to a documentChain to generate a final answer. It requires BaseMessage, RunnablePassthrough, and RunnableSequence from @langchain/core.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#_snippet_8 LANGUAGE: TypeScript
CODE:
```
import type { BaseMessage } from "@langchain/core/messages";
import { RunnablePassthrough, RunnableSequence,
} from "@langchain/core/runnables"; const parseRetrieverInput = (params: { messages: BaseMessage[] }) => { return params.messages[params.messages.length - 1].content;
}; const retrievalChain = RunnablePassthrough.assign({ context: RunnableSequence.from([parseRetrieverInput, retriever]),
}).assign({ answer: documentChain,
});
``` ---------------------------------------- TITLE: Setting Environment Variables for API Keys
DESCRIPTION: This snippet shows how to set essential environment variables, including `OPENAI_API_KEY` for OpenAI API access and optional `LANGSMITH_API_KEY` and `LANGSMITH_TRACING` for LangSmith observability. These variables are crucial for authentication and tracing.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_multiple_retrievers.ipynb#_snippet_1 LANGUAGE: shell
CODE:
```
OPENAI_API_KEY=your-api-key # Optional, use LangSmith for best-in-class observability
LANGSMITH_API_KEY=your-api-key
LANGSMITH_TRACING=true # Reduce tracing latency if you are not in a serverless environment
# LANGCHAIN_CALLBACKS_BACKGROUND=true
``` ---------------------------------------- TITLE: Setting Environment Variables for OpenAI and LangSmith
DESCRIPTION: This snippet shows how to set environment variables required for authenticating with the OpenAI API and optionally configuring LangSmith for observability and tracing. LangSmith helps in debugging and monitoring LangChain applications.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_no_queries.ipynb#_snippet_1 LANGUAGE: Shell
CODE:
```
OPENAI_API_KEY=your-api-key # Optional, use LangSmith for best-in-class observability
LANGSMITH_API_KEY=your-api-key
LANGSMITH_TRACING=true # Reduce tracing latency if you are not in a serverless environment
# LANGCHAIN_CALLBACKS_BACKGROUND=true
``` ---------------------------------------- TITLE: Setting Together AI API Key (Bash)
DESCRIPTION: This snippet demonstrates how to set the `TOGETHER_AI_API_KEY` environment variable in a Bash shell. This key is essential for authenticating requests to the Together AI API.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/together.ipynb#_snippet_0 LANGUAGE: Bash
CODE:
```
export TOGETHER_AI_API_KEY="your-api-key"
``` ---------------------------------------- TITLE: Setting xAI API Key Environment Variable - Bash
DESCRIPTION: This command sets the `XAI_API_KEY` environment variable, which is required for authenticating requests to the xAI API when using the `@langchain/xai` package.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-xai/README.md#_snippet_1 LANGUAGE: bash
CODE:
```
export XAI_API_KEY=
``` ---------------------------------------- TITLE: Invoking Chat Model with Follow-up Question (TypeScript)
DESCRIPTION: This snippet shows a subsequent invocation of the `llm` with a follow-up question, 'What's my name?'. It further illustrates that without explicitly passing the conversation history, the model lacks memory of previous turns and cannot answer context-dependent questions, leading to a poor chatbot experience.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#_snippet_3 LANGUAGE: typescript
CODE:
```
await llm.invoke([{ role: "user", content: "Whats my name" }])
``` ---------------------------------------- TITLE: Defining Weather Tool Schema with Zod (TypeScript)
DESCRIPTION: This snippet defines a Zod object schema named `Weather` for weather search parameters. It specifies `city` and `state` as required string properties, each with a description. This schema is used to validate and describe the expected input for the `get_weather` tool.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_tool_client.mdx#_snippet_2 LANGUAGE: typescript
CODE:
```
const Weather = z .object({ city: z.string().describe("City to search for weather"), state: z.string().describe("State abbreviation to search for weather"), }) .describe("Weather search parameters");
``` ---------------------------------------- TITLE: Creating an Extractor with OpenAI GPT-4 and LangChain (TypeScript)
DESCRIPTION: This snippet initializes a `ChatOpenAI` model instance, specifically `gpt-4-0125-preview`, configured for tool calling with a temperature of 0 for deterministic output. It then constructs an `extractionRunnable` by piping a `prompt` into the LLM, using `withStructuredOutput` to enforce a `peopleSchema` for extraction, and naming the schema 'people' for additional context.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_examples.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai"; // We will be using tool calling mode, which
// requires a tool calling capable model.
const llm = new ChatOpenAI({ // Consider benchmarking with the best model you can to get // a sense of the best possible quality. model: "gpt-4-0125-preview", temperature: 0,
}); // For function/tool calling, we can also supply an name for the schema
// to give the LLM additional context about what it's extracting.
const extractionRunnable = prompt.pipe(llm.withStructuredOutput(peopleSchema, { name: "people" }));
``` ---------------------------------------- TITLE: Performing Max Marginal Relevance Search in LangChain.js Vector Store (TypeScript)
DESCRIPTION: This snippet demonstrates how to execute a Maximal Marginal Relevance (MMR) search using vectorStoreInstance.maxMarginalRelevanceSearch. MMR optimizes for both similarity to the query and diversity among the retrieved documents. It shows how to pass options like k (number of results) and a filter to refine the search, then logs the content and metadata of the diverse results.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/google_cloudsql_pg.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
const options = { k: 4, filter: "\"source\" = 'https://example.com'",
}; const results = await vectorStoreInstance.maxMarginalRelevanceSearch("biology", options); for (const doc of results) { console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
} ``` ---------------------------------------- TITLE: Invoking a LangGraph Agent with a RAG Chain Tool
DESCRIPTION: This snippet converts the previously defined RAG chain into a tool named pet_expert, specifying its schema. It then creates a new createReactAgent instance, providing it with this RAG chain tool. Finally, it invokes the agent with a human message, demonstrating how the agent utilizes the RAG chain tool by populating its required parameters and streams the interaction output.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/convert_runnable_to_tool.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
const ragTool = ragChain.asTool({ name: "pet_expert", description: "Get information about pets.", schema: z.object({ context: z.string(), question: z.string(), answer_style: z.string(), }),
}); const agent = createReactAgent({ llm: llm, tools: [ragTool] }); const stream = await agent.stream({ messages: [ ["human", "What would a pirate say dogs are known for?"] ]
}); for await (const chunk of stream) { // Log output from the agent or tools node if (chunk.agent) { console.log("AGENT:", chunk.agent.messages[0]); } else if (chunk.tools) { console.log("TOOLS:", chunk.tools.messages[0]); } console.log("----");
}
``` ---------------------------------------- TITLE: Building and Invoking a RAG Chain (TypeScript)
DESCRIPTION: Constructs a LangChain RAG chain by combining the retriever, prompt, and LLM. It includes a formatDocs utility to prepare retrieved documents for the prompt and uses RunnableMap and RunnablePassthrough to manage inputs and outputs, ultimately returning both the generated answer and the source documents.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
import { Document } from "@langchain/core/documents";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { RunnableMap, RunnablePassthrough } from "@langchain/core/runnables"; /** * Format the documents into a readable string. */
const formatDocs = (input: Record<string, any>): string => { const { docs } = input; return "\n\n" + docs.map((doc: Document) => `Article title: ${doc.metadata.title}\nArticle Snippet: ${doc.pageContent}`).join("\n\n");
}
// subchain for generating an answer once we've done retrieval
const answerChain = prompt.pipe(llm).pipe(new StringOutputParser());
const map = RunnableMap.from({ question: new RunnablePassthrough(), docs: retriever,
})
// complete chain that calls the retriever -> formats docs to string -> runs answer subchain -> returns just the answer and retrieved docs.
const chain = map.assign({ context: formatDocs }).assign({ answer: answerChain }).pick(["answer", "docs"]) await chain.invoke("How fast are cheetahs?")
``` ---------------------------------------- TITLE: Integrating Message Trimming with Chat History in LangChain.js
DESCRIPTION: This example illustrates how to combine `trimMessages` with `InMemoryChatMessageHistory` and `RunnableWithMessageHistory` to manage the length of chat conversations. It ensures that even when retrieving a full chat history, only the most recent and relevant messages (up to `maxTokens`) are passed to the LLM, preventing context window overflow.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/trim_messages.ipynb#_snippet_8 LANGUAGE: TypeScript
CODE:
```
import { InMemoryChatMessageHistory } from "@langchain/core/chat_history";
import { RunnableWithMessageHistory } from "@langchain/core/runnables";
import { HumanMessage, trimMessages } from "@langchain/core/messages";
import { ChatOpenAI } from "@langchain/openai"; const chatHistory = new InMemoryChatMessageHistory(messages.slice(0, -1)) const dummyGetSessionHistory = async (sessionId: string) => { if (sessionId !== "1") { throw new Error("Session not found"); } return chatHistory; } const llm = new ChatOpenAI({ model: "gpt-4o" }); const trimmer = trimMessages({ maxTokens: 45, strategy: "last", tokenCounter: llm, includeSystem: true, }); const chain = trimmer.pipe(llm);
const chainWithHistory = new RunnableWithMessageHistory({ runnable: chain, getMessageHistory: dummyGetSessionHistory,
})
await chainWithHistory.invoke( [new HumanMessage("what do you call a speechless parrot")], { configurable: { sessionId: "1"} },
)
``` ---------------------------------------- TITLE: Chaining OpenAI LLM with Prompt Template (JavaScript)
DESCRIPTION: This example demonstrates how to chain an `OpenAI` model with a `PromptTemplate` in LangChain. It shows how to define a structured prompt with input variables and then pipe it to the LLM, enabling more complex and dynamic interactions with the model.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/openai.ipynb#_snippet_5 LANGUAGE: javascript
CODE:
```
import { PromptTemplate } from "@langchain/core/prompts" const prompt = new PromptTemplate({ template: "How to say {input} in {output_language}:\n", inputVariables: ["input", "output_language"],
}) const chain = prompt.pipe(llm);
await chain.invoke( { output_language: "German", input: "I love programming.", }
)
``` ---------------------------------------- TITLE: Chaining Bedrock Chat Model with Prompt Template in LangChain.js
DESCRIPTION: This snippet illustrates how to chain a Bedrock chat model (`llm`) with a `ChatPromptTemplate` to create a more dynamic and reusable prompt. It demonstrates using `fromMessages` to define a template with placeholders and then piping it to the model for invocation with specific input variables.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/bedrock.ipynb#_snippet_4 LANGUAGE: javascript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts" const prompt = ChatPromptTemplate.fromMessages( [ [ "system", "You are a helpful assistant that translates {input_language} to {output_language}.", ], ["human", "{input}"], ]
) const chain = prompt.pipe(llm);
await chain.invoke( { input_language: "English", output_language: "German", input: "I love programming.", }
)
``` ---------------------------------------- TITLE: Indexing, Hybrid Search, and LLM Integration with Azure AI Search (TypeScript)
DESCRIPTION: This comprehensive TypeScript example demonstrates the full workflow of using Azure AI Search with LangChain. It covers initializing the vector store and embeddings, loading and splitting documents from a web source, indexing these documents into Azure AI Search, performing a hybrid search query, and finally integrating with an OpenAI LLM via LangChain's `RetrievalQAChain` to answer questions based on the retrieved content.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/azure_aisearch.mdx#_snippet_2 LANGUAGE: typescript
CODE:
```
import { AzureAISearchVectorStore } from "@langchain/community/vectorstores/azure_aisearch";
import { OpenAIEmbeddings, OpenAI } from "@langchain/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { RetrievalQAChain } from "langchain/chains";
import { CheerioWebBaseLoader } from "langchain/document_loaders/web/cheerio"; // Load environment variables
import * as dotenv from "dotenv";
dotenv.config(); const AZURE_AI_SEARCH_BASE_URL = process.env.AZURE_AI_SEARCH_BASE_URL;
const AZURE_AI_SEARCH_API_KEY = process.env.AZURE_AI_SEARCH_API_KEY;
const OPENAI_API_KEY = process.env.OPENAI_API_KEY; if (!AZURE_AI_SEARCH_BASE_URL || !AZURE_AI_SEARCH_API_KEY || !OPENAI_API_KEY) { throw new Error( "Missing Azure AI Search or OpenAI environment variables." );
} async function run() { // 1. Initialize Embeddings and Vector Store const embeddings = new OpenAIEmbeddings({ openAIApiKey: OPENAI_API_KEY, }); const vectorStore = new AzureAISearchVectorStore(embeddings, { base_url: AZURE_AI_SEARCH_BASE_URL, api_key: AZURE_AI_SEARCH_API_KEY, index_name: "langchain-docs", // Example index name }); // 2. Load and Split Documents (Example: from a web page) const loader = new CheerioWebBaseLoader( "https://www.langchain.com/docs/concepts/#vectorstores" ); const docs = await loader.load(); const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200, }); const splitDocs = await textSplitter.splitDocuments(docs); // 3. Index Documents console.log("Indexing documents..."); await vectorStore.addDocuments(splitDocs); console.log("Documents indexed."); // 4. Perform Hybrid Search const query = "What are vector stores?"; console.log(`Performing hybrid search for: "${query}"`); const results = await vectorStore.similaritySearch(query, 3, { searchType: "hybrid", // Explicitly set hybrid search }); console.log("Search results:", results); // 5. Integrate with LLM const model = new OpenAI({ openAIApiKey: OPENAI_API_KEY }); const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever()); console.log(`Asking LLM: "${query}"`); const res = await chain.call({ query }); console.log({ res });
} run().catch(console.error);
``` ---------------------------------------- TITLE: Creating and Invoking a Custom Parsing Chain in LangChain.js
DESCRIPTION: This snippet constructs an LCEL chain by piping a prompt, a language model, and the custom `extractJsonFromOutput` function. It then invokes this chain with the defined schema and query, demonstrating an end-to-end custom parsing workflow for structured data extraction.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_parse.ipynb#_snippet_8 LANGUAGE: JavaScript
CODE:
```
const customParsingChain = prompt.pipe(model).pipe(extractJsonFromOutput); await customParsingChain.invoke({ schema: zodToJsonSchema(peopleSchema), customParsingQuery
});
``` ---------------------------------------- TITLE: Defining Schema, Prompt Template, and JSON Extraction Function in LangChain.js
DESCRIPTION: This snippet defines Zod schemas for structured data extraction, converts them to JSON schema, and constructs a system prompt template for instructing an LLM to return JSON. It also includes a JavaScript function `extractJsonFromOutput` to parse and validate JSON blocks from the model's text output using a regular expression.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_parse.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
import { z } from "zod";
import { zodToJsonSchema } from "zod-to-json-schema"; personSchema = z.object({ name: z.optional(z.string()).describe("The name of the person"), hair_color: z.optional(z.string()).describe("The color of the person's hair, if known"), height_in_meters: z.optional(z.string()).describe("Height measured in meters")
}).describe("Information about a person."); const peopleSchema = z.object({ people: z.array(personSchema),
}); const SYSTEM_PROMPT_TEMPLATE = [ "Answer the user's query. You must return your answer as JSON that matches the given schema:", "```json\n{schema}\n```.", "Make sure to wrap the answer in ```json and ``` tags. Conform to the given schema exactly.",
].join("\n"); const customParsingPrompt = ChatPromptTemplate.fromMessages([ ["system", SYSTEM_PROMPT_TEMPLATE], ["human", "{query}"],
]); const extractJsonFromOutput = (message) => { const text = message.content; // Define the regular expression pattern to match JSON blocks const pattern = /```json\s*((.|\n)*?)\s*```/gs; // Find all non-overlapping matches of the pattern in the string const matches = pattern.exec(text); if (matches && matches[1]) { try { return JSON.parse(matches[1].trim()); } catch (error) { throw new Error(`Failed to parse: ${matches[1]}`); } } else { throw new Error(`No JSON found in: ${message}`); }
}
``` ---------------------------------------- TITLE: Passing Multiple HTTP URL Images to OpenAI Model (JavaScript)
DESCRIPTION: This example demonstrates sending a HumanMessage with multiple image URLs to an OpenAI model. It illustrates how to include multiple image_url content blocks within a single message, allowing the model to process and respond to queries involving multiple visual inputs.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multimodal_inputs.ipynb#_snippet_3 LANGUAGE: JavaScript
CODE:
```
const message = new HumanMessage({ content: [ { type: "text", text: "are these two images the same?" }, { type: "image_url", image_url: { url: imageUrl } }, { type: "image_url", image_url: { url: imageUrl } }, ],
});
const response = await openAIModel.invoke([message]);
console.log(response.content);
``` ---------------------------------------- TITLE: Invoking Chat Model with HumanMessage - TypeScript
DESCRIPTION: This snippet demonstrates how to invoke a chat model using a HumanMessage object. It imports HumanMessage from '@langchain/core/messages' and then calls 'model.invoke' with an array containing a new HumanMessage instance, representing explicit user input to the model.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/messages.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { HumanMessage } from "@langchain/core/messages"; await model.invoke([new HumanMessage("Hello, how are you?")]);
``` ---------------------------------------- TITLE: Integrating RunnableWithMessageHistory in LangGraph Node (TypeScript)
DESCRIPTION: This snippet presents an optimized `callModel` function that integrates `RunnableWithMessageHistory`. Instead of manual history management, this approach delegates history reading and updating to the `runnable` instance, simplifying the node's logic and leveraging LangChain's built-in history capabilities.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/chat_history.ipynb#_snippet_4 LANGUAGE: TypeScript
CODE:
```
const runnable = new RunnableWithMessageHistory({ // ... configuration from existing code
}); const callModel = async ( state: typeof MessagesAnnotation.State, config: RunnableConfig
): Promise<Partial<typeof MessagesAnnotation.State>> => { // RunnableWithMessageHistory takes care of reading the message history // and updating it with the new human message and AI response. const aiMessage = await runnable.invoke(state.messages, config); return { messages: [aiMessage] };
};
``` ---------------------------------------- TITLE: Creating and Invoking a String PromptTemplate in LangChain.js (TypeScript)
DESCRIPTION: This snippet demonstrates how to create and use a `PromptTemplate` for single-string formatting in LangChain.js. It imports `PromptTemplate`, initializes it with a template string containing a variable, and then invokes it with an object to fill the variable, producing a prompt for a language model.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#_snippet_20 LANGUAGE: TypeScript
CODE:
```
import { PromptTemplate } from "@langchain/core/prompts"; const promptTemplate = PromptTemplate.fromTemplate( "Tell me a joke about {topic}"
); await promptTemplate.invoke({ topic: "cats" });
``` ---------------------------------------- TITLE: Invoking LangGraph Application with Follow-up Query - TypeScript
DESCRIPTION: This snippet shows a follow-up invocation of the LangGraph application, leveraging the previously established 'config' (including the 'thread_id') to maintain conversational context. The model processes a new query, "What is one way of doing it?", and its answer is logged, demonstrating the persistence of chat history across turns.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#_snippet_10 LANGUAGE: TypeScript
CODE:
```
const result2 = await app.invoke( { input: "What is one way of doing it?" }, config,
)
console.log(result2.answer);
``` ---------------------------------------- TITLE: Initializing a String Prompt Template in LangChain (JS)
DESCRIPTION: This snippet initializes a `PromptTemplate` instance from a template string in LangChain. It defines a reusable prompt structure with placeholders for `topic` and `language`, which can be formatted later. This template is designed for generating jokes based on specified parameters.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/prompts_composition.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
import { PromptTemplate } from "@langchain/core/prompts"; const prompt = PromptTemplate.fromTemplate(`Tell me a joke about {topic}, make it funny and in {language}`) prompt
``` ---------------------------------------- TITLE: Building a RAG Chain with AmazonKnowledgeBaseRetriever
DESCRIPTION: This snippet constructs a Retrieval Augmented Generation (RAG) chain using LangChain's RunnableSequence. It defines a prompt template, a document formatting function, and then chains the retriever, prompt, LLM, and output parser. The retriever.pipe(formatDocs) extracts and formats context, while RunnablePassthrough passes the original question, enabling a complete RAG workflow.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/bedrock-knowledge-bases.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers"; import type { Document } from "@langchain/core/documents"; const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided. Context: {context} Question: {question}`); const formatDocs = (docs: Document[]) => { return docs.map((doc) => doc.pageContent).join("\n\n");
} // See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([ { context: retriever.pipe(formatDocs), question: new RunnablePassthrough(), }, prompt, llm, new StringOutputParser(),
]);
``` ---------------------------------------- TITLE: Splitting Text by Tokens using LangChain's TokenTextSplitter (JavaScript)
DESCRIPTION: This snippet demonstrates how to use LangChain's `TokenTextSplitter` to divide a document into chunks based on token count. It loads text from a file, initializes the splitter with a specified `chunkSize` and `chunkOverlap`, and then processes the text. The `js-tiktoken` library, tuned for OpenAI models, is implicitly used by `TokenTextSplitter` to measure chunk sizes.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/split_by_token.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
import { TokenTextSplitter } from "@langchain/textsplitters";
import * as fs from "node:fs"; // Load an example document
const rawData = await fs.readFileSync("../../../../examples/state_of_the_union.txt");
const stateOfTheUnion = rawData.toString(); const textSplitter = new TokenTextSplitter({ chunkSize: 10, chunkOverlap: 0,
}); const texts = await textSplitter.splitText(stateOfTheUnion); console.log(texts[0]);
``` ---------------------------------------- TITLE: Constructing a RAG Chain with Tavily Retriever (TypeScript)
DESCRIPTION: This snippet constructs a Retrieval-Augmented Generation (RAG) chain using LangChain's `RunnableSequence`. It defines a prompt template, a document formatter, and orchestrates the flow from retriever output to LLM input, finally parsing the output as a string.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/tavily.ipynb#_snippet_4 LANGUAGE: TypeScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers"; import type { Document } from "@langchain/core/documents"; const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided. Context: {context} Question: {question}`); const formatDocs = (docs: Document[]) => { return docs.map((doc) => doc.pageContent).join("\n\n");
} // See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([ { context: retriever.pipe(formatDocs), question: new RunnablePassthrough(), }, prompt, llm, new StringOutputParser(),
]);
``` ---------------------------------------- TITLE: Defining LangChain Entity Extraction Chain with Zod
DESCRIPTION: This JavaScript snippet defines a LangChain chain for extracting entities (people and movies) from user input. It uses ChatOpenAI as the LLM, ChatPromptTemplate for instruction, and zod to define a structured output schema, ensuring the LLM's response adheres to a predefined format for entity names.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_mapping.ipynb#_snippet_4 LANGUAGE: JavaScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";
import { z } from "zod"; const llm = new ChatOpenAI({ model: "gpt-3.5-turbo", temperature: 0 }); const entitySchema = z.object({ names: z.array(z.string()).describe("All the person or movies appearing in the text")
}).describe("Identifying information about entities."); const prompt = ChatPromptTemplate.fromMessages( [ [ "system", "You are extracting person and movies from the text." ], [ "human", "Use the given format to extract information from the following\ninput: {question}" ] ]
); const entityChain = prompt.pipe(llm.withStructuredOutput(entitySchema));
``` ---------------------------------------- TITLE: Installing LangChain Core Dependencies
DESCRIPTION: Installs essential LangChain packages and their dependencies, including @langchain/openai, @langchain/community, and @langchain/core, which are fundamental for building and running LangChain applications that interact with various models and tools.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/apify_dataset.mdx#_snippet_1 LANGUAGE: bash
CODE:
```
npm install hnswlib-node @langchain/openai @langchain/community @langchain/core
``` ---------------------------------------- TITLE: Installing LangChain Core and Integration Packages
DESCRIPTION: This `npm` command installs key LangChain packages: `@langchain/openai` for OpenAI model integrations, `@langchain/community` for various community-contributed components, and `@langchain/core` which provides the foundational LangChain functionalities. These are necessary for building LangChain applications, including those utilizing chat memory.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/mongodb.mdx#_snippet_2 LANGUAGE: bash
CODE:
```
npm install @langchain/openai @langchain/community @langchain/core
``` ---------------------------------------- TITLE: Installing LangChain Community and Core Packages (npm)
DESCRIPTION: This command installs the necessary `@langchain/community` and `@langchain/core` packages using npm, which are required to use the `PremEmbeddings` class and interact with the LangChain framework.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/premai.mdx#_snippet_0 LANGUAGE: bash
CODE:
```
npm install @langchain/community @langchain/core
``` ---------------------------------------- TITLE: Implementing Tool Calling with ChatMistralAI in LangChain.js
DESCRIPTION: This snippet demonstrates how to define and use a custom tool with the ChatMistralAI model in LangChain.js. It involves defining a tool schema using Zod, creating a tool instance, binding it to the model, and then chaining it with a prompt to enable the model to invoke the tool based on user input. The output shows the tool calls made by the model.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/mistral.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { ChatMistralAI } from "@langchain/mistralai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { z } from "zod";
import { tool } from "@langchain/core/tools"; const calculatorSchema = z.object({ operation: z .enum(["add", "subtract", "multiply", "divide"]) .describe("The type of operation to execute."), number1: z.number().describe("The first number to operate on."), number2: z.number().describe("The second number to operate on."),
}); const calculatorTool = tool((input) => { return JSON.stringify(input);
}, { name: "calculator", description: "A simple calculator tool", schema: calculatorSchema,
}); // Bind the tool to the model
const modelWithTool = new ChatMistralAI({ model: "mistral-large-latest",
}).bindTools([calculatorTool]); const calcToolPrompt = ChatPromptTemplate.fromMessages([ [ "system", "You are a helpful assistant who always needs to use a calculator.", ], ["human", "{input}"],
]); // Chain your prompt, model, and output parser together
const chainWithCalcTool = calcToolPrompt.pipe(modelWithTool); const calcToolRes = await chainWithCalcTool.invoke({ input: "What is 2 + 2?",
});
console.log(calcToolRes.tool_calls);
``` ---------------------------------------- TITLE: Defining a LangChain RunnableSequence for Query Generation and Retrieval (JavaScript)
DESCRIPTION: This snippet defines a `RunnableSequence` in LangChain, orchestrating a multi-step process. It first generates multiple search queries, then maps these queries to a retriever for document lookup, and finally applies reciprocal rank fusion to combine the results. This sequence streamlines complex information retrieval workflows.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rag_fusion.ipynb#_snippet_9 LANGUAGE: javascript
CODE:
```
const chain = RunnableSequence.from([ generateQueries, retriever.map(), reciprocalRankFusion,
]);
``` ---------------------------------------- TITLE: Building a RAG Chain with LangGraph (JavaScript)
DESCRIPTION: This comprehensive snippet constructs a RAG (Retrieval Augmented Generation) chain using LangGraph. It involves loading web content, splitting it into chunks, indexing these chunks in a vector store, defining a prompt, and orchestrating retrieval and generation steps into a stateful graph for question answering.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#_snippet_5 LANGUAGE: JavaScript
CODE:
```
import "cheerio";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { Document } from "@langchain/core/documents";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { pull } from "langchain/hub";
import { Annotation, StateGraph } from "@langchain/langgraph";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters"; // Load and chunk contents of blog
const pTagSelector = "p";
const cheerioLoader = new CheerioWebBaseLoader( "https://lilianweng.github.io/posts/2023-06-23-agent/", { selector: pTagSelector }
); const docs = await cheerioLoader.load(); const splitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200
});
const allSplits = await splitter.splitDocuments(docs); // Index chunks
await vectorStore.addDocuments(allSplits) // Define prompt for question-answering
const promptTemplate = await pull<ChatPromptTemplate>("rlm/rag-prompt"); // Define state for application
const InputStateAnnotation = Annotation.Root({ question: Annotation<string>,
}); const StateAnnotation = Annotation.Root({ question: Annotation<string>, context: Annotation<Document[]>, answer: Annotation<string>,
}); // Define application steps
const retrieve = async (state: typeof InputStateAnnotation.State) => { const retrievedDocs = await vectorStore.similaritySearch(state.question) return { context: retrievedDocs };
}; const generate = async (state: typeof StateAnnotation.State) => { const docsContent = state.context.map(doc => doc.pageContent).join("\n"); const messages = await promptTemplate.invoke({ question: state.question, context: docsContent }); const response = await llm.invoke(messages); return { answer: response.content };
}; // Compile application and test
const graph = new StateGraph(StateAnnotation) .addNode("retrieve", retrieve) .addNode("generate", generate) .addEdge("__start__", "retrieve") .addEdge("retrieve", "generate") .addEdge("generate", "__end__") .compile();
``` ---------------------------------------- TITLE: Integrating Supabase Self-Query Retriever into a LangChain RAG Chain (TypeScript)
DESCRIPTION: This snippet demonstrates how to construct a Retrieval Augmented Generation (RAG) chain using LangChain. It integrates a `selfQueryRetriever` to fetch relevant documents, formats them for context, and then passes them along with the user's question to an LLM for answer generation. It uses `RunnableSequence` to define the flow and `ChatPromptTemplate` for prompt engineering.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/supabase.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers"; import type { Document } from "@langchain/core/documents"; const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided. Context: {context} Question: {question}`); const formatDocs = (docs: Document[]) => { return docs.map((doc) => JSON.stringify(doc)).join("\n\n");
} // See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([ { context: selfQueryRetriever.pipe(formatDocs), question: new RunnablePassthrough(), }, prompt, llm, new StringOutputParser(),
]);
``` ---------------------------------------- TITLE: Defining Zod Schema for Query Analysis Output
DESCRIPTION: This snippet defines a `searchSchema` using Zod, specifying the expected structure for the LLM's query analysis output. It includes fields for a primary `query`, an optional array of `subQueries` with a detailed description for their generation, and an optional `publishYear`. This schema ensures the LLM's output conforms to a predictable and usable format.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_few_shot.ipynb#_snippet_3 LANGUAGE: typescript
CODE:
```
import { z } from "zod"; const subQueriesDescription = `
If the original question contains multiple distinct sub-questions,
or if there are more generic questions that would be helpful to answer in
order to answer the original question, write a list of all relevant sub-questions.
Make sure this list is comprehensive and covers all parts of the original question.
It's ok if there's redundancy in the sub-questions, it's better to cover all the bases than to miss some.
Make sure the sub-questions are as narrowly focused as possible in order to get the most relevant results.` const searchSchema = z.object({ query: z.string().describe("Primary similarity search query applied to video transcripts."), subQueries: z.array(z.string()).optional().describe(subQueriesDescription), publishYear: z.number().optional().describe("Year video was published")
})
``` ---------------------------------------- TITLE: Configuring LLM for Raw Structured Output in LangChain.js
DESCRIPTION: This snippet demonstrates how to configure an LLM model in LangChain.js to return raw output alongside parsed structured data. By setting `includeRaw: true` in `withStructuredOutput`, the output format changes to include both the original message and the `parsed` value, which is useful for handling imperfect LLM generations and avoiding exceptions.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#_snippet_4 LANGUAGE: TypeScript
CODE:
```
const joke = z.object({ setup: z.string().describe("The setup of the joke"), punchline: z.string().describe("The punchline to the joke"), rating: z.number().optional().describe("How funny the joke is, from 1 to 10")
}); const structuredLlm = model.withStructuredOutput(joke, { includeRaw: true, name: "joke" }); await structuredLlm.invoke("Tell me a joke about cats");
``` ---------------------------------------- TITLE: Enabling Web Search with OpenAI Built-in Tools in LangChain.js
DESCRIPTION: This example shows how to integrate OpenAI's built-in web search tool with ChatOpenAI using the .bindTools() method. By passing {"type": "web_search_preview"}, the model can perform web searches to ground its responses, providing external context for queries like current events.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#_snippet_14 LANGUAGE: TypeScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai"; const llm = new ChatOpenAI({ model: "gpt-4o-mini" }).bindTools([ { type: "web_search_preview" },
]); await llm.invoke("What was a positive news story from today?");
``` ---------------------------------------- TITLE: Constructing LCEL Chain with Tool Parser (TypeScript)
DESCRIPTION: This snippet constructs a LangChain Expression Language (LCEL) chain by piping the `prompt`, the `modelWithTools`, and a `JsonOutputKeyToolsParser` together. The parser is configured to extract the output for the `get_weather` tool based on the `Weather` Zod schema, ensuring structured output.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_tool_client.mdx#_snippet_6 LANGUAGE: typescript
CODE:
```
const chain = prompt.pipe(modelWithTools).pipe( new JsonOutputKeyToolsParser<z.infer<typeof Weather>>({ keyName: "get_weather", zodSchema: Weather, })
);
``` ---------------------------------------- TITLE: Streaming Agent Response with Proper Noun Query (JavaScript)
DESCRIPTION: This snippet demonstrates how to stream the agent's response to a user query, 'How many albums does alis in chain have?'. It initializes the input messages and then iterates through the streamed steps, printing each message. This showcases the agent's process, including its use of the `searchProperNouns` tool for spell correction.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#_snippet_33 LANGUAGE: JavaScript
CODE:
```
let inputs4 = { messages: [{ role: "user", content: "How many albums does alis in chain have?" }] }; for await ( const step of await agent2.stream(inputs4, { streamMode: "values", })
) { const lastMessage = step.messages[step.messages.length - 1]; prettyPrint(lastMessage); console.log("-----\n");
}
``` ---------------------------------------- TITLE: Indexing and Retrieving Documents with MemoryVectorStore (TypeScript)
DESCRIPTION: This snippet illustrates how to index a sample document into a `MemoryVectorStore` using the previously initialized `embeddings` object. It then demonstrates creating a retriever from the vector store and invoking it with a query to find the most similar document. The `pageContent` of the retrieved document is then accessed.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/bedrock.ipynb#_snippet_2 LANGUAGE: TypeScript
CODE:
```
// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory"; const text = "LangChain is the framework for building context-aware reasoning applications"; const vectorstore = await MemoryVectorStore.fromDocuments( [{ pageContent: text, metadata: {} }], embeddings,
); // Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1); // Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?"); retrievedDocuments[0].pageContent;
``` ---------------------------------------- TITLE: Transforming LangChain Vector Store into a Retriever (TypeScript)
DESCRIPTION: This snippet shows how to convert a LangChain vector store into a `retriever` for easier integration into LangChain chains. It demonstrates configuring the retriever with an optional filter and the number of results (`k`), then invoking it with a query to retrieve relevant documents.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/supabase.ipynb#_snippet_10 LANGUAGE: TypeScript
CODE:
```
const retriever = vectorStore.asRetriever({ // Optional filter filter: filter, k: 2,
});
await retriever.invoke("biology");
``` ---------------------------------------- TITLE: Invoking OpenAI Vision with Hosted Image URL in LangChain.js
DESCRIPTION: This snippet demonstrates how to initialize a `ChatOpenAI` model with `gpt-4-vision-preview` and send a multimodal `HumanMessage` containing both text and a hosted image URL. It shows how to structure the `content` array for image and text inputs to the model.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/openai_vision_multimodal.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
// Deno.env.set("OPENAI_API_KEY", ""); import { ChatOpenAI } from "npm:langchain@0.0.185/chat_models/openai";
import { HumanMessage } from "npm:langchain@0.0.185/schema"; const chat = new ChatOpenAI({ model: "gpt-4-vision-preview", maxTokens: 1024,
}); // Messages can now take an array of content in addition to a string
const hostedImageMessage = new HumanMessage({ content: [ { type: "text", text: "What does this image say?", }, { type: "image_url", image_url: "https://www.freecodecamp.org/news/content/images/2023/05/Screenshot-2023-05-29-at-5.40.38-PM.png", }, ],
}); await chat.invoke([hostedImageMessage]);
``` ---------------------------------------- TITLE: Calculating Cosine Similarity in TypeScript
DESCRIPTION: This TypeScript snippet defines a function to compute the cosine similarity between two numerical vectors, a common method for comparing text embeddings. It calculates the dot product of the vectors and divides it by the product of their magnitudes (L2 norms) to yield a similarity score. The example demonstrates how to use this function with hypothetical 'queryResult' and 'documentResult' vectors.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/embedding_models.mdx#_snippet_2 LANGUAGE: typescript
CODE:
```
function cosineSimilarity(vec1: number[], vec2: number[]): number { const dotProduct = vec1.reduce((sum, val, i) => sum + val * vec2[i], 0); const norm1 = Math.sqrt(vec1.reduce((sum, val) => sum + val * val, 0)); const norm2 = Math.sqrt(vec2.reduce((sum, val) => sum + val * val, 0)); return dotProduct / (norm1 * norm2);
} const similarity = cosineSimilarity(queryResult, documentResult);
console.log("Cosine Similarity:", similarity);
``` ---------------------------------------- TITLE: Executing a Query with the Agent Executor in TypeScript
DESCRIPTION: This code demonstrates how to send a natural language query to the agent executor and stream its responses. It iterates through the streamed events, logging tool calls or final content, showcasing the agent's ability to process and respond to database-related questions.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/sql.ipynb#_snippet_5 LANGUAGE: typescript
CODE:
```
const exampleQuery = "Can you list 10 artists from my database?" const events = await agentExecutor.stream( { messages: [["user", exampleQuery]]}, { streamMode: "values", }
) for await (const event of events) { const lastMsg = event.messages[event.messages.length - 1]; if (lastMsg.tool_calls?.length) { console.dir(lastMsg.tool_calls, { depth: null }); } else if (lastMsg.content) { console.log(lastMsg.content); }
}
``` ---------------------------------------- TITLE: Installing LangChain.js with npm, yarn, or pnpm
DESCRIPTION: This snippet demonstrates how to install the LangChain.js library using popular Node.js package managers: npm, yarn, or pnpm. It is a prerequisite for developing any application with LangChain.js.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/langchain/README.md#_snippet_0 LANGUAGE: Shell
CODE:
```
npm install -S langchain
``` LANGUAGE: Shell
CODE:
```
yarn add langchain
``` LANGUAGE: Shell
CODE:
```
pnpm add langchain
``` ---------------------------------------- TITLE: Creating a Tool with `tool` Function in TypeScript
DESCRIPTION: This snippet demonstrates how to create a tool using the `@langchain/core/tools` `tool` function. It defines a `multiply` tool that takes two numbers, `a` and `b`, and returns their product, along with its name, description, and a Zod schema for input validation.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tool_calling.mdx#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { tool } from "@langchain/core/tools"; const multiply = tool( ({ a, b }: { a: number; b: number }): number => { /** * Multiply a and b. */ return a * b; }, { name: "multiply", description: "Multiply two numbers", schema: z.object({ a: z.number(), b: z.number(), }), }
);
``` ---------------------------------------- TITLE: Few-Shot Prompting for Tool Calling with Custom Operators (TypeScript)
DESCRIPTION: This snippet illustrates few-shot prompting by providing the LLM with a sequence of `HumanMessage`, `AIMessage` (including `ToolCalls`), and `ToolMessage` examples. These examples teach the model to associate the '🦜' operator with the 'divide' operation, improving its ability to correctly invoke the `calculator` tool for similar future queries. It then logs the tool calls for the final query.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_few_shot.ipynb#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { HumanMessage, AIMessage, ToolMessage } from "@langchain/core/messages"; const res = await llmWithTools.invoke([ new HumanMessage("What is 333382 🦜 1932?"), new AIMessage({ content: "The 🦜 operator is shorthand for division, so we call the divide tool.", tool_calls: [{ id: "12345", name: "calculator", args: { number1: 333382, number2: 1932, operation: "divide", } }] }), new ToolMessage({ tool_call_id: "12345", content: "The answer is 172.558." }), new AIMessage("The answer is 172.558."), new HumanMessage("What is 6 🦜 2?"), new AIMessage({ content: "The 🦜 operator is shorthand for division, so we call the divide tool.", tool_calls: [{ id: "54321", name: "calculator", args: { number1: 6, number2: 2, operation: "divide", } }] }), new ToolMessage({ tool_call_id: "54321", content: "The answer is 3." }), new AIMessage("The answer is 3."), new HumanMessage("What is 3 🦜 12?")
]); console.log(res.tool_calls);
``` ---------------------------------------- TITLE: Using OpenAI JSON Mode with LangChain
DESCRIPTION: Demonstrates how to configure an OpenAI `ChatOpenAI` model in LangChain to enforce JSON output using `response_format: { type: "json_object" }`. The model is invoked to generate a JSON object containing random numbers, and its string content is logged.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/structured_outputs.mdx#_snippet_4 LANGUAGE: typescript
CODE:
```
import { ChatOpenAI } from "@langchain/openai"; const model = new ChatOpenAI({ model: "gpt-4",
}).bind({ response_format: { type: "json_object" },
}); const aiMsg = await model.invoke( "Return a JSON object with key 'random_nums' and a value of 10 random numbers in [0-99]"
);
console.log(aiMsg.content);
``` ---------------------------------------- TITLE: Initializing PostgresChatMessageHistory for Conversation Storage
DESCRIPTION: This snippet shows how to set up `PostgresChatMessageHistory` to store and retrieve conversation messages in a Postgres database. It first initializes a dedicated chat history table using the `PostgresEngine` instance, then creates a `PostgresChatMessageHistory` instance by providing the engine, a session ID, and the table name. This enables persistent storage for chat conversations.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-cloud-sql-pg/README.md#_snippet_3 LANGUAGE: javascript
CODE:
```
// ChatHistory table initialization
await engine.initChatHistoryTable("chat_message_table"); const historyInstance = await PostgresChatMessageHistory.initialize(engine, "test", "chat_message_table");
``` ---------------------------------------- TITLE: Building the Query Analyzer Chain with Function Calling
DESCRIPTION: This snippet constructs a LangChain RunnableSequence for query analysis. It binds the LLM with a 'search' function tool defined by the Zod schema, allowing the LLM to optionally call this function to generate a search query based on the user's question and a system prompt.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_no_queries.ipynb#_snippet_5 LANGUAGE: TypeScript
CODE:
```
import { zodToJsonSchema } from "zod-to-json-schema";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableSequence, RunnablePassthrough } from "@langchain/core/runnables"; const system = `You have the ability to issue search queries to get information to help answer user information. You do not NEED to look things up. If you don't need to, then just respond normally.`;
const prompt = ChatPromptTemplate.fromMessages( [ ["system", system], ["human", "{question}"], ]
)
const llmWithTools = llm.bind({ tools: [{ type: "function" as const, function: { name: "search", description: "Search over a database of job records.", parameters: zodToJsonSchema(searchSchema), } }]
})
const queryAnalyzer = RunnableSequence.from([ { question: new RunnablePassthrough(), }, prompt, llmWithTools
])
``` ---------------------------------------- TITLE: Using Code Execution Results in Chat History (TypeScript)
DESCRIPTION: This snippet illustrates how to pass the result of a previous code execution (codeExecutionResult) back to the model as part of the chat history. This allows the model to use its previous output as context for subsequent interactions, enabling it to explain its reasoning, the code it wrote, and the answer it obtained.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_generativeai.ipynb#_snippet_11 LANGUAGE: TypeScript
CODE:
```
const codeExecutionExplanation = await codeExecutionModel.invoke([ codeExecutionResult, { role: "user", content: "Please explain the question I asked, the code you wrote, and the answer you got.", }
]) console.log(codeExecutionExplanation.content);
``` ---------------------------------------- TITLE: Calling Tools with Google Generative AI Chat Model (JavaScript)
DESCRIPTION: This example demonstrates tool calling with `ChatGoogleGenerativeAI` using a base64-encoded image fetched via `axios`. It initializes a `gemini-1.5-pro-latest` model and binds the `weatherTool`. A `ChatPromptTemplate` is used to structure the input, which includes a `HumanMessage` with the base64 image data as a `media` type. The prompt is piped to the model, and the resulting tool calls are logged, showing Google GenAI's multimodal tool-calling integration.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calls_multimodal.ipynb#_snippet_3 LANGUAGE: javascript
CODE:
```
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import axios from "axios";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
import { HumanMessage } from "@langchain/core/messages"; const axiosRes = await axios.get(imageUrl, { responseType: "arraybuffer" });
const base64 = btoa( new Uint8Array(axiosRes.data).reduce( (data, byte) => data + String.fromCharCode(byte), '' )
); const model = new ChatGoogleGenerativeAI({ model: "gemini-1.5-pro-latest" }).bindTools([weatherTool]); const prompt = ChatPromptTemplate.fromMessages([ ["system", "describe the weather in this image"], new MessagesPlaceholder("message")
]); const response = await prompt.pipe(model).invoke({ message: new HumanMessage({ content: [{ type: "media", mimeType: "image/jpeg", data: base64, }] })
});
console.log(response.tool_calls);
``` ---------------------------------------- TITLE: Initializing OpenAI Embeddings in LangChain (TypeScript)
DESCRIPTION: This TypeScript snippet demonstrates how to import and initialize the `OpenAIEmbeddings` class from `@langchain/openai`. This creates an instance of the embedding model, which can then be used to generate vector representations of text, serving as a prerequisite for embedding operations.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/embed_text.mdx#_snippet_1 LANGUAGE: typescript
CODE:
```
import { OpenAIEmbeddings } from "@langchain/openai"; const embeddings = new OpenAIEmbeddings();
``` ---------------------------------------- TITLE: Creating a Basic Prompt and Model Chain (TypeScript)
DESCRIPTION: This snippet demonstrates how to construct a basic LangChain Expression Language (LCEL) chain. It defines a `ChatPromptTemplate` for system and human messages, initializes a `ChatOpenAI` model, and pipes them together with a `StringOutputParser` to process and format an equation statement.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/binding.ipynb#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai"; const prompt = ChatPromptTemplate.fromMessages( [ [ "system", "Write out the following equation using algebraic symbols then solve it. Use the format\n\nEQUATION:...\nSOLUTION:...\n\n", ], ["human", "{equation_statement}"], ]
) const model = new ChatOpenAI({ temperature: 0 }); const runnable = prompt.pipe(model).pipe(new StringOutputParser()); const res = await runnable.invoke({ equation_statement: "x raised to the third plus seven equals 12"
}); console.log(res);
``` ---------------------------------------- TITLE: Defining and Constructing a LangGraph for Map-Reduce Summarization (TypeScript)
DESCRIPTION: This comprehensive snippet defines the state, nodes, and edges for a LangGraph application designed for recursive map-reduce summarization. It includes functions for calculating token lengths, generating individual summaries, mapping documents to summary generation nodes, collecting and collapsing summaries based on a token limit, and finally generating a consolidated summary. The graph dynamically decides whether to collapse summaries further or proceed to final summarization based on token count.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#_snippet_10 LANGUAGE: TypeScript
CODE:
```
import { collapseDocs, splitListOfDocs,
} from "langchain/chains/combine_documents/reduce";
import { Document } from "@langchain/core/documents";
import { StateGraph, Annotation, Send } from "@langchain/langgraph"; let tokenMax = 1000; async function lengthFunction(documents) { const tokenCounts = await Promise.all(documents.map(async (doc) => { return llm.getNumTokens(doc.pageContent); })); return tokenCounts.reduce((sum, count) => sum + count, 0);
} const OverallState = Annotation.Root({ contents: Annotation<string[]>, // Notice here we pass a reducer function. // This is because we want combine all the summaries we generate // from individual nodes back into one list. - this is essentially // the "reduce" part summaries: Annotation<string[]>({ reducer: (state, update) => state.concat(update), }), collapsedSummaries: Annotation<Document[]>, finalSummary: Annotation<string>,
}); // This will be the state of the node that we will "map" all
// documents to in order to generate summaries
interface SummaryState { content: string;
} // Here we generate a summary, given a document
const generateSummary = async (state: SummaryState): Promise<{ summaries: string[] }> => { const prompt = await mapPrompt.invoke({context: state.content}); const response = await llm.invoke(prompt); return { summaries: [String(response.content)] };
}; // Here we define the logic to map out over the documents
// We will use this an edge in the graph
const mapSummaries = (state: typeof OverallState.State) => { // We will return a list of `Send` objects // Each `Send` object consists of the name of a node in the graph // as well as the state to send to that node return state.contents.map((content) => new Send("generateSummary", { content }));
}; const collectSummaries = async (state: typeof OverallState.State) => { return { collapsedSummaries: state.summaries.map(summary => new Document({pageContent: summary})) };
}; async function _reduce(input) { const prompt = await reducePrompt.invoke({ docs: input }); const response = await llm.invoke(prompt); return String(response.content);
} // Add node to collapse summaries
const collapseSummaries = async (state: typeof OverallState.State) => { const docLists = splitListOfDocs(state.collapsedSummaries, lengthFunction, tokenMax); const results = []; for (const docList of docLists) { results.push(await collapseDocs(docList, _reduce)); } return { collapsedSummaries: results };
}; // This represents a conditional edge in the graph that determines
// if we should collapse the summaries or not
async function shouldCollapse(state: typeof OverallState.State) { let numTokens = await lengthFunction(state.collapsedSummaries); if (numTokens > tokenMax) { return "collapseSummaries"; } else { return "generateFinalSummary"; }
} // Here we will generate the final summary
const generateFinalSummary = async (state: typeof OverallState.State) => { const response = await _reduce(state.collapsedSummaries); return { finalSummary: response};
}; // Construct the graph
const graph = new StateGraph(OverallState) .addNode("generateSummary", generateSummary) .addNode("collectSummaries", collectSummaries) .addNode("collapseSummaries", collapseSummaries) .addNode("generateFinalSummary", generateFinalSummary) .addConditionalEdges( "__start__", mapSummaries, ["generateSummary"] ) .addEdge("generateSummary", "collectSummaries") .addConditionalEdges( "collectSummaries", shouldCollapse, ["collapseSummaries", "generateFinalSummary"] ) .addConditionalEdges( "collapseSummaries", shouldCollapse,
``` ---------------------------------------- TITLE: Instantiating OpenAI LLM in LangChain (JavaScript)
DESCRIPTION: This code demonstrates how to create an instance of the `OpenAI` language model in LangChain. It illustrates common configuration parameters such as the model name, temperature, maximum tokens, timeout, retry attempts, and API key, allowing for fine-tuned control over the model's behavior.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/openai.ipynb#_snippet_3 LANGUAGE: javascript
CODE:
```
import { OpenAI } from "@langchain/openai" const llm = new OpenAI({ model: "gpt-3.5-turbo-instruct", temperature: 0, maxTokens: undefined, timeout: undefined, maxRetries: 2, apiKey: process.env.OPENAI_API_KEY, // other params...
})
``` ---------------------------------------- TITLE: Implementing Ollama Native Tool Calling (JavaScript)
DESCRIPTION: This snippet demonstrates how to define and bind a custom tool (`weatherTool`) to a `ChatOllama` model that supports native tool calling. The tool is defined with a name, description, and a Zod schema for its input parameters. The `bindTools` method integrates the tool with the LLM, allowing the model to invoke it based on user prompts, and the result is then logged.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama.ipynb#_snippet_6 LANGUAGE: javascript
CODE:
```
import { tool } from "@langchain/core/tools";
import { ChatOllama } from "@langchain/ollama";
import { z } from "zod"; const weatherTool = tool((_) => "Da weather is weatherin", { name: "get_current_weather", description: "Get the current weather in a given location", schema: z.object({ location: z.string().describe("The city and state, e.g. San Francisco, CA"), }),
}); // Define the model
const llmForTool = new ChatOllama({ model: "llama3-groq-tool-use",
}); // Bind the tool to the model
const llmWithTools = llmForTool.bindTools([weatherTool]); const resultFromTool = await llmWithTools.invoke( "What's the weather like today in San Francisco? Ensure you use the 'get_current_weather' tool."
); console.log(resultFromTool);
``` ---------------------------------------- TITLE: Building Baseline RAG Chain with LangChain Expression Language
DESCRIPTION: This snippet constructs the baseline RAG chain using LangChain Expression Language (LCEL). It defines a sequence where the retriever fetches documents, `formatDocs` processes them, the original question is passed through, and then the prompt, model, and string output parser are applied to generate the final answer.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rewrite.ipynb#_snippet_2 LANGUAGE: JavaScript
CODE:
```
const chain = RunnableSequence.from([ { context: retriever.pipe(formatDocs), question: new RunnablePassthrough() }, prompt, model, new StringOutputParser()
]);
``` ---------------------------------------- TITLE: Defining a Custom LangChain Retriever Class (TypeScript)
DESCRIPTION: This snippet defines `CustomRetriever`, extending `BaseRetriever` to implement a custom document retrieval mechanism. It overrides `_getRelevantDocuments` to return static `Document` objects based on the query, illustrating how to integrate custom data sources or logic into LangChain.js retrievers.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_retriever.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { BaseRetriever, type BaseRetrieverInput,
} from "@langchain/core/retrievers";
import type { CallbackManagerForRetrieverRun } from "@langchain/core/callbacks/manager";
import { Document } from "@langchain/core/documents"; export interface CustomRetrieverInput extends BaseRetrieverInput {} export class CustomRetriever extends BaseRetriever { lc_namespace = ["langchain", "retrievers"]; constructor(fields?: CustomRetrieverInput) { super(fields); } async _getRelevantDocuments( query: string, runManager?: CallbackManagerForRetrieverRun ): Promise<Document[]> { // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing // const additionalDocs = await someOtherRunnable.invoke(params, runManager?.getChild()); return [ // ...additionalDocs, new Document({ pageContent: `Some document pertaining to ${query}`, metadata: {}, }), new Document({ pageContent: `Some other document pertaining to ${query}`, metadata: {}, }), ]; }
}
``` ---------------------------------------- TITLE: Setting LangSmith Tracing and API Key in TypeScript
DESCRIPTION: This snippet shows how to configure LangSmith for observability by setting the LANGSMITH_TRACING and LANGSMITH_API_KEY environment variables. LangSmith provides best-in-class tracing and monitoring for LangChain applications.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/exa_search.ipynb#_snippet_1 LANGUAGE: TypeScript
CODE:
```
process.env.LANGSMITH_TRACING="true"
process.env.LANGSMITH_API_KEY="your-api-key"
``` ---------------------------------------- TITLE: Building and Invoking a Tool-Calling Chain
DESCRIPTION: This comprehensive snippet demonstrates how to construct and invoke a complex tool-calling chain. It defines a `ChatPromptTemplate`, binds the `DuckDuckGoSearch` tool to the LLM, and creates a `RunnableLambda` to manage the interaction flow. The chain processes user input, generates AI messages, executes tool calls, and then re-invokes the LLM with the tool results to formulate a final response.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/duckduckgo_search.ipynb#_snippet_5 LANGUAGE: typescript
CODE:
```
import { HumanMessage } from "@langchain/core/messages";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableLambda } from "@langchain/core/runnables"; const prompt = ChatPromptTemplate.fromMessages( [ ["system", "You are a helpful assistant."], ["placeholder", "{messages}"], ]
) const llmWithTools = llm.bindTools([tool]); const chain = prompt.pipe(llmWithTools); const toolChain = RunnableLambda.from( async (userInput: string, config) => { const humanMessage = new HumanMessage(userInput,); const aiMsg = await chain.invoke({ messages: [new HumanMessage(userInput)], }, config); const toolMsgs = await tool.batch(aiMsg.tool_calls, config); return chain.invoke({ messages: [humanMessage, aiMsg, ...toolMsgs], }, config); }
); const toolChainResult = await toolChain.invoke("how many people have climbed mount everest?");
``` ---------------------------------------- TITLE: Installing LangChain OpenAI and Core Packages (Bash)
DESCRIPTION: This command installs the necessary npm packages, `@langchain/openai` for integrating OpenAI models and `@langchain/core` for core LangChain functionalities. These packages are essential prerequisites for developing applications that utilize LangChain agents and tools.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/lambda_agent.mdx#_snippet_0 LANGUAGE: bash
CODE:
```
npm install @langchain/openai @langchain/core
``` ---------------------------------------- TITLE: Installing LangChain Dependencies (npm)
DESCRIPTION: This snippet provides the command to install the necessary LangChain packages, including `@langchain/openai`, `@langchain/community`, and `@langchain/core`, which are essential prerequisites for implementing the `MultiVectorRetriever` and related functionalities demonstrated in this guide.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multi_vector.mdx#_snippet_0 LANGUAGE: bash
CODE:
```
npm install @langchain/openai @langchain/community @langchain/core
``` ---------------------------------------- TITLE: Importing OpenAI LLM in TypeScript
DESCRIPTION: This TypeScript import statement brings the 'OpenAI' class from the 'langchain/llms/openai' module into scope. It is a prerequisite for interacting with OpenAI's large language models within a LangChain.js application, allowing you to instantiate and use the OpenAI LLM.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/examples/src/document_loaders/example_data/notion.md#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { OpenAI } from "langchain/llms/openai";
``` ---------------------------------------- TITLE: Composing Runnables with RunnableSequence in LangChain (TypeScript)
DESCRIPTION: Illustrates an alternative method for sequential composition using `RunnableSequence.from()`. This approach explicitly defines a sequence of runnables, where each runnable's output feeds into the next, achieving the same chaining effect as `pipe()`.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#_snippet_4 LANGUAGE: TypeScript
CODE:
```
import { RunnableLambda, RunnableSequence } from "@langchain/core/runnables"; const runnable1 = RunnableLambda.from((x: any) => { return { foo: x };
}); const runnable2 = RunnableLambda.from((x: any) => [x].concat([x])); const chain = RunnableSequence.from([ runnable1, runnable2,
]); await chain.invoke(2);
``` ---------------------------------------- TITLE: Initializing OpenAI Chat Model (JavaScript)
DESCRIPTION: This snippet initializes an instance of ChatOpenAI, a chat model from the @langchain/openai package. It configures the model to use gpt-4o, which serves as the primary language model for generating responses in the conversational RAG application.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#_snippet_3 LANGUAGE: javascript
CODE:
```
// @lc-docs-hide-cell
import { ChatOpenAI } from "@langchain/openai"; const llm = new ChatOpenAI({ model: "gpt-4o" });
``` ---------------------------------------- TITLE: Initializing the ChatOpenAI Model
DESCRIPTION: This code initializes an instance of the ChatOpenAI model, specifically 'gpt-4o' with a temperature of 0. This model will be used for the query analysis step, leveraging its function calling capabilities.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_no_queries.ipynb#_snippet_4 LANGUAGE: TypeScript
CODE:
```
import { ChatOpenAI } from '@langchain/openai'; const llm = new ChatOpenAI({ model: "gpt-4o", temperature: 0,
})
``` ---------------------------------------- TITLE: Initializing RunnableParallel in TypeScript
DESCRIPTION: This snippet demonstrates how to create a `RunnableParallel` instance, allowing multiple runnables to execute concurrently with the same input. The runnables are provided as key-value pairs, where keys will correspond to the output object's keys.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/lcel.mdx#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { RunnableParallel } from "@langchain/core/runnables";
const chain = new RunnableParallel({ key1: runnable1, key2: runnable2,
});
``` ---------------------------------------- TITLE: Creating a Joke Generation Chain (JavaScript)
DESCRIPTION: This snippet defines a `ChatPromptTemplate` to format input for a joke, then chains it with the previously initialized `model` and a `StringOutputParser`. This creates a `RunnableSequence` that takes a `topic` and returns a joke as a string.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sequence.ipynb#_snippet_1 LANGUAGE: JavaScript
CODE:
```
import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts"; const prompt = ChatPromptTemplate.fromTemplate("tell me a joke about {topic}") const chain = prompt.pipe(model).pipe(new StringOutputParser())
``` ---------------------------------------- TITLE: Modifying LangChain RAG Chain to Return Sources (JavaScript)
DESCRIPTION: Modifies the RAG chain to include the retrieved source documents in the final output. It uses `RunnableMap` and `assign` to pass through the raw documents and then format them for the prompt, ensuring sources are available alongside the generated answer.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_sources.ipynb#_snippet_6 LANGUAGE: javascript
CODE:
```
import { RunnableMap, RunnablePassthrough, RunnableSequence
} from "@langchain/core/runnables";
import { formatDocumentsAsString } from "langchain/util/document"; const ragChainWithSources = RunnableMap.from({ // Return raw documents here for now since we want to return them at // the end - we'll format in the next step of the chain context: retriever, question: new RunnablePassthrough(),
}).assign({ answer: RunnableSequence.from([ (input) => { return { // Now we format the documents as strings for the prompt context: formatDocumentsAsString(input.context), question: input.question }; }, prompt, llm, new StringOutputParser() ]),
}) await ragChainWithSources.invoke("What is Task Decomposition")
``` ---------------------------------------- TITLE: Instantiating __module_name__ LLM (JavaScript/TypeScript)
DESCRIPTION: This snippet demonstrates how to instantiate the `__module_name__` Large Language Model (LLM) object. It configures the model with parameters such as `model` name, `temperature` for creativity, `maxTokens` for output length, `timeout`, and `maxRetries` for robustness. This object is the primary interface for interacting with the LLM.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/llms.ipynb#_snippet_2 LANGUAGE: javascript
CODE:
```
import { __module_name__ } from "__full_import_path__" const llm = new __module_name__({ model: "model-name", temperature: 0, maxTokens: undefined, timeout: undefined, maxRetries: 2, // other params...
})
``` ---------------------------------------- TITLE: Integrating Query Analysis into LangGraph QA Application (TypeScript)
DESCRIPTION: This comprehensive snippet defines the state and nodes for a LangGraph application, integrating a query analysis step. It includes `analyzeQuery` to generate structured search queries, `retrieveQA` to fetch filtered documents from the vector store, and `generateQA` to produce an answer. The graph orchestrates these steps from user question to final answer.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#_snippet_24 LANGUAGE: TypeScript
CODE:
```
const StateAnnotationQA = Annotation.Root({ question: Annotation<string>, // highlight-start search: Annotation<z.infer<typeof searchSchema>>, // highlight-end context: Annotation<Document[]>, answer: Annotation<string>,
}); // highlight-start
const analyzeQuery = async (state: typeof InputStateAnnotation.State) => { const result = await structuredLlm.invoke(state.question) return { search: result }
};
// highlight-end const retrieveQA = async (state: typeof StateAnnotationQA.State) => { // highlight-start const filter = (doc) => doc.metadata.section === state.search.section; const retrievedDocs = await vectorStore.similaritySearch( state.search.query, 2, filter ) // highlight-end return { context: retrievedDocs };
}; const generateQA = async (state: typeof StateAnnotationQA.State) => { const docsContent = state.context.map(doc => doc.pageContent).join("\n"); const messages = await promptTemplate.invoke({ question: state.question, context: docsContent }); const response = await llm.invoke(messages); return { answer: response.content };
}; const graphQA = new StateGraph(StateAnnotationQA) .addNode("analyzeQuery", analyzeQuery) .addNode("retrieveQA", retrieveQA) .addNode("generateQA", generateQA) .addEdge("__start__", "analyzeQuery") .addEdge("analyzeQuery", "retrieveQA") .addEdge("retrieveQA", "generateQA") .addEdge("generateQA", "__end__") .compile();
``` ---------------------------------------- TITLE: Installing LangChain Core Libraries (npm)
DESCRIPTION: This snippet demonstrates how to install the core LangChain and LangChain Core libraries using npm, which are essential dependencies for building applications with LangChain. This command ensures that the necessary packages are available in your project.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#_snippet_0 LANGUAGE: Bash
CODE:
```
npm i langchain @langchain/core
``` ---------------------------------------- TITLE: Binding OpenAI Tool Schema to ChatOpenAI Model (JavaScript)
DESCRIPTION: This snippet demonstrates how to bind a custom OpenAI-style tool schema, specifically for a calculator function, directly to a `ChatOpenAI` model using the `bind()` method. It defines the tool's type, name, description, and parameters as a JSON schema, then invokes the model with a query to show the binding in action.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calling.ipynb#_snippet_3 LANGUAGE: JavaScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai"; const model = new ChatOpenAI({ model: "gpt-4o" }); const modelWithTools = model.bind({ tools: [{ "type": "function", "function": { "name": "calculator", "description": "Can perform mathematical operations.", "parameters": { "type": "object", "properties": { "operation": { "type": "string", "description": "The type of operation to execute.", "enum": ["add", "subtract", "multiply", "divide"] }, "number1": {"type": "number", "description": "First integer"}, "number2": {"type": "number", "description": "Second integer"} }, "required": ["number1", "number2"] } } }]
}); await modelWithTools.invoke(`Whats 119 times 8?`);
``` ---------------------------------------- TITLE: Attaching OpenAI Tools to a Model (TypeScript)
DESCRIPTION: This snippet illustrates how to attach OpenAI tools to a `ChatOpenAI` model using `withConfig()`. It defines a `get_current_weather` function tool and binds it to the model, enabling the model to utilize this tool for relevant queries without requiring the caller to explicitly pass tool definitions during invocation.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/binding.ipynb#_snippet_2 LANGUAGE: TypeScript
CODE:
```
const tools = [ { "type": "function", "function": { "name": "get_current_weather", "description": "Get the current weather in a given location", "parameters": { "type": "object", "properties": { "location": { "type": "string", "description": "The city and state, e.g. San Francisco, CA", }, "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}, }, "required": ["location"], }, }, }
]; const modelWithTools = new ChatOpenAI({ model: "gpt-4o" }).withConfig({ tools }); await modelWithTools.invoke("What's the weather in SF, NYC and LA?")
``` ---------------------------------------- TITLE: Composing Runnables with pipe() in LangChain (TypeScript)
DESCRIPTION: Demonstrates sequential composition of `Runnable` instances using the `pipe()` method. The output of the first runnable becomes the input for the second, creating a processing chain. This example transforms an input number into an object and then concatenates it into an array.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { RunnableLambda } from "@langchain/core/runnables"; const runnable1 = RunnableLambda.from((x: any) => { return { foo: x };
}); const runnable2 = RunnableLambda.from((x: any) => [x].concat([x])); const chain = runnable1.pipe(runnable2); await chain.invoke(2);
``` ---------------------------------------- TITLE: Chaining Langchain Vertex AI Model with Prompt Template
DESCRIPTION: This snippet illustrates how to chain a ChatVertexAI model with a ChatPromptTemplate. It defines a system and human message template with placeholders, allowing for structured and dynamic input to the language model for tasks like translation.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_vertex_ai.ipynb#_snippet_11 LANGUAGE: JavaScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts" const prompt = ChatPromptTemplate.fromMessages( [ [ "system", "You are a helpful assistant that translates {input_language} to {output_language}." ], ["human", "{input}"] ]
) const chain = prompt.pipe(llm);
await chain.invoke( { input_language: "English", output_language: "German", input: "I love programming." }
);
``` ---------------------------------------- TITLE: Adding Documents to a LangChain Vector Store
DESCRIPTION: This snippet illustrates how to add a list of Document objects to a LangChain vector store using the addDocuments method. Each Document contains pageContent (the text to be embedded) and optional metadata, allowing for structured storage of unstructured data. This operation populates the vector store with semantically searchable content.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/vectorstores.mdx#_snippet_1 LANGUAGE: typescript
CODE:
```
import { Document } from "@langchain/core/documents"; const document1 = new Document({ pageContent: "I had chocalate chip pancakes and scrambled eggs for breakfast this morning.", metadata: { source: "tweet" }
}); const document2 = new Document({ pageContent: "The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.", metadata: { source: "news" }
}); const documents = [document1, document2]; await vectorStore.addDocuments(documents);
``` ---------------------------------------- TITLE: Initializing ChatOpenAI Model in LangChain
DESCRIPTION: This snippet initializes a `ChatOpenAI` instance, which serves as the language model for subsequent tool-calling operations. It sets up the model with `gpt-4o-mini`, a compact and efficient model suitable for various tasks. This model will be used to demonstrate tool binding and invocation.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_runtime.ipynb#_snippet_0 LANGUAGE: TypeScript
CODE:
```
// @lc-docs-hide-cell import { ChatOpenAI } from "@langchain/openai"; const llm = new ChatOpenAI({ model: "gpt-4o-mini" })
``` ---------------------------------------- TITLE: Setting up RAG Ingest Pipeline with Watsonx Embeddings (JavaScript)
DESCRIPTION: This snippet initializes a RAG ingest pipeline. It uses `WatsonxEmbeddings` for document embedding, `CharacterTextSplitter` to chunk text, and `MemoryVectorStore` to store and retrieve documents. It demonstrates how to load text, split it, embed it, store it, and then retrieve relevant documents based on a query.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/ibm.ipynb#_snippet_8 LANGUAGE: JavaScript
CODE:
```
import { readFileSync } from "node:fs";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { WatsonxEmbeddings } from "@langchain/community/embeddings/ibm";
import { CharacterTextSplitter } from "@langchain/textsplitters"; const embeddings = new WatsonxEmbeddings({ version: "YYYY-MM-DD", serviceUrl: process.env.API_URL, projectId: "<PROJECT_ID>", spaceId: "<SPACE_ID>", model: "ibm/slate-125m-english-rtrvr",
}); const textSplitter = new CharacterTextSplitter({ chunkSize: 400, chunkOverlap: 0,
}); const query = "What did the president say about Ketanji Brown Jackson";
const text = readFileSync("state_of_the_union.txt", "utf8"); const docs = await textSplitter.createDocuments([text]);
const vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);
const vectorStoreRetriever = vectorStore.asRetriever(); const result = await vectorStoreRetriever.invoke(query);
console.log(result);
``` ---------------------------------------- TITLE: Integrating Message Trimmer into LangGraph Chain (JS/TS)
DESCRIPTION: This snippet shows how to integrate the `trimMessages` helper into a LangGraph chain. The `callModel4` function first trims the incoming `state.messages` before passing them to the prompt template and then to the LLM. This ensures that only a limited number of recent messages are sent to the model, preventing context window overflow. The snippet also defines a basic `StateGraph` workflow.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#_snippet_20 LANGUAGE: TypeScript
CODE:
```
const callModel4 = async (state: typeof GraphAnnotation.State) => { // highlight-start const trimmedMessage = await trimmer.invoke(state.messages); const prompt = await promptTemplate2.invoke({ messages: trimmedMessage, language: state.language }); const response = await llm.invoke(prompt); // highlight-end return { messages: [response] };
}; const workflow4 = new StateGraph(GraphAnnotation) .addNode("model", callModel4) .addEdge(START, "model") .addEdge("model", END); const app4 = workflow4.compile({ checkpointer: new MemorySaver() });
``` ---------------------------------------- TITLE: Invoking Runnables in Parallel with RunnableParallel (TypeScript)
DESCRIPTION: Shows how to execute multiple runnables concurrently using `RunnableParallel.from()`. This creates a structure where different runnables process the same input in parallel, and their outputs are combined into a single object, keyed by the names provided.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#_snippet_5 LANGUAGE: TypeScript
CODE:
```
import { RunnableLambda, RunnableParallel } from "@langchain/core/runnables"; const runnable1 = RunnableLambda.from((x: any) => { return { foo: x };
}); const runnable2 = RunnableLambda.from((x: any) => [x].concat([x])); const chain = RunnableParallel.from({ first: runnable1, second: runnable2,
}); await chain.invoke(2);
``` ---------------------------------------- TITLE: Creating and Invoking a Vector Store Retriever
DESCRIPTION: This snippet converts the `vectorstore` into a retriever, configured to return the top 4 most relevant documents. It then invokes the `retriever` with a specific query, 'how can langsmith help with testing?', to fetch relevant documents, which are then logged to the console.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#_snippet_4 LANGUAGE: JavaScript
CODE:
```
const retriever = vectorstore.asRetriever(4); const docs = await retriever.invoke("how can langsmith help with testing?"); console.log(docs);
``` ---------------------------------------- TITLE: Streaming Chat Model Responses - JavaScript
DESCRIPTION: This snippet demonstrates how to stream responses from a LangChain chat model using the `stream` method. It sends a prompt to the model and then iterates asynchronously over the received chunks, printing each chunk's content followed by a delimiter.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_streaming.ipynb#_snippet_1 LANGUAGE: JavaScript
CODE:
```
const stream = await model.stream("Write me a 1 verse song about goldfish on the moon") for await (const chunk of stream) { console.log(`${chunk.content}\n---`);
}
``` ---------------------------------------- TITLE: Defining a Conditional Query Transforming Retriever Chain (TypeScript)
DESCRIPTION: This snippet defines a RunnableBranch that conditionally applies query transformation. If the input messages array contains only one message (initial query), it directly uses parseRetrieverInput and retriever. Otherwise, it pipes the queryTransformPrompt through an LLM and StringOutputParser before passing it to the retriever, ensuring follow-up questions are properly handled. It requires RunnableBranch and StringOutputParser.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#_snippet_12 LANGUAGE: TypeScript
CODE:
```
import { RunnableBranch } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers"; const queryTransformingRetrieverChain = RunnableBranch.from([ [ (params: { messages: BaseMessage[] }) => params.messages.length === 1, RunnableSequence.from([parseRetrieverInput, retriever]), ], queryTransformPrompt .pipe(llm) .pipe(new StringOutputParser()) .pipe(retriever),
]).withConfig({ runName: "chat_retriever_chain" });
``` ---------------------------------------- TITLE: Declarative Message Trimming with LangChain.js Runnables
DESCRIPTION: This example demonstrates how to use `trimMessages` declaratively within a LangChain.js runnable chain. By configuring `trimMessages` without an initial message input, it creates a `RunnableLambda` that can be piped directly into an LLM, ensuring messages are trimmed before being passed to the model.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/trim_messages.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { trimMessages } from "@langchain/core/messages"; const llm = new ChatOpenAI({ model: "gpt-4o" }) // Notice we don't pass in messages. This creates
// a RunnableLambda that takes messages as input
const trimmer = trimMessages({ maxTokens: 45, strategy: "last", tokenCounter: llm, includeSystem: true,
}) const chain = trimmer.pipe(llm);
await chain.invoke(messages)
``` ---------------------------------------- TITLE: Using `streamEvents` with a Non-Streaming Component in LangChain.js (TypeScript)
DESCRIPTION: This snippet illustrates how `streamEvents` can still provide streaming output from intermediate steps (like the chat model and parser) even when a non-streaming component (`extractCountryNames`) is present in the chain. This demonstrates that `streamEvents` offers more granular visibility into the chain's execution flow compared to `stream`.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#_snippet_23 LANGUAGE: TypeScript
CODE:
```
const eventStream = await chain.streamEvents( `output a list of the countries france, spain and japan and their populations in JSON format.
Use a dict with an outer key of "countries" which contains a list of countries.
Each country should have the key "name" and "population"
Your output should ONLY contain valid JSON data. Do not include any other text or content in your output.`, { version: "v2" },
); let eventCount = 0; for await (const event of eventStream) { // Truncate the output if (eventCount > 30) { continue; } const eventType = event.event; if (eventType === "on_chat_model_stream") { console.log(`Chat model chunk: ${event.data.chunk.message.content}`); } else if (eventType === "on_parser_stream") { console.log(`Parser chunk: ${JSON.stringify(event.data.chunk)}`); } else { console.log(eventType) } eventCount += 1;
}
``` ---------------------------------------- TITLE: Automatic Coercion of Custom Functions in LangChain Chains (TypeScript)
DESCRIPTION: This example illustrates how custom TypeScript functions are automatically coerced into Runnables when used within a `RunnableSequence.from` chain, eliminating the need for explicit `RunnableLambda` creation. The chain first generates a story using a prompt and a model, then a custom function extracts the first five characters from the model's output.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/functions.ipynb#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { RunnableSequence } from "@langchain/core/runnables"; const storyPrompt = ChatPromptTemplate.fromTemplate("Tell me a short story about {topic}"); const storyModel = new ChatOpenAI({ model: "gpt-4o" }); const chainWithCoercedFunction = RunnableSequence.from([ storyPrompt, storyModel, (input) => input.content.slice(0, 5),
]); await chainWithCoercedFunction.invoke({ "topic": "bears" });
``` ---------------------------------------- TITLE: Returning Streamable Data from Server Action (TypeScript)
DESCRIPTION: This snippet returns an object containing the `stream.value` from the `runAgent` server action. This allows the client component to receive and read the streamed data, enabling real-time updates from the LangChain agent.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#_snippet_8 LANGUAGE: typescript
CODE:
```
return { streamData: stream.value };
``` ---------------------------------------- TITLE: Indexing and Retrieving Data with MemoryVectorStore and Azure OpenAI Embeddings in LangChain.js
DESCRIPTION: This example illustrates how to use the initialized embeddings object to index a sample document into a MemoryVectorStore and then retrieve the most similar text. It shows the process of creating a vector store from documents and converting it into a retriever for RAG flows.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/azure_openai.ipynb#_snippet_4 LANGUAGE: TypeScript
CODE:
```
// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory"; const text = "LangChain is the framework for building context-aware reasoning applications"; const vectorstore = await MemoryVectorStore.fromDocuments( [{ pageContent: text, metadata: {} }], embeddings,
); // Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1); // Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?"); retrievedDocuments[0].pageContent;
``` ---------------------------------------- TITLE: Setting up pgvector Extension and Documents Table in Supabase (SQL)
DESCRIPTION: This SQL script enables the `pgvector` extension in a PostgreSQL database, which is essential for handling vector embeddings. It then creates a `documents` table to store text content, associated metadata, and vector embeddings. Finally, it defines a `match_documents` function for performing similarity searches on the stored embeddings, allowing filtering by metadata.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/supabase.ipynb#_snippet_0 LANGUAGE: SQL
CODE:
```
-- Enable the pgvector extension to work with embedding vectors
create extension vector; -- Create a table to store your documents
create table documents ( id bigserial primary key, content text, -- corresponds to Document.pageContent metadata jsonb, -- corresponds to Document.metadata embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed
); -- Create a function to search for documents
create function match_documents ( query_embedding vector(1536), match_count int DEFAULT null, filter jsonb DEFAULT '{}'
) returns table ( id bigint, content text, metadata jsonb, embedding jsonb, similarity float
)
language plpgsql
as $$
#variable_conflict use_column
begin return query select id, content, metadata, (embedding::text)::jsonb as embedding, 1 - (documents.embedding <=> query_embedding) as similarity from documents where metadata @> filter order by documents.embedding <=> query_embedding limit match_count;
end;
$$;
``` ---------------------------------------- TITLE: Producing Structured Outputs with LangChain Chat Models (TypeScript)
DESCRIPTION: This example illustrates how LangChain's chat model interface facilitates generating structured outputs. It shows defining a schema (e.g., using Zod) and then binding this schema to a model using the `withStructuredOutput()` method, providing a unified approach for structured responses regardless of the underlying provider's API.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/why_langchain.mdx#_snippet_1 LANGUAGE: TypeScript
CODE:
```
// Define tool as a Zod schema
const schema = z.object({ ... });
// Bind schema to model
const modelWithStructure = model.withStructuredOutput(schema)
``` ---------------------------------------- TITLE: Defining Zod Schema and Invoking Model for Structured Output (Basic)
DESCRIPTION: This snippet demonstrates how to define a Zod schema for structured output, including optional fields and descriptions. It then shows how to apply this schema to a LangChain model using `withStructuredOutput()` and invoke the model to generate a joke matching the defined structure.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
import { z } from "zod"; const joke = z.object({ setup: z.string().describe("The setup of the joke"), punchline: z.string().describe("The punchline to the joke"), rating: z.number().optional().describe("How funny the joke is, from 1 to 10")
}); const structuredLlm = model.withStructuredOutput(joke); await structuredLlm.invoke("Tell me a joke about cats")
``` ---------------------------------------- TITLE: Recommended Tool Calling Workflow in TypeScript
DESCRIPTION: This snippet illustrates the recommended pseudo-code workflow for integrating tools with a model in LangChain. It shows how tools are created, bound to a model using `bindTools()`, and then invoked with user input, expecting tool call arguments in the response.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tool_calling.mdx#_snippet_0 LANGUAGE: TypeScript
CODE:
```
// Tool creation
const tools = [myTool];
// Tool binding
const modelWithTools = model.bindTools(tools);
// Tool calling
const response = await modelWithTools.invoke(userInput);
``` ---------------------------------------- TITLE: Chaining ChatOpenAI with Prompt Template (JavaScript)
DESCRIPTION: This example demonstrates how to create a LangChain sequence by piping a `ChatPromptTemplate` to the `ChatOpenAI` model. This allows for dynamic prompt construction using input variables, enabling more complex and flexible conversational flows.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#_snippet_5 LANGUAGE: javascript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts" const prompt = ChatPromptTemplate.fromMessages( [ [ "system", "You are a helpful assistant that translates {input_language} to {output_language}.", ], ["human", "{input}"], ]
) const chain = prompt.pipe(llm);
await chain.invoke( { input_language: "English", output_language: "German", input: "I love programming.", }
)
``` ---------------------------------------- TITLE: Streaming with LCEL Chain (JavaScript)
DESCRIPTION: This snippet constructs a LangChain Expression Language (LCEL) chain by piping a `ChatPromptTemplate`, the `ChatOpenAI` model, and a `StringOutputParser`. It then streams the output of this chain for a given topic, demonstrating end-to-end streaming capabilities with LCEL for more complex workflows.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#_snippet_5 LANGUAGE: JavaScript
CODE:
```
import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts"; const prompt = ChatPromptTemplate.fromTemplate("Tell me a joke about {topic}"); const parser = new StringOutputParser(); const chain = prompt.pipe(model).pipe(parser); const stream = await chain.stream({ topic: "parrot",
}); for await (const chunk of stream) { console.log(`${chunk}|`)
}
``` ---------------------------------------- TITLE: Integrating ArxivRetriever into a LangChain RAG Chain - TypeScript
DESCRIPTION: This snippet demonstrates how to integrate the `ArxivRetriever` into a LangChain RAG (Retrieval Augmented Generation) chain. It sets up an LLM, a prompt template, and a runnable sequence that pipes retrieved documents as context to answer a user's question.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/arxiv-retriever.mdx#_snippet_3 LANGUAGE: typescript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence,
} from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";
import type { Document } from "@langchain/core/documents"; const llm = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0,
}); const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided. Context: {context} Question: {question}`); const formatDocs = (docs: Document[]) => { return docs.map((doc) => doc.pageContent).join("\n\n");
}; const ragChain = RunnableSequence.from([ { context: retriever.pipe(formatDocs), question: new RunnablePassthrough(), }, prompt, llm, new StringOutputParser(),
]); await ragChain.invoke("What are the latest advances in quantum computing?");
``` ---------------------------------------- TITLE: Setting Up LangChain Agent Logic (TypeScript)
DESCRIPTION: This asynchronous immediately-invoked function expression (IIFE) sets up the core LangChain agent. It defines `TavilySearchResults` as the tool, pulls a pre-configured prompt from LangChain Hub, creates a `createToolCallingAgent` with the LLM, tools, and prompt, and finally initializes an `AgentExecutor` to manage the agent's execution.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#_snippet_6 LANGUAGE: typescript
CODE:
``` (async () => { const tools = [new TavilySearchResults({ maxResults: 1 })]; const prompt = await pull<ChatPromptTemplate>( "hwchase17/openai-tools-agent", ); const agent = createToolCallingAgent({ llm, tools, prompt, }); const agentExecutor = new AgentExecutor({ agent, tools, });
``` ---------------------------------------- TITLE: Invoking ChatOpenAI Model with a Simple Message in LangChain.js
DESCRIPTION: This code demonstrates how to invoke the initialized `ChatOpenAI` model with a basic user message. It sends a list containing a single user message and then accesses the `content` field of the response, showing a direct text generation interaction. This relies on the `model` object from the previous step.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#_snippet_7 LANGUAGE: javascript
CODE:
```
const response = await model.invoke([{ role: "user", content: "hi!"
}]); response.content;
``` ---------------------------------------- TITLE: Creating Multimodal Chains with ChatPromptTemplate in LangChain.js
DESCRIPTION: This snippet illustrates how to construct a `ChatPromptTemplate` that incorporates multimodal messages using `MessagesPlaceholder`. It then pipes this prompt with the `ChatOpenAI` model and a `StringOutputParser` to create a chain, demonstrating how to integrate vision capabilities into more complex LangChain workflows.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/openai_vision_multimodal.ipynb#_snippet_1 LANGUAGE: JavaScript
CODE:
```
import { ChatPromptTemplate, MessagesPlaceholder } from "npm:langchain@0.0.185/prompts";
import { StringOutputParser } from "npm:langchain@0.0.185/schema/output_parser"; const prompt = ChatPromptTemplate.fromMessages([ ["system", "Answer all questions like a pirate."], new MessagesPlaceholder("input"),
]); const chain = prompt.pipe(chat).pipe(new StringOutputParser()); await chain.invoke({ input: [ hostedImageMessage, ],
});
``` ---------------------------------------- TITLE: Creating ExaRetriever Tool and React Agent in JavaScript
DESCRIPTION: This snippet demonstrates how to set up an `ExaRetriever` using an `Exa` client and convert it into a LangChain tool. It then constructs a `createReactAgent` using the previously defined LLM and the newly created search tool. The agent is configured to use the `search` tool for retrieving web content.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/exa_search.ipynb#_snippet_9 LANGUAGE: javascript
CODE:
```
import Exa from "exa-js";
import { createRetrieverTool } from "langchain/tools/retriever";
import { ExaRetriever } from "@langchain/exa";
import { createReactAgent } from "@langchain/langgraph/prebuilt"; // @lc-ts-ignore
const agentClient = new Exa(process.env.EXASEARCH_API_KEY); const exaRetrieverForAgent = new ExaRetriever({ // @lc-ts-ignore client: agentClient, searchArgs: { numResults: 2, },
}); // Convert the ExaRetriever into a tool
const searchToolForAgent = createRetrieverTool(exaRetrieverForAgent, { name: "search", description: "Get the contents of a webpage given a string search query.",
}); const toolsForAgent = [searchToolForAgent]; const agentExecutor = createReactAgent({ llm: llmForAgent, tools: toolsForAgent,
})
``` ---------------------------------------- TITLE: Instantiating ChatOpenAI Model (JavaScript)
DESCRIPTION: This code demonstrates how to create a new instance of the `ChatOpenAI` model from the `@langchain/openai` package. It specifies the desired model (e.g., 'gpt-4o') and can include other configuration parameters like `temperature` to control response creativity.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#_snippet_2 LANGUAGE: javascript
CODE:
```
import { ChatOpenAI } from "@langchain/openai" const llm = new ChatOpenAI({ model: "gpt-4o", temperature: 0, // other params...
})
``` ---------------------------------------- TITLE: Streaming Agent Responses with LangGraph (JavaScript)
DESCRIPTION: This code demonstrates how to stream responses from a LangGraph agent executor. It sends a user query to the agent and iterates through the streamed events, logging either tool calls or the agent's content output as they become available.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/toolkits.ipynb#_snippet_5 LANGUAGE: JavaScript
CODE:
```
const exampleQuery = "..." const events = await agentExecutor.stream( { messages: [["user", exampleQuery]]}, { streamMode: "values", }
) for await (const event of events) { const lastMsg = event.messages[event.messages.length - 1]; if (lastMsg.tool_calls?.length) { console.dir(lastMsg.tool_calls, { depth: null }); } else if (lastMsg.content) { console.log(lastMsg.content); }
}
``` ---------------------------------------- TITLE: Defining a LangChain Tool with Artifact Support - JavaScript
DESCRIPTION: This snippet defines a LangChain tool named `generateRandomInts` that generates an array of random integers. It uses `zod` for schema validation and is configured with `responseFormat: "content_and_artifact"` to return both a model-facing content string and a hidden artifact (the array of numbers) as a tuple. This allows the tool to provide specific output to the model while making the full result available for downstream processing.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_artifacts.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
import { z } from "zod";
import { tool } from "@langchain/core/tools"; const randomIntToolSchema = z.object({ min: z.number(), max: z.number(), size: z.number(),
}); const generateRandomInts = tool(async ({ min, max, size }) => { const array: number[] = []; for (let i = 0; i < size; i++) { array.push(Math.floor(Math.random() * (max - min + 1)) + min); } return [ `Successfully generated array of ${size} random ints in [${min}, ${max}].`, array, ];
}, { name: "generateRandomInts", description: "Generate size random ints in the range [min, max].", schema: randomIntToolSchema, responseFormat: "content_and_artifact",
});
``` ---------------------------------------- TITLE: Initializing ChatOpenAI Language Model
DESCRIPTION: This snippet illustrates how to initialize a ChatOpenAI instance, specifying the language model to be used for summarization. It configures the model to 'gpt-4o-mini' with a temperature of 0, ensuring deterministic and consistent outputs for the summarization task.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#_snippet_3 LANGUAGE: JavaScript
CODE:
```
import { ChatOpenAI } from '@langchain/openai'; const llm = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0,
})
``` ---------------------------------------- TITLE: Setting OpenAI API Key Environment Variable (bash)
DESCRIPTION: This command sets the OPENAI_API_KEY environment variable, which is required for authenticating with the OpenAI API when using OpenAI models with Stagehand.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/stagehand.mdx#_snippet_2 LANGUAGE: bash
CODE:
```
export OPENAI_API_KEY="your-openai-api-key"
``` ---------------------------------------- TITLE: Defining a Chat Prompt Template for Tool Calling (JavaScript)
DESCRIPTION: This snippet constructs a ChatPromptTemplate using @langchain/core/prompts. It defines a system message that instructs the LLM on its role, the available tools (injected via {rendered_tools}), and the required JSON output format ({"name": "...", "arguments": {...}}). A user message placeholder {input} is also included.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_prompting.ipynb#_snippet_9 LANGUAGE: javascript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts"; const systemPrompt = `You are an assistant that has access to the following set of tools. Here are the names and descriptions for each tool: {rendered_tools} Given the user input, return the name and input of the tool to use. Return your response as a JSON blob with 'name' and 'arguments' keys.`; const prompt = ChatPromptTemplate.fromMessages( [["system", systemPrompt], ["user", "{input}"]]
)
``` ---------------------------------------- TITLE: Initializing Vector Store for Semantic Similarity Example Selection - JavaScript
DESCRIPTION: This snippet initializes a `MemoryVectorStore` with example data for use with `SemanticSimilarityExampleSelector`. It defines a set of input/output examples, prepares them for vectorization by concatenating input and output, and then uses `OpenAIEmbeddings` to create embeddings and populate the in-memory vector store. This setup is crucial for enabling semantic similarity-based example retrieval.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples_chat.ipynb#_snippet_4 LANGUAGE: JavaScript
CODE:
```
import { SemanticSimilarityExampleSelector } from "@langchain/core/example_selectors";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from '@langchain/openai'; const examples = [ { input: '2+2', output: '4' }, { input: '2+3', output: '5' }, { input: '2+4', output: '6' }, { input: 'What did the cow say to the moon?', output: 'nothing at all' }, { input: 'Write me a poem about the moon', output: 'One for the moon, and one for me, who are we to talk about the moon?', },
]; const toVectorize = examples.map((example) => `${example.input} ${example.output}`);
const embeddings = new OpenAIEmbeddings();
const vectorStore = await MemoryVectorStore.fromTexts(toVectorize, examples, embeddings);
``` ---------------------------------------- TITLE: Invoking a LangChain.js RAG Chain with a Query
DESCRIPTION: This snippet shows how to execute the previously defined `ragChain` with a specific query. It uses the `invoke` method to pass a question, triggering the retrieval and generation process configured in the RAG chain. The expected output is an answer based on the retrieved documents.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/weaviate.ipynb#_snippet_8 LANGUAGE: TypeScript
CODE:
```
await ragChain.invoke("Which movies are rated higher than 8.5?");
``` ---------------------------------------- TITLE: Invoking the RAG Chain (TypeScript)
DESCRIPTION: This code demonstrates how to execute the previously constructed RAG chain with a specific query. The chain will retrieve context using the Tavily retriever, pass it to the LLM along with the question, and return the generated answer.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/tavily.ipynb#_snippet_5 LANGUAGE: TypeScript
CODE:
```
await ragChain.invoke(query);
``` ---------------------------------------- TITLE: Invoking LLM with Tool-Relevant Input - TypeScript
DESCRIPTION: Illustrates invoking an LLM with an input that is relevant to a configured tool. The model is expected to identify the relevance and initiate a tool call, populating the `tool_calls` attribute in the `AIMessage` result.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tool_calling.mdx#_snippet_6 LANGUAGE: typescript
CODE:
```
const result = await llmWithTools.invoke("What is 2 multiplied by 3?");
``` ---------------------------------------- TITLE: Invoking ChatOpenAI with System and User Messages (JavaScript)
DESCRIPTION: This snippet illustrates how to invoke the instantiated `ChatOpenAI` model with a list of messages. It includes a 'system' role message to define the AI's behavior and a 'user' role message providing the input, demonstrating a basic chat interaction.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#_snippet_3 LANGUAGE: javascript
CODE:
```
const aiMsg = await llm.invoke([ { role: "system", content: "You are a helpful assistant that translates English to French. Translate the user sentence.", }, { role: "user", content: "I love programming." },
])
aiMsg
``` ---------------------------------------- TITLE: Binding Tools to a LangChain Model - JavaScript
DESCRIPTION: This snippet demonstrates how to bind the `generateRandomInts` tool to a language model (`llm`) using `llm.bindTools()`. It then invokes the model with a natural language prompt, expecting the model to generate tool calls. The `aiMessage.tool_calls` property will contain the tool call objects suggested by the model, which can then be executed.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_artifacts.ipynb#_snippet_3 LANGUAGE: JavaScript
CODE:
```
const llmWithTools = llm.bindTools([generateRandomInts]) const aiMessage = await llmWithTools.invoke("generate 6 positive ints less than 25")
aiMessage.tool_calls
``` ---------------------------------------- TITLE: Implementing Fallback Chains for Robust LangChain.js Tool Calling
DESCRIPTION: This snippet demonstrates how to use fallbacks to improve the robustness of tool calling. It defines a `badChain` (using `gpt-3.5-turbo`) and a `betterChain` (using `gpt-4-1106-preview`) with the same `complexTool`. The `badChain` is configured with `withFallbacks` to automatically switch to the `betterChain` if the initial invocation fails, allowing for more resilient tool execution.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_error.ipynb#_snippet_3 LANGUAGE: JavaScript
CODE:
```
const badChain = llmWithTools .pipe((message) => message.tool_calls?.[0].args) .pipe(complexTool); const betterModel = new ChatOpenAI({ model: "gpt-4-1106-preview", temperature: 0,
}).bindTools([complexTool]); const betterChain = betterModel .pipe((message) => message.tool_calls?.[0].args) .pipe(complexTool); const chainWithFallback = badChain.withFallbacks([betterChain]); await chainWithFallback.invoke("use complex tool. the args are 5, 2.1, potato");
``` ---------------------------------------- TITLE: Streaming Responses with AzureChatOpenAI
DESCRIPTION: This TypeScript example illustrates how to use the `stream` method of `AzureChatOpenAI` to receive responses incrementally. It's useful for applications requiring real-time output or handling large responses efficiently.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-azure-openai/README.md#_snippet_4 LANGUAGE: typescript
CODE:
```
import { AzureChatOpenAI } from "@langchain/azure-openai"; const model = new AzureChatOpenAI({ // Note that the following are optional, and will default to the values below // if not provided. azureOpenAIEndpoint: process.env.AZURE_OPENAI_API_ENDPOINT, azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY, azureOpenAIApiDeploymentName: process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME,
});
const response = await model.stream(new HumanMessage("Hello world!"));
``` ---------------------------------------- TITLE: Invoking Chat Model with Multiple Tool Calls (JavaScript)
DESCRIPTION: This snippet illustrates how a chat model can handle and generate multiple tool calls in a single invocation. It sends a query requiring two distinct arithmetic operations, expecting the model to generate two separate tool calls. The `.tool_calls` attribute of the response object is then accessed to inspect the generated tool calls.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calling.ipynb#_snippet_2 LANGUAGE: JavaScript
CODE:
```
const res = await llmWithTools.invoke("What is 3 * 12? Also, what is 11 + 49?"); res.tool_calls;
``` ---------------------------------------- TITLE: Implementing Custom Token Counting and Imperative Message Trimming in LangChain.js
DESCRIPTION: This snippet defines two asynchronous functions, `strTokenCounter` and `tiktokenCounter`, for calculating token counts of message content and a list of messages, respectively, using the 'gpt-4' model's encoding. It then demonstrates the imperative use of `trimMessages` with a custom `tiktokenCounter` to limit the total tokens in a message array.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/trim_messages.ipynb#_snippet_5 LANGUAGE: TypeScript
CODE:
```
import { encodingForModel } from '@langchain/core/utils/tiktoken';
import { BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage, MessageContent, MessageContentText } from '@langchain/core/messages'; async function strTokenCounter(messageContent: MessageContent): Promise<number> { if (typeof messageContent === 'string') { return ( await encodingForModel("gpt-4") ).encode(messageContent).length; } else { if (messageContent.every((x) => x.type === "text" && x.text)) { return ( await encodingForModel("gpt-4") ).encode((messageContent as MessageContentText[]).map(({ text }) => text).join("")).length; } throw new Error(`Unsupported message content ${JSON.stringify(messageContent)}`); }
} async function tiktokenCounter(messages: BaseMessage[]): Promise<number> { let numTokens = 3; // every reply is primed with <|start|>assistant<|message|> const tokensPerMessage = 3; const tokensPerName = 1; for (const msg of messages) { let role: string; if (msg instanceof HumanMessage) { role = 'user'; } else if (msg instanceof AIMessage) { role = 'assistant'; } else if (msg instanceof ToolMessage) { role = 'tool'; } else if (msg instanceof SystemMessage) { role = 'system'; } else { throw new Error(`Unsupported message type ${msg.constructor.name}`); } numTokens += tokensPerMessage + (await strTokenCounter(role)) + (await strTokenCounter(msg.content)); if (msg.name) { numTokens += tokensPerName + (await strTokenCounter(msg.name)); } } return numTokens;
} await trimMessages(messages, { maxTokens: 45, strategy: 'last', tokenCounter: tiktokenCounter,
});
``` ---------------------------------------- TITLE: Implementing a Tool-Calling Chain with SerpAPI and LangChain.js
DESCRIPTION: This comprehensive snippet constructs a tool-calling chain using `ChatPromptTemplate`, `RunnableLambda`, and the `SerpAPI` tool. It defines a prompt, binds the tool to the LLM, and creates a runnable that handles user input, invokes the LLM, processes tool calls, and generates a final response.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/serpapi.ipynb#_snippet_6 LANGUAGE: typescript
CODE:
```
import { HumanMessage } from "@langchain/core/messages";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableLambda } from "@langchain/core/runnables"; const prompt = ChatPromptTemplate.fromMessages( [ ["system", "You are a helpful assistant."], ["placeholder", "{messages}"], ]
) const llmWithTools = llm.bindTools([tool]); const chain = prompt.pipe(llmWithTools); const toolChain = RunnableLambda.from( async (userInput: string, config) => { const humanMessage = new HumanMessage(userInput,); const aiMsg = await chain.invoke({ messages: [new HumanMessage(userInput)], }, config); const toolMsgs = await tool.batch(aiMsg.tool_calls, config); return chain.invoke({ messages: [humanMessage, aiMsg, ...toolMsgs], }, config); }
); const toolChainResult = await toolChain.invoke("what is the current weather in sf?");
``` ---------------------------------------- TITLE: Defining and Binding Custom Tools with Gemini Models (TypeScript)
DESCRIPTION: This snippet illustrates how to define and bind a custom tool (fakeBrowserTool) to a ChatGoogleGenerativeAI model in LangChain.js. It uses zod for schema validation, explicitly defining url and query parameters. The example then shows how to invoke the model with a human prompt that triggers the tool, demonstrating the tool-calling capability and logging the tool_calls result.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_generativeai.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { tool } from "@langchain/core/tools";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { z } from "zod"; // Define your tool
const fakeBrowserTool = tool((_) => { return "The search result is xyz..."
}, { name: "browser_tool", description: "Useful for when you need to find something on the web or summarize a webpage.", schema: z.object({ url: z.string().describe("The URL of the webpage to search."), query: z.string().optional().describe("An optional search query to use."), }),
}) const llmWithTool = new ChatGoogleGenerativeAI({ model: "gemini-pro",
}).bindTools([fakeBrowserTool]) // Bind your tools to the model const toolRes = await llmWithTool.invoke([ [ "human", "Search the web and tell me what the weather will be like tonight in new york. use a popular weather website", ],
]); console.log(toolRes.tool_calls);
``` ---------------------------------------- TITLE: Querying SelfQueryRetriever with Combined Filters - LangChain.js
DESCRIPTION: This advanced example demonstrates the `SelfQueryRetriever`'s ability to process complex natural language queries involving multiple conditions and logical operators (e.g., 'comedy or drama' and 'less than 90 minutes'). The retriever translates these into appropriate vector store filters.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/self_query.ipynb#_snippet_5 LANGUAGE: TypeScript
CODE:
```
await selfQueryRetriever.invoke( "Which movies are either comedy or drama and are less than 90 minutes?"
);
``` ---------------------------------------- TITLE: Creating a Postgres Document Loader Instance (TypeScript)
DESCRIPTION: This snippet demonstrates how to initialize a 'PostgresLoader' instance for loading data from a PostgreSQL table into LangChain 'Document's. It specifies the table name, content columns, metadata columns, and format for data extraction.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/google.mdx#_snippet_22 LANGUAGE: typescript
CODE:
```
const documentLoaderArgs: PostgresLoaderOptions = { tableName: "test_table_custom", contentColumns: ["fruit_name", "variety"], metadataColumns: [ "fruit_id", "quantity_in_stock", "price_per_unit", "organic" ], format: "text"
}; const documentLoaderInstance = await PostgresLoader.initialize( PEInstance, documentLoaderArgs
); const documents = await documentLoaderInstance.load();
``` ---------------------------------------- TITLE: Setting up RAG Chain with Vectara Self-Query Retriever (TypeScript)
DESCRIPTION: This snippet demonstrates how to construct a Retrieval Augmented Generation (RAG) chain using LangChain. It defines a chat prompt, a document formatter, and then sequences the self-query retriever with the prompt and an LLM to answer questions based on retrieved context. It highlights how retrieved documents are formatted to include metadata.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/vectara.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } "@langchain/core/output_parsers"; import type { Document } from "@langchain/core/documents"; const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided. Context: {context} Question: {question}`); const formatDocs = (docs: Document[]) => { return docs.map((doc) => JSON.stringify(doc)).join("\n\n");
} // See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([ { context: selfQueryRetriever.pipe(formatDocs), question: new RunnablePassthrough(), }, prompt, llm, new StringOutputParser(),
]);
``` ---------------------------------------- TITLE: Composing a Chat Prompt Template with Messages in LangChain (JS)
DESCRIPTION: This snippet demonstrates composing a `ChatPromptTemplate` using `HumanMessagePromptTemplate.fromTemplate`. It combines a pre-existing `SystemMessage` (`prompt`) with static `HumanMessage` and `AIMessage` instances, and a dynamic `input` placeholder. This allows for building complex chat sequences with both fixed and variable parts.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/prompts_composition.ipynb#_snippet_3 LANGUAGE: JavaScript
CODE:
```
import { HumanMessagePromptTemplate } from "@langchain/core/prompts" const newPrompt = HumanMessagePromptTemplate.fromTemplate([prompt, new HumanMessage("Hi"), new AIMessage("what?"), "{input}"])
``` ---------------------------------------- TITLE: Defining a ChatPromptTemplate with Examples in LangChain (JavaScript)
DESCRIPTION: This snippet defines a `ChatPromptTemplate` using LangChain's core prompts. It includes a system prompt for an expert extraction algorithm and a `MessagesPlaceholder` to dynamically insert reference examples, enhancing extraction quality. The prompt is designed to take input `text` and `examples`.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_examples.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts"; const SYSTEM_PROMPT_TEMPLATE = `You are an expert extraction algorithm.
Only extract relevant information from the text.
If you do not know the value of an attribute asked to extract, you may omit the attribute's value.`; // Define a custom prompt to provide instructions and any additional context.
// 1) You can add examples into the prompt template to improve extraction quality
// 2) Introduce additional parameters to take context into account (e.g., include metadata
// about the document from which the text was extracted.)
const prompt = ChatPromptTemplate.fromMessages([ ["system", SYSTEM_PROMPT_TEMPLATE], // ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ new MessagesPlaceholder("examples"), // ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ ["human", "{text}"]
]);
``` ---------------------------------------- TITLE: Initializing ChatOpenAI Model (JavaScript)
DESCRIPTION: This snippet initializes a ChatOpenAI instance from `@langchain/openai` to be used as the Language Model (LLM) in the application. It configures the model to `gpt-4o` with a temperature of 0 for deterministic responses. This LLM will be invoked by the LangGraph application.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/message_history.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai"; const llm = new ChatOpenAI({ model: "gpt-4o", temperature: 0,
});
``` ---------------------------------------- TITLE: Reusing MariaDB Connections with Connection Pooling in LangChain.js
DESCRIPTION: Illustrates how to reuse MariaDB database connections by creating a connection pool and passing it to `MariaDBStore` instances. This example initializes a pool, configures `MariaDBStore` with it, adds documents, performs a similarity search, and then demonstrates creating another `MariaDBStore` instance using the same pool for a different collection, finally closing the pool.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mariadb.ipynb#_snippet_10 LANGUAGE: JavaScript
CODE:
```
import { OpenAIEmbeddings } from "@langchain/openai";
import { MariaDBStore } from "@langchain/community/vectorstores/mariadb";
import mariadb from "mariadb"; // First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/mariadb const reusablePool = mariadb.createPool({ host: "127.0.0.1", port: 3306, user: "myuser", password: "ChangeMe", database: "api",
}); const originalConfig = { pool: reusablePool, tableName: "testlangchainjs", collectionName: "sample", collectionTableName: "collections", columns: { idColumnName: "id", vectorColumnName: "vect", contentColumnName: "content", metadataColumnName: "metadata", },
}; // Set up the DB.
// Can skip this step if you've already initialized the DB.
// await MariaDBStore.initialize(new OpenAIEmbeddings(), originalConfig);
const mariadbStore = new MariaDBStore(new OpenAIEmbeddings(), originalConfig); await mariadbStore.addDocuments([ { pageContent: "what's this", metadata: { a: 2 } }, { pageContent: "Cat drinks milk", metadata: { a: 1 } },
]); const results = await mariadbStore.similaritySearch("water", 1); console.log(results); /* [ Document { pageContent: 'Cat drinks milk', metadata: { a: 1 } } ]
*/ const mariadbStore2 = new MariaDBStore(new OpenAIEmbeddings(), { pool: reusablePool, tableName: "testlangchainjs", collectionTableName: "collections", collectionName: "some_other_collection", columns: { idColumnName: "id", vectorColumnName: "vector", contentColumnName: "content", metadataColumnName: "metadata", },
}); const results2 = await mariadbStore2.similaritySearch("water", 1); console.log(results2); /* []
*/ await reusablePool.end();
``` ---------------------------------------- TITLE: Initializing ChatOpenAI for Chaining
DESCRIPTION: This snippet initializes a `ChatOpenAI` instance, specifying the `gpt-4o-mini` model. This language model will be used in conjunction with the tool within a LangChain runnable sequence.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/tools.ipynb#_snippet_5 LANGUAGE: typescript
CODE:
```
import { ChatOpenAI } from "@langchain/openai" const llm = new ChatOpenAI({ model: "gpt-4o-mini"
})
``` ---------------------------------------- TITLE: Configuring OpenAI Agent with Custom Tool (JavaScript)
DESCRIPTION: This comprehensive snippet sets up an OpenAI agent using LangChain Expression Language. It initializes a `ChatOpenAI` model, binds the custom `informationTool` as a function for the LLM, and constructs a `ChatPromptTemplate` with system instructions, chat history, and agent scratchpad placeholders. It also includes a helper function `_formatChatHistory` to structure chat messages and defines the `RunnableSequence` for the agent's execution flow, culminating in an `AgentExecutor`.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_semantic.ipynb#_snippet_5 LANGUAGE: JavaScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { AgentExecutor } from "langchain/agents";
import { formatToOpenAIFunctionMessages } from "langchain/agents/format_scratchpad";
import { OpenAIFunctionsAgentOutputParser } from "langchain/agents/openai/output_parser";
import { convertToOpenAIFunction } from "@langchain/core/utils/function_calling";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
import { AIMessage, BaseMessage, HumanMessage } from "@langchain/core/messages";
import { RunnableSequence } from "@langchain/core/runnables"; const llm = new ChatOpenAI({ model: "gpt-3.5-turbo", temperature: 0 })
const tools = [informationTool] const llmWithTools = llm.bind({ functions: tools.map(convertToOpenAIFunction),
}) const prompt = ChatPromptTemplate.fromMessages( [ [ "system", "You are a helpful assistant that finds information about movies and recommends them. If tools require follow up questions, make sure to ask the user for clarification. Make sure to include any available options that need to be clarified in the follow up questions Do only the things the user specifically requested." ], new MessagesPlaceholder("chat_history"), ["human", "{input}"], new MessagesPlaceholder("agent_scratchpad"), ]
) const _formatChatHistory = (chatHistory) => { const buffer: Array<BaseMessage> = [] for (const [human, ai] of chatHistory) { buffer.push(new HumanMessage({ content: human })) buffer.push(new AIMessage({ content: ai })) } return buffer
} const agent = RunnableSequence.from([ { input: (x) => x.input, chat_history: (x) => { if ("chat_history" in x) { return _formatChatHistory(x.chat_history); } return []; }, agent_scratchpad: (x) => { if ("steps" in x) { return formatToOpenAIFunctionMessages( x.steps ); } return []; }, }, prompt, llmWithTools, new OpenAIFunctionsAgentOutputParser(),
]) const agentExecutor = new AgentExecutor({ agent, tools });
``` ---------------------------------------- TITLE: Invoking LangGraph with Follow-up Query (TypeScript)
DESCRIPTION: This snippet demonstrates a follow-up invocation of the graphWithMemory, sending a new user query that builds upon the previous conversation. Because the graph is configured with a checkpointer and the same threadConfig, the model can access the full conversational context, allowing it to generate contextually relevant responses.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#_snippet_17 LANGUAGE: TypeScript
CODE:
```
let inputs4 = { messages: [{ role: "user", content: "Can you look up some common ways of doing it?" }] }; for await ( const step of await graphWithMemory.stream(inputs4, threadConfig)
) { const lastMessage = step.messages[step.messages.length - 1]; prettyPrint(lastMessage); console.log("-----\n");
}
``` ---------------------------------------- TITLE: Reranking Retrieved Documents with WatsonxRerank (JavaScript)
DESCRIPTION: This snippet demonstrates how to use `WatsonxRerank` to reorder and score previously retrieved documents based on a given query. It initializes the reranker with specific version, service URL, project ID, and model, then applies it to the `result` from the retriever.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/ibm.ipynb#_snippet_9 LANGUAGE: JavaScript
CODE:
```
import { WatsonxRerank } from "@langchain/community/document_compressors/ibm"; const reranker = new WatsonxRerank({ version: "2024-05-31", serviceUrl: process.env.WATSONX_AI_SERVICE_URL, projectId: process.env.WATSONX_AI_PROJECT_ID, model: "cross-encoder/ms-marco-minilm-l-12-v2",
});
const compressed = await reranker.rerank(result, query);
console.log(compressed);
``` ---------------------------------------- TITLE: Basic RunnablePassthrough Usage with RunnableParallel (JavaScript)
DESCRIPTION: This snippet demonstrates the fundamental functionality of `RunnablePassthrough` when used with `RunnableParallel`. It shows how an input object is passed through unchanged by `RunnablePassthrough`, while another part of the parallel runnable can simultaneously modify a different aspect of the input.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/passthrough.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
import { RunnableParallel, RunnablePassthrough } from "@langchain/core/runnables"; const runnable = RunnableParallel.from({ passed: new RunnablePassthrough<{ num: number }>(), modified: (input: { num: number }) => input.num + 1,
}); await runnable.invoke({ num: 1 });
``` ---------------------------------------- TITLE: Invoking a LangChain.js RAG Chain with a Query
DESCRIPTION: This snippet shows how to execute the previously defined `ragChain` with a specific question. It uses the `invoke` method to pass the query 'Which movies are rated higher than 8.5?' to the chain, triggering the retrieval and generation process. This demonstrates the chain's ability to answer questions based on the retrieved context.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/hnswlib.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
await ragChain.invoke("Which movies are rated higher than 8.5?")
``` ---------------------------------------- TITLE: Invoking SerpAPI Tool with a Model-Generated ToolCall in LangChain.js
DESCRIPTION: This snippet illustrates invoking the `SerpAPI` tool using a `ToolCall` object, which is typically generated by a language model. The `ToolCall` includes `args` for the input, an `id`, the tool's `name`, and its `type`.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/serpapi.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
// This is usually generated by a model, but we'll create a tool call directly for demo purposes.
const modelGeneratedToolCall = { args: { input: "what is the current weather in SF?" }, id: "1", name: tool.name, type: "tool_call",
} await tool.invoke(modelGeneratedToolCall)
``` ---------------------------------------- TITLE: Chaining Models with Prompt Templates in LangChain.js
DESCRIPTION: This snippet demonstrates how to create a chat prompt template and chain it with a language model (llm) in LangChain.js. It shows how to define system and human messages with placeholders and then invoke the chain with specific input values.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/azure.ipynb#_snippet_4 LANGUAGE: TypeScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts" const prompt = ChatPromptTemplate.fromMessages( [ [ "system", "You are a helpful assistant that translates {input_language} to {output_language}." ], ["human", "{input}"] ]
) const chain = prompt.pipe(llm);
await chain.invoke( { input_language: "English", output_language: "German", input: "I love programming." }
)
``` ---------------------------------------- TITLE: Defining Tools and Model in LangChain.js
DESCRIPTION: This snippet defines two simple tools, `add` and `multiply`, using `zod` for schema validation and `@langchain/core/tools`. It then initializes a `ChatOpenAI` model and binds these tools to it, preparing the model for tool-enabled interactions. Dependencies include `zod`, `@langchain/core/tools`, and `@langchain/openai`.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_streaming.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
import { z } from "zod";
import { tool } from "@langchain/core/tools";
import { ChatOpenAI } from "@langchain/openai"; const addTool = tool(async (input) => { return input.a + input.b;
}, { name: "add", description: "Adds a and b.", schema: z.object({ a: z.number(), b: z.number(), }),
}); const multiplyTool = tool(async (input) => { return input.a * input.b;
}, { name: "multiply", description: "Multiplies a and b.", schema: z.object({ a: z.number(), b: z.number(), }),
}); const tools = [addTool, multiplyTool]; const model = new ChatOpenAI({ model: "gpt-4o", temperature: 0,
}); const modelWithTools = model.bindTools(tools);
``` ---------------------------------------- TITLE: Specifying Output Method (JSON Mode) for Structured Output
DESCRIPTION: This advanced example shows how to explicitly specify the output method, such as 'json_mode' for OpenAI models, when using `withStructuredOutput()`. This allows fine-grained control over how the model generates structured data, often paired with a more specific prompt.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#_snippet_3 LANGUAGE: JavaScript
CODE:
```
const structuredLlm = model.withStructuredOutput(joke, { method: "json_mode", name: "joke"
}) await structuredLlm.invoke( "Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys"
)
``` ---------------------------------------- TITLE: Creating and Invoking a Translation Chain with ChatPromptTemplate (JavaScript)
DESCRIPTION: This snippet demonstrates how to construct a LangChain.js chain for language translation. It initializes a ChatPromptTemplate with system and human messages, then pipes it with an undefined 'llm' (language model) to form a chain. The chain is subsequently invoked with specific input and output languages, along with the text to be translated.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cerebras.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts" const prompt = ChatPromptTemplate.fromMessages( [ [ "system", "You are a helpful assistant that translates {input_language} to {output_language}." ], ["human", "{input}"] ]
) const chain = prompt.pipe(llm);
await chain.invoke( { input_language: "English", output_language: "German", input: "I love programming." }
)
``` ---------------------------------------- TITLE: Enabling Built-in Code Execution Tool (TypeScript)
DESCRIPTION: This snippet demonstrates how to enable the built-in CodeExecutionTool for a ChatGoogleGenerativeAI model. By simply passing an empty object to codeExecution, the model gains the ability to generate and execute code. The example shows invoking the model with a mathematical problem, expecting it to use code execution to find the sum, and then logging the result.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_generativeai.ipynb#_snippet_10 LANGUAGE: TypeScript
CODE:
```
import { CodeExecutionTool } from "@google/generative-ai";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai"; const codeExecutionTool: CodeExecutionTool = { codeExecution: {}, // Simply pass an empty object to enable it.
};
const codeExecutionModel = new ChatGoogleGenerativeAI({ model: "gemini-1.5-pro", temperature: 0, maxRetries: 0,
}).bindTools([codeExecutionTool]); const codeExecutionResult = await codeExecutionModel.invoke("Use code execution to find the sum of the first and last 3 numbers in the following list: [1, 2, 3, 72638, 8, 727, 4, 5, 6]"); console.dir(codeExecutionResult.content, { depth: null });
``` ---------------------------------------- TITLE: Constructing a RAG Chain with LangChain.js
DESCRIPTION: This code snippet shows how to build a Retrieval-Augmented Generation (RAG) chain using LangChain.js components. It defines a prompt template, a document formatting function, and then constructs a `RunnableSequence` that pipes the retriever's output into the prompt, followed by an LLM and a string output parser.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/retrievers.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers"; import type { Document } from "@langchain/core/documents"; const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided. Context: {context} Question: {question}`); const formatDocs = (docs: Document[]) => { return docs.map((doc) => doc.pageContent).join("\n\n");
} // See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([ { context: retriever.pipe(formatDocs), question: new RunnablePassthrough(), }, prompt, llm, new StringOutputParser(),
]);
``` ---------------------------------------- TITLE: Creating In-Memory Vector Store with OpenAI Embeddings
DESCRIPTION: This code creates an in-memory vector store using `MemoryVectorStore`. It takes the previously split document chunks (`allSplits`) and generates embeddings for them using `OpenAIEmbeddings`. The resulting `vectorstore` allows for efficient similarity searches on the document content.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#_snippet_3 LANGUAGE: JavaScript
CODE:
```
import { OpenAIEmbeddings } from "@langchain/openai";
import { MemoryVectorStore } from "langchain/vectorstores/memory"; const vectorstore = await MemoryVectorStore.fromDocuments( allSplits, new OpenAIEmbeddings()
);
``` ---------------------------------------- TITLE: Performing Similarity Search with Embedded Query (LangChain.js)
DESCRIPTION: This snippet illustrates how to perform a similarity search using a pre-computed embedding of the query. First, the query string is converted into an embedding using `embeddings.embedQuery`, then `similaritySearchVectorWithScore` is used to find similar documents based on this vector, returning results with scores.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#_snippet_12 LANGUAGE: JavaScript
CODE:
```
const embedding = await embeddings.embedQuery( "How were Nike's margins impacted in 2023?"
) const results3 = await vectorStore.similaritySearchVectorWithScore( embedding, 1
) results3[0]
``` ---------------------------------------- TITLE: Generating AI Response with Retrieved Context (TypeScript)
DESCRIPTION: This asynchronous function processes recent ToolMessage instances to format retrieved content into a system message. It constructs a prompt by combining the system message with relevant conversation history and invokes a language model (llm) to generate a response, returning it as a message array.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#_snippet_9 LANGUAGE: typescript
CODE:
```
// Step 3: Generate a response using the retrieved content.
async function generate(state: typeof MessagesAnnotation.State) { // Get generated ToolMessages let recentToolMessages = []; for (let i = state["messages"].length - 1; i >= 0; i--) { let message = state["messages"][i]; if (message instanceof ToolMessage) { recentToolMessages.push(message); } else { break; } } let toolMessages = recentToolMessages.reverse(); // Format into prompt const docsContent = toolMessages.map(doc => doc.content).join("\n"); const systemMessageContent = "You are an assistant for question-answering tasks. " + "Use the following pieces of retrieved context to answer " + "the question. If you don't know the answer, say that you " + "don't know. Use three sentences maximum and keep the " + "answer concise." + "\n\n" + `${docsContent}`; const conversationMessages = state.messages.filter(message => message instanceof HumanMessage || message instanceof SystemMessage || (message instanceof AIMessage && message.tool_calls.length == 0) ); const prompt = [new SystemMessage(systemMessageContent), ...conversationMessages]; // Run const response = await llm.invoke(prompt) return { messages: [response] };
}
``` ---------------------------------------- TITLE: Converting a Function to a Runnable with RunnableLambda (TypeScript)
DESCRIPTION: Demonstrates how to wrap a standard JavaScript/TypeScript function into a `Runnable` using `RunnableLambda.from()`. This allows any pure function to be integrated into LCEL chains, making it invokable, batchable, and streamable like other runnables.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { RunnableLambda } from "@langchain/core/runnables"; const adder = (x: number) => { return x + 5;
}; const runnable = RunnableLambda.from(adder); await runnable.invoke(5);
``` ---------------------------------------- TITLE: Passing ConsoleCallbackHandler at Runtime in LangChain.js
DESCRIPTION: This snippet demonstrates how to pass a `ConsoleCallbackHandler` to a LangChain chain at runtime. It initializes a prompt and a model, pipes them into a chain, and then invokes the chain, attaching the handler via the `callbacks` keyword argument to log execution details to the console.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/callbacks_runtime.ipynb#_snippet_0 LANGUAGE: TypeScript
CODE:
```
import { ConsoleCallbackHandler } from "@langchain/core/tracers/console";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatAnthropic } from "@langchain/anthropic"; const handler = new ConsoleCallbackHandler(); const prompt = ChatPromptTemplate.fromTemplate(`What is 1 + {number}?`);
const model = new ChatAnthropic({ model: "claude-3-sonnet-20240229",
}); const chain = prompt.pipe(model); await chain.invoke({ number: "2" }, { callbacks: [handler] });
``` ---------------------------------------- TITLE: Initializing Chat Model with LangChain (JavaScript)
DESCRIPTION: This snippet initializes a ChatOpenAI instance from LangChain, configuring it to use the 'gpt-4o-mini' model with a temperature of 0. This model will be used for generating SQL queries and answers. It depends on the `@langchain/openai` package.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#_snippet_5 LANGUAGE: JavaScript
CODE:
```
// @lc-docs-hide-cell
import { ChatOpenAI } from '@langchain/openai'; const llm = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0,
})
``` ---------------------------------------- TITLE: Performing Direct Similarity Search with Filters in LangChain.js
DESCRIPTION: Demonstrates how to perform a basic similarity search on a vector store using `similaritySearch` with a filter. It retrieves documents similar to 'biology', limited to 2 results, and filters by documents with `year: 2021` in their metadata. The results, including page content and metadata, are then logged to the console.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mariadb.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, { "year": 2021 });
for (const doc of similaritySearchResults) { console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
``` ---------------------------------------- TITLE: Streaming Internal Events from a Runnable in LangChain.js
DESCRIPTION: Illustrates how to use `streamEvents()` to receive a stream of internal events generated during a runnable's execution. This provides detailed insights into the chain's progress and intermediate steps for monitoring and debugging.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#_snippet_15 LANGUAGE: TypeScript
CODE:
```
import { RunnableLambda } from "@langchain/core/runnables"; const runnable1 = RunnableLambda.from((x: number) => { return { foo: x, };
}).withConfig({ runName: "first",
}); async function* generatorFn(x: { foo: number }) { for (let i = 0; i < x.foo; i++) { yield i.toString(); }
} const runnable2 = RunnableLambda.from(generatorFn).withConfig({ runName: "second",
}); const chain = runnable1.pipe(runnable2); for await (const event of chain.streamEvents(2, { version: "v1" })) { console.log(`event=${event.event} | name=${event.name} | data=${JSON.stringify(event.data)}`);
}
``` ---------------------------------------- TITLE: Invoking OpenAI Chat Model with Caching and Long Context in LangChain.js
DESCRIPTION: This snippet demonstrates how to initialize and invoke an OpenAI chat model (gpt-4o-mini-2024-07-18) using LangChain.js, incorporating a long system context. It shows how to structure messages for the model, including a system role with extensive text and a user query, and then logs the token usage from the model's response metadata.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#_snippet_24 LANGUAGE: JavaScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai"; const modelWithCaching = new ChatOpenAI({ model: "gpt-4o-mini-2024-07-18",
}); // CACHED_TEXT is some string longer than 1024 tokens
const LONG_TEXT = `You are a pirate. Always respond in pirate dialect.\n\nUse the following as context when answering questions:\n\n${CACHED_TEXT}`; const longMessages = [ { role: "system", content: LONG_TEXT, }, { role: "user", content: "What types of messages are supported in LangChain?", },
]; const originalRes = await modelWithCaching.invoke(longMessages); console.log("USAGE:", originalRes.response_metadata.usage);
``` ---------------------------------------- TITLE: Handling All Expected Tool Responses (TypeScript)
DESCRIPTION: This snippet demonstrates the correct way to continue a conversation after a model's `AIMessage` contains multiple `tool_calls`. It adds the second required `ToolMessage` to the `chatHistory`, ensuring that there is one `ToolMessage` for each `tool_call` from the previous `AIMessage`, allowing the model invocation to succeed.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/troubleshooting/errors/INVALID_TOOL_RESULTS.ipynb#_snippet_2 LANGUAGE: TypeScript
CODE:
```
const toolResponse2 = await dummyTool.invoke(responseMessage.tool_calls![1]); chatHistory.push(toolResponse2); await modelWithTools.invoke(chatHistory);
``` ---------------------------------------- TITLE: Implementing MatryoshkaRetriever with OpenAI and Chroma (TypeScript)
DESCRIPTION: Demonstrates how to initialize and use the `MatryoshkaRetriever` with OpenAI embeddings and a Chroma vector store for adaptive retrieval. This setup enables a two-pass search for reduced latency, first using lower-dimensional sub-vectors and then re-ranking with full embeddings. It requires a running Chroma instance.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/reduce_retrieval_latency.mdx#_snippet_2 LANGUAGE: typescript
CODE:
```
import { OpenAIEmbeddings } from "@langchain/openai";
import { Chroma } from "@langchain/community/vectorstores/chroma";
import { MatryoshkaRetriever } from "@langchain/community/retrievers/matryoshka_retriever";
import { Document } from "@langchain/core/documents"; async function runMatryoshkaRetrieverExample() { // Initialize OpenAI Embeddings const embeddings = new OpenAIEmbeddings(); // For demonstration, create a dummy Chroma vector store // In a real application, you would load an existing one or populate it. const docs = [ new Document({ pageContent: "LangChain is a framework for developing applications powered by language models." }), new Document({ pageContent: "Matryoshka embeddings enable faster vector search." }), new Document({ pageContent: "Adaptive Retrieval uses a two-pass approach for efficiency." }), new Document({ pageContent: "Chroma is a popular open-source vector database." }) ]; const vectorStore = await Chroma.fromDocuments(docs, embeddings, { collectionName: "matryoshka_example", url: "http://localhost:8000" // Ensure Chroma is running }); // Initialize MatryoshkaRetriever const retriever = new MatryoshkaRetriever({ vectorstore: vectorStore, embeddings: embeddings, // k: Number of documents for the first pass (lower-dimensional search) k: 5, // m: Number of top documents from the first pass to re-rank with full embeddings m: 2 }); // Perform a retrieval query const query = "What is adaptive retrieval?"; const relevantDocuments = await retriever.getRelevantDocuments(query); console.log("Retrieved documents using MatryoshkaRetriever:"); for (const doc of relevantDocuments) { console.log(`- ${doc.pageContent}`); }
} runMatryoshkaRetrieverExample().catch(console.error);
``` ---------------------------------------- TITLE: Initializing ChatOpenAI Model (JavaScript)
DESCRIPTION: This code initializes a `ChatOpenAI` instance from the `@langchain/openai` package. It configures the model to use 'gpt-4o' with a temperature of 0, ensuring deterministic responses for consistent output.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#_snippet_1 LANGUAGE: JavaScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai"; const model = new ChatOpenAI({ model: "gpt-4o", temperature: 0,
});
``` ---------------------------------------- TITLE: Complete Jina Embeddings Usage Example (TypeScript)
DESCRIPTION: This comprehensive example demonstrates the full workflow of using `JinaEmbeddings`. It initializes the embedding client with an API key and a specific model, then performs both single query embedding using `embedQuery` and multiple document embedding using `embedDocuments`. The example showcases handling various document types, including text and image data, and logs the resulting embeddings.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/jina.mdx#_snippet_5 LANGUAGE: typescript
CODE:
```
import { JinaEmbeddings } from "@langchain/community/embeddings/jina";
import { localImageToBase64 } from "@langchain/community/embeddings/jina/util"; const embeddings = new JinaEmbeddings({ apiKey: "YOUR_API_TOKEN", model: "jina-embeddings-v2-base-en"
}); async function runExample() { const queryEmbedding = await embeddings.embedQuery("Example query text."); console.log("Query Embedding:", queryEmbedding); const documents = [ "hello", { text: "hello" }, { image: "https://i.ibb.co/nQNGqL0/beach1.jpg" }, { image: await localImageToBase64("beach1.jpg") } ]; const documentEmbeddings = await embeddings.embedDocuments(documents); console.log("Document Embeddings:", documentEmbeddings);
} runExample();
``` ---------------------------------------- TITLE: Initializing ChatOpenAI Model
DESCRIPTION: Initializes a `ChatOpenAI` instance from `@langchain/openai` with a specified model (`gpt-4o-mini`) and a temperature of 0. This configured language model will serve as the foundation for subsequent text classification and structured output tasks.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/classification.ipynb#_snippet_0 LANGUAGE: javascript
CODE:
```
import { ChatOpenAI } from '@langchain/openai'; const llm = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0,
})
``` ---------------------------------------- TITLE: Transforming Chroma Vector Store into a Retriever (JavaScript)
DESCRIPTION: This snippet shows how to convert a Chroma vector store into a LangChain retriever. It configures the retriever with an optional metadata filter and specifies `k` to retrieve the top 2 results. The `invoke` method is then used to perform a retrieval operation for the query 'biology'.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/chroma.ipynb#_snippet_8 LANGUAGE: javascript
CODE:
```
const retriever = vectorStore.asRetriever({ // Optional filter filter: filter, k: 2,
});
await retriever.invoke("biology");
``` ---------------------------------------- TITLE: Streaming LangGraph QA Application Execution (JavaScript)
DESCRIPTION: This code executes the `graphQA` with a specific input question and streams the updates. It logs the initial input and then each chunk of the streamed output, allowing observation of the intermediate steps and results of the query analysis and retrieval process.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#_snippet_26 LANGUAGE: JavaScript
CODE:
```
let inputsQA = { question: "What does the end of the post say about Task Decomposition?" }; console.log(inputsQA)
console.log("\n====\n");
for await ( const chunk of await graphQA.stream(inputsQA, { streamMode: "updates", })
) { console.log(chunk); console.log("\n====\n");
}
``` ---------------------------------------- TITLE: Wrapping Information Retrieval as LangChain Tool (JavaScript)
DESCRIPTION: This snippet demonstrates how to wrap the `getInformation` function as a LangChain tool using `@langchain/core/tools`. The `informationTool` is defined with a name, a descriptive purpose for the LLM, and a Zod schema for its input parameters, specifically requiring an `entity` string. This allows the LLM agent to understand when and how to use this tool.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_semantic.ipynb#_snippet_4 LANGUAGE: JavaScript
CODE:
```
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const informationTool = tool((input) => { return getInformation(input.entity);
}, { name: "Information", description: "useful for when you need to answer questions about various actors or movies", schema: z.object({ entity: z.string().describe("movie or a person mentioned in the question"), }),
});
``` ---------------------------------------- TITLE: Including Input in Output with RunnablePassthrough (TypeScript)
DESCRIPTION: Shows how to include the original input dictionary directly into the output dictionary when composing runnables, typically within a `RunnableParallel`. `RunnablePassthrough` acts as an identity function, passing its input through unchanged, useful for preserving context.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#_snippet_8 LANGUAGE: TypeScript
CODE:
```
import { RunnableLambda, RunnableParallel, RunnablePassthrough
} from "@langchain/core/runnables"; const runnable = RunnableLambda.from((x: { foo: number }) => { return x.foo + 7;
}); const chain = RunnableParallel.from({ bar: runnable, baz: new RunnablePassthrough(),
}); await chain.invoke({ foo: 10 });
``` ---------------------------------------- TITLE: Invoking LangChain Generation Chain and Tracing in TypeScript
DESCRIPTION: This snippet demonstrates how to invoke the previously defined `generateChain` with a user query. It initializes a `TraceGroup` for LangSmith tracing to monitor the chain's execution. The `invoke` method is called with the user's input, and the raw result from the LLM is then logged to the console for inspection.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/basic_critique_revise.ipynb#_snippet_2 LANGUAGE: TypeScript
CODE:
```
import { TraceGroup } from "npm:langchain@0.0.173/callbacks"; const traceGroup = new TraceGroup("CritiqueReviseChain");
const groupManager = await traceGroup.start(); const userQuery = `Set a reminder to renew our online property ads next week.`; let result = await generateChain.invoke({ query: userQuery
}, { callbacks: groupManager }); console.log(result);
``` ---------------------------------------- TITLE: Generating Citations with Custom Document Splits using LangChain.js Text Splitters
DESCRIPTION: This snippet demonstrates how to use LangChain's `MarkdownTextSplitter` to pre-process and chunk a document (e.g., a README file) before passing it to Anthropic for citation generation. By formatting the splits into a `custom content document` type, users gain fine-grained control over how context is provided, enabling more precise citations based on meaningful text segments.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#_snippet_20 LANGUAGE: JavaScript
CODE:
```
import { ChatAnthropic } from "@langchain/anthropic";
import { MarkdownTextSplitter } from "langchain/text_splitter"; function formatToAnthropicDocuments(documents: string[]) { return { type: "document", source: { type: "content", content: documents.map((document) => ({ type: "text", text: document })), }, citations: { enabled: true }, };
} // Pull readme
const readmeResponse = await fetch( "https://raw.githubusercontent.com/langchain-ai/langchainjs/master/README.md"
); const readme = await readmeResponse.text(); // Split into chunks
const splitter = new MarkdownTextSplitter({ chunkOverlap: 0, chunkSize: 50,
});
const documents = await splitter.splitText(readme); // Construct message
const messageWithSplitDocuments = { role: "user", content: [ formatToAnthropicDocuments(documents), { type: "text", text: "Give me a link to LangChain's tutorials. Cite your sources" }, ],
}; // Query LLM
const citationsModelWithSplits = new ChatAnthropic({ model: "claude-3-5-sonnet-latest",
});
const resWithSplits = await citationsModelWithSplits.invoke([messageWithSplitDocuments]); console.log(JSON.stringify(resWithSplits.content, null, 2));
``` ---------------------------------------- TITLE: Configuring LLM for Document Citation (ID-based) - JavaScript
DESCRIPTION: This snippet configures a LangChain LLM to produce structured output, specifically an answer along with integer IDs of the cited sources. It uses Zod to define the output schema, ensuring the model's response adheres to a predefined format for document citation. The `withStructuredOutput` method is used to enforce this schema, making the model specify which provided documents justify its answer.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#_snippet_5 LANGUAGE: javascript
CODE:
```
import { z } from "zod"; const llmWithTool1 = llm.withStructuredOutput( z.object({ answer: z.string().describe("The answer to the user question, which is based only on the given sources."), citations: z.array(z.number()).describe("The integer IDs of the SPECIFIC sources which justify the answer.") }).describe("A cited source from the given text"), { name: "cited_answers" }
); const exampleQ = `What is Brian's height? Source: 1
Information: Suzy is 6'2" Source: 2
Information: Jeremiah is blonde Source: 3
Information: Brian is 3 inches shorter than Suzy`; await llmWithTool1.invoke(exampleQ);
``` ---------------------------------------- TITLE: Invoking Conversational Retrieval Chain with Initial Query (TypeScript)
DESCRIPTION: This snippet demonstrates invoking the conversationalRetrievalChain with an initial user query. Since it's the first message, the chain will bypass the query transformation step and directly use the input for retrieval, then generate an answer using the documentChain.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#_snippet_14 LANGUAGE: TypeScript
CODE:
```
await conversationalRetrievalChain.invoke({ messages: [new HumanMessage("Can LangSmith help test my LLM applications?")],
});
``` ---------------------------------------- TITLE: Initializing OpenAI Chat Model (TypeScript)
DESCRIPTION: This code initializes an instance of `ChatOpenAI`, configuring it to use the 'gpt-4o' model with a temperature of 0 for deterministic responses. This language model will serve as the core intelligence for the agent, processing user inputs and generating responses.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_tools.ipynb#_snippet_1 LANGUAGE: typescript
CODE:
```
import { ChatOpenAI } from "@langchain/openai"; const llm = new ChatOpenAI({ model: "gpt-4o", temperature: 0,
});
``` ---------------------------------------- TITLE: Defining a RAG Chain with AzionRetriever in TypeScript
DESCRIPTION: This snippet defines a Retrieval Augmented Generation (RAG) chain using LangChain's RunnableSequence. It integrates a retriever (implicitly AzionRetriever) to fetch context, a chat prompt, an LLM, and an output parser to answer questions based on the provided context. It requires `@langchain/core/prompts`, `@langchain/core/runnables`, and `@langchain/core/output_parsers`.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/azion-edgesql.mdx#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence,
} from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers"; import type { Document } from "@langchain/core/documents"; const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided. Context: {context} Question: {question}`); const formatDocs = (docs: Document[]) => { return docs.map((doc) => doc.pageContent).join("\n\n");
}; // See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([ { context: retriever.pipe(formatDocs), question: new RunnablePassthrough(), }, prompt, llm, new StringOutputParser(),
]);
``` ---------------------------------------- TITLE: Implementing Summary Memory with LangGraph in JavaScript
DESCRIPTION: This comprehensive snippet defines a LangGraph workflow that incorporates summary memory. The `callModel3` function conditionally summarizes chat history using an LLM when the message count exceeds a threshold, then invokes the LLM with the summary and current input. It also sets up the `StateGraph` and compiles it with an in-memory checkpointer for state management.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_memory.ipynb#_snippet_9 LANGUAGE: JavaScript
CODE:
```
import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from "@langchain/langgraph";
import { RemoveMessage } from "@langchain/core/messages"; // Define the function that calls the model
const callModel3 = async (state: typeof MessagesAnnotation.State) => { const systemPrompt = "You are a helpful assistant. " + "Answer all questions to the best of your ability. " + "The provided chat history includes a summary of the earlier conversation."; const systemMessage = { role: "system", content: systemPrompt }; const messageHistory = state.messages.slice(0, -1); // exclude the most recent user input // Summarize the messages if the chat history reaches a certain size if (messageHistory.length >= 4) { const lastHumanMessage = state.messages[state.messages.length - 1]; // Invoke the model to generate conversation summary const summaryPrompt = "Distill the above chat messages into a single summary message. " + "Include as many specific details as you can."; const summaryMessage = await llm.invoke([ ...messageHistory, { role: "user", content: summaryPrompt } ]); // Delete messages that we no longer want to show up const deleteMessages = state.messages.map(m => new RemoveMessage({ id: m.id })); // Re-add user message const humanMessage = { role: "user", content: lastHumanMessage.content }; // Call the model with summary & response const response = await llm.invoke([systemMessage, summaryMessage, humanMessage]); return { messages: [summaryMessage, humanMessage, response, ...deleteMessages] }; } else { const response = await llm.invoke([systemMessage, ...state.messages]); return { messages: response }; }
}; const workflow3 = new StateGraph(MessagesAnnotation) // Define the node and edge .addNode("model", callModel3) .addEdge(START, "model") .addEdge("model", END); // Add simple in-memory checkpointer
const app3 = workflow3.compile({ checkpointer: new MemorySaver() });
``` ---------------------------------------- TITLE: Initializing In-Memory Vector Store with LangChain.js
DESCRIPTION: This snippet initializes a `MemoryVectorStore` using the previously defined `embeddings` instance. This in-memory vector store will be used to store and retrieve document chunks based on their semantic similarity to a given query, forming the retrieval component of the RAG system.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#_snippet_4 LANGUAGE: JavaScript
CODE:
```
// @lc-docs-hide-cell
import { MemoryVectorStore } from "langchain/vectorstores/memory"; const vectorStore = new MemoryVectorStore(embeddings);
``` ---------------------------------------- TITLE: Initializing MemoryVectorStore in JavaScript
DESCRIPTION: This snippet initializes a MemoryVectorStore instance, a vector store from LangChain, which stores and retrieves document embeddings in memory. It takes the previously initialized 'embeddings' model as a dependency, enabling the vector store to process and manage vector representations of text data for efficient retrieval.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#_snippet_2 LANGUAGE: JavaScript
CODE:
```
import { MemoryVectorStore } from "langchain/vectorstores/memory"; const vectorStore = new MemoryVectorStore(embeddings);
``` ---------------------------------------- TITLE: Instantiating a History-Aware Retriever - LangChain.js
DESCRIPTION: This code instantiates a `historyAwareRetriever` using `createHistoryAwareRetriever` from `langchain/chains/history_aware_retriever`. It requires an `llm`, a base `retriever`, and the `rephrasePrompt` (defined previously) to rephrase the input query based on chat history. This ensures that document retrieval is contextually relevant to the ongoing conversation.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
import { createHistoryAwareRetriever } from "langchain/chains/history_aware_retriever"; const historyAwareRetriever = await createHistoryAwareRetriever({ llm, retriever, rephrasePrompt: contextualizeQPrompt
});
``` ---------------------------------------- TITLE: Creating a Prompt Template with Object Inputs (TypeScript)
DESCRIPTION: This snippet defines a `ChatPromptTemplate` that accepts multiple inputs: a `language` string for system instructions and a `messages` placeholder for chat history. It then pipes this prompt to an `llm` (assumed to be previously defined) to create a runnable, demonstrating how to structure prompts for object-based inputs.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/message_history.ipynb#_snippet_6 LANGUAGE: typescript
CODE:
```
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts"; const prompt = ChatPromptTemplate.fromMessages([ ["system", "Answer in {language}."], new MessagesPlaceholder("messages"),
]) const runnable = prompt.pipe(llm);
``` ---------------------------------------- TITLE: Streaming LangGraph Steps (JavaScript)
DESCRIPTION: This snippet demonstrates how LangGraph natively handles streaming intermediate steps using its `stream` method. By setting `streamMode: "updates"`, it provides a way to observe the agent's progress and state changes as they occur.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/migrate_agent.ipynb#_snippet_11 LANGUAGE: JavaScript
CODE:
```
const langGraphStream = await app.stream( { messages: [{ role: "user", content: query }] }, { streamMode: "updates" },
); for await (const step of langGraphStream) { console.log(step);
}
``` ---------------------------------------- TITLE: Constructing a LangGraph Agent Executor in LangChain.js
DESCRIPTION: This code initializes an agent executor using `createReactAgent` from LangGraph's prebuilt modules. It requires an LLM instance and a list of tools, setting up the core agent logic for processing user queries.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#_snippet_14 LANGUAGE: JavaScript
CODE:
```
import { createReactAgent } from "@langchain/langgraph/prebuilt"; const agentExecutor = createReactAgent({ llm, tools })
``` ---------------------------------------- TITLE: Initializing Chat Model with OpenAI
DESCRIPTION: This snippet initializes a `ChatOpenAI` instance, configuring it to use the 'gpt-4o' model with a temperature of 0. This model instance (`llm`) will be used for subsequent language model operations, ensuring deterministic responses.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai"; const llm = new ChatOpenAI({ model: "gpt-4o", temperature: 0,
});
``` ---------------------------------------- TITLE: Initializing ChatOpenAI Model for Tool Chaining in TypeScript
DESCRIPTION: This code initializes a ChatOpenAI instance, which serves as the language model for a tool-calling chain. It specifies the gpt-4o model and sets the temperature to 0 for deterministic outputs, preparing the LLM to interact with tools.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search_community.ipynb#_snippet_5 LANGUAGE: typescript
CODE:
```
import { ChatOpenAI } from "@langchain/openai" const llm = new ChatOpenAI({ model: "gpt-4o", temperature: 0,
})
``` ---------------------------------------- TITLE: Implementing Message Preprocessing with trimMessages in LangChain.js
DESCRIPTION: This snippet demonstrates how to use `trimMessages` from `@langchain/core/messages` to preprocess conversation history before it's passed to a `ChatOpenAI` model. It configures `trimMessages` to limit the conversation to the last 5 messages, keeping system messages, and then pipes this preprocessor with a tool-bound model. It also shows how to define a simple tool and invoke the preprocessed model with a full conversation history.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/conversation_buffer_window_memory.ipynb#_snippet_8 LANGUAGE: JavaScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { AIMessage, HumanMessage, SystemMessage, BaseMessage, trimMessages } from "@langchain/core/messages";
import { tool } from "@langchain/core/tools";
import { z } from "zod"; const model3 = new ChatOpenAI({ model: "gpt-4o" }); const whatDidTheCowSay = tool( (): string => { return "foo"; }, { name: "what_did_the_cow_say", description: "Check to see what the cow said.", schema: z.object({}), }
); const messageProcessor = trimMessages( { tokenCounter: (msgs) => msgs.length, // <-- .length will simply count the number of messages rather than tokens maxTokens: 5, // <-- allow up to 5 messages. strategy: "last", // The startOn is specified // to make sure we do not generate a sequence where // a ToolMessage that contains the result of a tool invocation // appears before the AIMessage that requested a tool invocation // as this will cause some chat models to raise an error. startOn: "human", includeSystem: true, // <-- Keep the system message allowPartial: false, }
); // Note that we bind tools to the model first!
const modelWithTools = model3.bindTools([whatDidTheCowSay]); const modelWithPreprocessor = messageProcessor.pipe(modelWithTools); const fullHistory = [ new SystemMessage("you're a good assistant, you always respond with a joke."), new HumanMessage("i wonder why it's called langchain"), new AIMessage('Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!'), new HumanMessage("and who is harrison chasing anyways"), new AIMessage("Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!"), new HumanMessage("why is 42 always the answer?"), new AIMessage("Because it's the only number that's constantly right, even when it doesn't add up!"), new HumanMessage("What did the cow say?"),
]; // We pass it explicitly to the modelWithPreprocessor for illustrative purposes.
// If you're using `RunnableWithMessageHistory` the history will be automatically
// read from the source that you configure.
const result = await modelWithPreprocessor.invoke(fullHistory);
console.log(result);
``` ---------------------------------------- TITLE: Transforming Vector Store into a Retriever (JavaScript)
DESCRIPTION: This snippet demonstrates how to convert an existing vector store instance into a retriever using the `asRetriever` method. It allows for optional filtering and specifies the number of top results (`k`) to retrieve. The `invoke` method is then used to query the retriever with a specific term.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/vectorstores.ipynb#_snippet_8 LANGUAGE: JavaScript
CODE:
```
const retriever = vectorStore.asRetriever({ // Optional filter filter: filter, k: 2,
});
await retriever.invoke("biology");
``` ---------------------------------------- TITLE: Instantiating ChatOpenAI LLM
DESCRIPTION: This code initializes a ChatOpenAI language model instance with a specified model ('gpt-4o-mini') and temperature, which will be used as the underlying LLM for the OpenApiToolkit.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/openapi.ipynb#_snippet_1 LANGUAGE: typescript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
const llm = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0,
})
``` ---------------------------------------- TITLE: Creating a Retriever Tool for LangChain.js Agent
DESCRIPTION: This snippet demonstrates how to convert a LangChain retriever into a tool accessible by an agent. It uses `createRetrieverTool` to define the tool's name and description, making it available for the agent to use for searching specific content.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#_snippet_13 LANGUAGE: JavaScript
CODE:
```
import { createRetrieverTool } from "langchain/tools/retriever"; const tool = createRetrieverTool( retriever, { name: "blog_post_retriever", description: "Searches and returns excerpts from the Autonomous Agents blog post.", }
)
const tools = [tool]
``` ---------------------------------------- TITLE: Importing Dependencies for Baseline RAG in LangChain.js
DESCRIPTION: This snippet imports necessary modules from LangChain.js for building a baseline Retrieval-Augmented Generation (RAG) chain. It includes components for prompt templating, chat models (OpenAI), output parsing, runnable sequences, and a Tavily search API retriever, along with document types.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rewrite.ipynb#_snippet_0 LANGUAGE: JavaScript
CODE:
```
// Deno.env.set("OPENAI_API_KEY", "");
// Deno.env.set("TAVILY_API_KEY", ""); import { PromptTemplate } from "npm:langchain@0.0.172/prompts";
import { ChatOpenAI } from "npm:langchain@0.0.172/chat_models/openai";
import { StringOutputParser } from "npm:langchain@0.0.172/schema/output_parser";
import { RunnableSequence, RunnablePassthrough } from "npm:langchain@0.0.172/schema/runnable";
import { TavilySearchAPIRetriever } from "npm:langchain@0.0.172/retrievers/tavily_search_api";
import type { Document } from "npm:langchain@0.0.172/schema/document";
``` ---------------------------------------- TITLE: Defining Schema and Initializing StructuredOutputParser in JavaScript
DESCRIPTION: This snippet demonstrates how to define a Zod schema for expected output, initialize a `StructuredOutputParser` from it, and construct a `RunnableSequence` chain. It also shows how to retrieve the format instructions for prompting the LLM.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_structured.ipynb#_snippet_0 LANGUAGE: javascript
CODE:
```
import { z } from "zod";
import { RunnableSequence } from "@langchain/core/runnables";
import { StructuredOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts"; const zodSchema = z.object({ answer: z.string().describe("answer to the user's question"), source: z.string().describe("source used to answer the user's question, should be a website."),
}) const parser = StructuredOutputParser.fromZodSchema(zodSchema); const chain = RunnableSequence.from([ ChatPromptTemplate.fromTemplate( "Answer the users question as best as possible.\n{format_instructions}\n{question}" ), model, parser,
]); console.log(parser.getFormatInstructions());
``` ---------------------------------------- TITLE: Extending LangGraph State for Custom Prompt Parameters (LangChain.js)
DESCRIPTION: This snippet demonstrates how to extend the state of a `StateGraph` in LangChain.js to accommodate new input parameters like `language`. It defines `GraphAnnotation` to include both messages and the new `language` field. The `callModel3` function now invokes `promptTemplate2` with this extended state, allowing the LLM to use the `language` parameter for its responses. The workflow is then compiled with this new state definition.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#_snippet_16 LANGUAGE: JavaScript
CODE:
```
import { START, END, StateGraph, MemorySaver, MessagesAnnotation, Annotation } from "@langchain/langgraph"; // Define the State
const GraphAnnotation = Annotation.Root({ ...MessagesAnnotation.spec, language: Annotation<string>(),
}); // Define the function that calls the model
const callModel3 = async (state: typeof GraphAnnotation.State) => { const prompt = await promptTemplate2.invoke(state); const response = await llm.invoke(prompt); return { messages: [response] };
}; const workflow3 = new StateGraph(GraphAnnotation) .addNode("model", callModel3) .addEdge(START, "model") .addEdge("model", END); const app3 = workflow3.compile({ checkpointer: new MemorySaver() });
``` ---------------------------------------- TITLE: Adding Memory to LangChain.js Agent with LangGraph Checkpointer
DESCRIPTION: This code integrates built-in persistence into the LangChain agent using LangGraph's `MemorySaver`. It creates a `MemorySaver` instance and passes it as a `checkpointSaver` to the `createReactAgent` function, enabling stateful conversations.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#_snippet_16 LANGUAGE: JavaScript
CODE:
```
import { MemorySaver } from "@langchain/langgraph"; const memory3 = new MemorySaver(); const agentExecutor2 = createReactAgent({ llm, tools, checkpointSaver: memory3 })
``` ---------------------------------------- TITLE: Implementing RAG Logic Directly (JavaScript)
DESCRIPTION: This snippet shows how to implement RAG application logic by directly invoking individual components without LangGraph. It performs a similarity search, formats retrieved documents, invokes a prompt template with the question and context, and finally uses an LLM to generate an answer.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#_snippet_16 LANGUAGE: javascript
CODE:
```
let question = "..." const retrievedDocs = await vectorStore.similaritySearch(question)
const docsContent = retrievedDocs.map(doc => doc.pageContent).join("\n");
const messages = await promptTemplate.invoke({ question: question, context: docsContent });
const answer = await llm.invoke(messages);
``` ---------------------------------------- TITLE: Integrating HNSWLib Self-Query Retriever into a LangChain.js RAG Chain
DESCRIPTION: This snippet demonstrates how to build a Retrieval Augmented Generation (RAG) chain using LangChain.js components. It defines a prompt, a document formatter, and a RunnableSequence that pipes the self-query retriever's output through the formatter, combines it with a question, and passes it to an LLM for generating an answer. It requires `@langchain/core` and a pre-initialized `selfQueryRetriever` and `llm`.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/hnswlib.ipynb#_snippet_5 LANGUAGE: TypeScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers"; import type { Document } from "@langchain/core/documents"; const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided. Context: {context} Question: {question}`); const formatDocs = (docs: Document[]) => { return docs.map((doc) => JSON.stringify(doc)).join("\n\n");
} // See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([ { context: selfQueryRetriever.pipe(formatDocs), question: new RunnablePassthrough(), }, prompt, llm, new StringOutputParser(),
]);
``` ---------------------------------------- TITLE: Integrating Pinecone Self-Query Retriever into a LangChain RAG Chain (TypeScript)
DESCRIPTION: This snippet demonstrates how to construct a Retrieval Augmented Generation (RAG) chain in LangChain.js using a Pinecone self-query retriever. It defines a prompt template, a document formatter, and then sequences these components with the retriever, an LLM, and an output parser to answer questions based on retrieved context. The `formatDocs` utility ensures document metadata is included.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/pinecone.ipynb#_snippet_7 LANGUAGE: typescript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers"; import type { Document } from "@langchain/core/documents"; const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided. Context: {context} Question: {question}`); const formatDocs = (docs: Document[]) => { return docs.map((doc) => JSON.stringify(doc)).join("\n\n");
} // See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([ { context: selfQueryRetriever.pipe(formatDocs), question: new RunnablePassthrough(), }, prompt, llm, new StringOutputParser(),
]);
``` ---------------------------------------- TITLE: Initializing ChatOpenAI Language Model in LangChain.js
DESCRIPTION: This snippet initializes a `ChatOpenAI` instance, configuring it to use the `gpt-4o-mini` model with a temperature of 0 for deterministic responses. This model instance will be used for subsequent language model interactions and tool binding. It requires the `@langchain/openai` package.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#_snippet_6 LANGUAGE: javascript
CODE:
```
// @lc-docs-hide-cell import { ChatOpenAI } from "@langchain/openai"; const model = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0 })
``` ---------------------------------------- TITLE: Adding Documents to LangChain.js Vector Store (TypeScript)
DESCRIPTION: This snippet demonstrates how to add an array of Document objects to a LangChain.js vector store. It shows the creation of sample documents with pageContent and metadata, and then uses await vectorStore.addDocuments(documents, { ids: ids }) to store them, optionally providing unique IDs generated by uuidv4.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/google_cloudsql_pg.ipynb#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { v4 as uuidv4 } from "uuid";
import type { Document } from "@langchain/core/documents"; const document1: Document = { pageContent: "The powerhouse of the cell is the mitochondria", metadata: { page: 0, source: "https://example.com" },
}; const document2: Document = { pageContent: "Buildings are made out of brick", metadata: { page: 1, source: "https://example.com" },
}; const document3: Document = { pageContent: "Mitochondria are made out of lipids", metadata: { page: 2, source: "https://example.com" },
}; const document4: Document = { pageContent: "The 2024 Olympics are in Paris", metadata: { page: 3, source: "https://example.com" },
}; const documents = [document1, document2, document3, document4]; const ids = [uuidv4(), uuidv4(), uuidv4(), uuidv4()]; await vectorStore.addDocuments(documents, { ids: ids }); ``` ---------------------------------------- TITLE: Indexing Documents in Vector Store (LangChain.js)
DESCRIPTION: This snippet demonstrates how to add a collection of documents (`allSplits`) to the initialized vector store. The `addDocuments` method indexes the provided documents, making them searchable.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#_snippet_9 LANGUAGE: JavaScript
CODE:
```
await vectorStore.addDocuments(allSplits)
``` ---------------------------------------- TITLE: Performing Direct Similarity Search with LangChain.js
DESCRIPTION: This snippet demonstrates how to perform a basic similarity search on a vector store. It queries for 'biology' and retrieves the top 2 most similar documents, optionally applying a filter. The results, including page content and metadata, are then logged to the console.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pinecone.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
// Optional filter
const filter = { source: "https://example.com" }; const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, filter); for (const doc of similaritySearchResults) { console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
``` ---------------------------------------- TITLE: Validating LLM Output with Zod Schema in TypeScript
DESCRIPTION: This code defines a utility function `outputValidator` that uses the `safeParse` method of the previously defined Zod schema to validate the LLM's output. It then applies this validator to the `result` obtained from the LLM and logs the validation outcome, which includes any errors if the output does not conform to the schema.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/basic_critique_revise.ipynb#_snippet_3 LANGUAGE: TypeScript
CODE:
```
const outputValidator = (output: unknown) => zodSchema.safeParse(output); let validatorResult = outputValidator(result); console.log(JSON.stringify(validatorResult, null, 2));
``` ---------------------------------------- TITLE: Converting Examples to LLM Messages
DESCRIPTION: Defines `toolExampleToMessages`, a helper function that converts a structured example (input and tool calls) into an array of LangChain `BaseMessage` objects, suitable for LLM function-calling models. It generates `HumanMessage`, `AIMessage` with tool calls, and `ToolMessage` instances, then processes the `examples` array to create `exampleMessages`.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_few_shot.ipynb#_snippet_10 LANGUAGE: javascript
CODE:
```
import { AIMessage, BaseMessage, HumanMessage, SystemMessage, ToolMessage, } from "@langchain/core/messages"; import { v4 as uuidV4 } from "uuid"; const toolExampleToMessages = (example: Record<string, any>): Array<BaseMessage> => { const messages: Array<BaseMessage> = [new HumanMessage({ content: example.input })]; const openaiToolCalls = example.toolCalls.map((toolCall) => { return { id: uuidV4(), type: "function" as const, function: { name: "search", arguments: JSON.stringify(toolCall), }, }; }); messages.push(new AIMessage({ content: "", additional_kwargs: { tool_calls: openaiToolCalls } })); const toolOutputs = "toolOutputs" in example ? example.toolOutputs : Array(openaiToolCalls.length).fill("You have correctly called this tool."); toolOutputs.forEach((output, index) => { messages.push(new ToolMessage({ content: output, tool_call_id: openaiToolCalls[index].id })); }); return messages; } const exampleMessages = examples.map((ex) => toolExampleToMessages(ex)).flat();
``` ---------------------------------------- TITLE: Chaining ChatOllama with Prompt Template (JavaScript)
DESCRIPTION: This example illustrates chaining a `ChatPromptTemplate` with the `ChatOllama` model. The prompt template dynamically constructs messages using placeholders for input and output languages, and the actual input. The `pipe` method creates a runnable chain, which is then invoked with specific values to generate a translated response.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama.ipynb#_snippet_5 LANGUAGE: javascript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts" const prompt = ChatPromptTemplate.fromMessages( [ [ "system", "You are a helpful assistant that translates {input_language} to {output_language}.", ], ["human", "{input}"], ]
) const chain = prompt.pipe(llm);
await chain.invoke( { input_language: "English", output_language: "German", input: "I love programming.", }
)
``` ---------------------------------------- TITLE: Creating a Tool with the `tool` Wrapper Function in LangChain (TypeScript)
DESCRIPTION: This snippet illustrates how to create a tool by wrapping a JavaScript function using the `tool` utility from `@langchain/core/tools`. It defines an `adder` tool that takes two numbers (`a`, `b`) as input, calculates their sum, and returns a string. This method is ideal when the tool needs to execute a specific function and is compatible with existing LangChain tool calling infrastructure.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_tools.ipynb#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { z } from "zod";
import { tool } from "@langchain/core/tools"; const adderSchema = z.object({ a: z.number(), b: z.number(),
});
const adderTool = tool(async (input): Promise<string> => { const sum = input.a + input.b; return `The sum of ${input.a} and ${input.b} is ${sum}`;
}, { name: "adder", description: "Adds two numbers together", schema: adderSchema,
}); await adderTool.invoke({ a: 1, b: 2 });
``` ---------------------------------------- TITLE: Initializing and Testing Message Trimmer in LangChain (JS/TS)
DESCRIPTION: This snippet demonstrates how to initialize the `trimMessages` helper from `@langchain/core/messages` and test its functionality. It configures the trimmer to keep a maximum of 10 tokens, prioritizing the last messages, and includes system messages. The example then defines a list of `messages` and invokes the trimmer to show its effect on the conversation history.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#_snippet_19 LANGUAGE: TypeScript
CODE:
```
import { SystemMessage, HumanMessage, AIMessage, trimMessages } from "@langchain/core/messages" const trimmer = trimMessages({ maxTokens: 10, strategy: "last", tokenCounter: (msgs) => msgs.length, includeSystem: true, allowPartial: false, startOn: "human",
}) const messages = [ new SystemMessage("you're a good assistant"), new HumanMessage("hi! I'm bob"), new AIMessage("hi!"), new HumanMessage("I like vanilla ice cream"), new AIMessage("nice"), new HumanMessage("whats 2 + 2"), new AIMessage("4"), new HumanMessage("thanks"), new AIMessage("no problem!"), new HumanMessage("having fun?"), new AIMessage("yes!"),
] await trimmer.invoke(messages)
``` ---------------------------------------- TITLE: Invoking Document Chain with Retrieved Context
DESCRIPTION: This snippet demonstrates how to invoke the `documentChain` with a human message and the previously retrieved `docs` as context. The chain processes the question using the provided context to generate an answer, showcasing how RAG works by augmenting the LLM's response with external data.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
import { HumanMessage, AIMessage } from "@langchain/core/messages"; await documentChain.invoke({ messages: [new HumanMessage("Can LangSmith help test my LLM applications?")], context: docs,
});
``` ---------------------------------------- TITLE: Creating a React Agent with SqlToolkit Tools in TypeScript
DESCRIPTION: This snippet demonstrates how to create a React-style agent executor using LangGraph's prebuilt function. It combines the previously defined LLM and the SqlToolkit's tools to enable conversational interaction with the SQL database.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/sql.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
import { createReactAgent } from "@langchain/langgraph/prebuilt" const agentExecutor = createReactAgent({ llm, tools });
``` ---------------------------------------- TITLE: Filtering Streaming Events by Component Tags (JS)
DESCRIPTION: This snippet illustrates filtering `streamEvents` using component `tags`. It applies the tag `"my_chain"` to the overall chain and then uses `includeTags: ["my_chain"]` in the `streamEvents` options to receive events only from components associated with that tag, including inherited tags.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#_snippet_18 LANGUAGE: JavaScript
CODE:
```
const chain = model .pipe(new JsonOutputParser().withConfig({ runName: "my_parser" })) .withConfig({ tags: ["my_chain"] }); const eventStream = await chain.streamEvents( `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`, { version: "v2" }, { includeTags: ["my_chain"] }
); let eventCount = 0; for await (const event of eventStream) { // Truncate the output if (eventCount > 10) { continue; } console.log(event); eventCount += 1;
}
``` ---------------------------------------- TITLE: Instantiating ElasticVectorSearch with OpenAI Embeddings (TypeScript)
DESCRIPTION: This comprehensive snippet illustrates the instantiation of ElasticVectorSearch for use as a vector store. It includes importing necessary modules, initializing OpenAIEmbeddings, configuring the Elasticsearch client with various authentication methods (API key, username/password), handling TLS certificates for local deployments, and finally creating the ElasticVectorSearch instance with the configured client and index name. This setup prepares the vector store for operations like adding documents and performing similarity searches.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/elasticsearch.ipynb#_snippet_2 LANGUAGE: typescript
CODE:
```
import { ElasticVectorSearch, type ElasticClientArgs,
} from "@langchain/community/vectorstores/elasticsearch";
import { OpenAIEmbeddings } from "@langchain/openai"; import { Client, type ClientOptions } from "@elastic/elasticsearch"; import * as fs from "node:fs"; const embeddings = new OpenAIEmbeddings({ model: "text-embedding-3-small",
}); const config: ClientOptions = { node: process.env.ELASTIC_URL ?? "https://127.0.0.1:9200",
}; if (process.env.ELASTIC_API_KEY) { config.auth = { apiKey: process.env.ELASTIC_API_KEY, };
} else if (process.env.ELASTIC_USERNAME && process.env.ELASTIC_PASSWORD) { config.auth = { username: process.env.ELASTIC_USERNAME, password: process.env.ELASTIC_PASSWORD, };
}
// Local Docker deploys require a TLS certificate
if (process.env.ELASTIC_CERT_PATH) { config.tls = { ca: fs.readFileSync(process.env.ELASTIC_CERT_PATH), rejectUnauthorized: false, }
}
const clientArgs: ElasticClientArgs = { client: new Client(config), indexName: process.env.ELASTIC_INDEX ?? "test_vectorstore",
}; const vectorStore = new ElasticVectorSearch(embeddings, clientArgs);
``` ---------------------------------------- TITLE: Initializing MemoryVectorStore with an Embedding Model in LangChain
DESCRIPTION: This snippet demonstrates how to initialize a MemoryVectorStore instance in LangChain. It requires an embedding model as an argument, which is used to convert text into vector representations for storage and similarity search. This setup is crucial for enabling semantic search capabilities within the vector store.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/vectorstores.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { MemoryVectorStore } from "langchain/vectorstores/memory";
// Initialize with an embedding model
const vectorStore = new MemoryVectorStore(new SomeEmbeddingModel());
``` ---------------------------------------- TITLE: Chaining Prompt Template with LLM in LangChain.js
DESCRIPTION: This snippet demonstrates how to create a PromptTemplate and chain it with an LLM (Language Model) using the pipe method in LangChain.js. It shows how to invoke the chain with specific input variables to generate a prompt for translation.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/azure.ipynb#_snippet_5 LANGUAGE: JavaScript
CODE:
```
import { PromptTemplate } from "@langchain/core/prompts" const prompt = new PromptTemplate({ template: "How to say {input} in {output_language}:\n", inputVariables: ["input", "output_language"],
}) const chain = prompt.pipe(llm);
await chain.invoke( { output_language: "German", input: "I love programming.", }
)
``` ---------------------------------------- TITLE: Streaming and Decoding LangChain Events (Server-Side) - JavaScript
DESCRIPTION: This snippet demonstrates how to initialize a LangChain.js chain, stream events from it using `streamEvents` with `text/event-stream` encoding, and then decode and log these events. It uses `TextDecoder` to convert binary event data into human-readable strings, showing how to process the streamed output on the server.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#_snippet_19 LANGUAGE: JavaScript
CODE:
```
const chain = model .pipe(new JsonOutputParser().withConfig({ runName: "my_parser" })) .withConfig({ tags: ["my_chain"] }); const eventStream = await chain.streamEvents( `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`, { version: "v2", encoding: "text/event-stream", },
); let eventCount = 0; const textDecoder = new TextDecoder(); for await (const event of eventStream) { // Truncate the output if (eventCount > 3) { continue; } console.log(textDecoder.decode(event)); eventCount += 1;
}
``` ---------------------------------------- TITLE: Streaming JSON Output with JsonOutputParser in LangChain.js
DESCRIPTION: This snippet demonstrates how to stream JSON output from a LangChain model using the `JsonOutputParser`. It pipes the model's output through the parser, allowing for chunk-by-chunk processing of potentially incomplete JSON. The `stream` method yields partial JSON objects as they are generated, which are then logged to the console.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
import { JsonOutputParser } from "@langchain/core/output_parsers" const chain = model.pipe(new JsonOutputParser());
const stream = await chain.stream( `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`
); for await (const chunk of stream) { console.log(chunk);
}
``` ---------------------------------------- TITLE: Installing LangChain with npm
DESCRIPTION: This command installs the core LangChain library using npm, saving it as a production dependency in your project. It's the primary way to get started with LangChain in a Node.js environment.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#_snippet_0 LANGUAGE: bash
CODE:
```
npm install -S langchain
``` ---------------------------------------- TITLE: Indexing and Retrieving Documents with MemoryVectorStore - JavaScript
DESCRIPTION: This snippet illustrates how to use MistralAIEmbeddings with MemoryVectorStore for indexing and retrieval. It creates a vector store from a sample document, configures it as a retriever, and then invokes the retriever to find the most similar text based on a query.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/mistralai.ipynb#_snippet_3 LANGUAGE: JavaScript
CODE:
```
// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory"; const text = "LangChain is the framework for building context-aware reasoning applications"; const vectorstore = await MemoryVectorStore.fromDocuments( [{ pageContent: text, metadata: {} }], embeddings,
); // Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1); // Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?"); retrievedDocuments[0].pageContent;
``` ---------------------------------------- TITLE: Defining LangGraph Application State Annotations
DESCRIPTION: Defines the structure for the application's state within a LangGraph workflow using `Annotation.Root`. It specifies `InputStateAnnotation` for initial user input and `StateAnnotation` to track the question, generated SQL query, query result, and final answer across different steps.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
import { Annotation } from "@langchain/langgraph"; const InputStateAnnotation = Annotation.Root({ question: Annotation<string>
}); const StateAnnotation = Annotation.Root({ question: Annotation<string>, query: Annotation<string>, result: Annotation<string>, answer: Annotation<string>
});
``` ---------------------------------------- TITLE: Transforming Qdrant Vector Store into a Retriever (JavaScript)
DESCRIPTION: This snippet demonstrates how to convert a `QdrantVectorStore` instance into a retriever, allowing for easier integration into LangChain chains. It shows how to apply an optional filter and limit the number of results (`k`). The `invoke` method is then used to query the retriever with a specific term.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/qdrant.ipynb#_snippet_7 LANGUAGE: javascript
CODE:
```
const retriever = vectorStore.asRetriever({ // Optional filter filter: filter, k: 2,
});
await retriever.invoke("biology");
``` ---------------------------------------- TITLE: Adding Chat Memory to LangChain AgentExecutor (JavaScript)
DESCRIPTION: This snippet illustrates how to integrate chat memory into a LangChain `AgentExecutor` using `RunnableWithMessageHistory` and `ChatMessageHistory`. It enables multi-turn conversations by persisting and retrieving chat history, allowing the agent to remember previous interactions within a session.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/migrate_agent.ipynb#_snippet_8 LANGUAGE: JavaScript
CODE:
```
import { ChatMessageHistory } from "@langchain/community/stores/message/in_memory";
import { RunnableWithMessageHistory } from "@langchain/core/runnables"; const memory = new ChatMessageHistory();
const agentExecutorWithMemory = new RunnableWithMessageHistory({ runnable: agentExecutor, getMessageHistory: () => memory, inputMessagesKey: "input", historyMessagesKey: "chat_history",
}); const config = { configurable: { sessionId: "test-session" } }; agentOutput = await agentExecutorWithMemory.invoke( { input: "Hi, I'm polly! What's the output of magic_function of 3?" }, config,
); console.log(agentOutput.output); agentOutput = await agentExecutorWithMemory.invoke( { input: "Remember my name?" }, config,
); console.log("---");
console.log(agentOutput.output);
console.log("---"); agentOutput = await agentExecutorWithMemory.invoke( { input: "what was that output again?" }, config,
); console.log(agentOutput.output);
``` ---------------------------------------- TITLE: Handling Runtime Requests and Setting Context Variables in LangChain
DESCRIPTION: This snippet defines a `RunnableLambda` that encapsulates the logic for handling runtime requests. It demonstrates how to set a `userId` context variable using `setContextVariable` before binding tools to the LLM and invoking it. This ensures that tools called within this runnable can access the `userId` at runtime.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_runtime.ipynb#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { setContextVariable } from "@langchain/core/context";
import { BaseChatModel } from "@langchain/core/language_models/chat_models";
import { RunnableLambda } from "@langchain/core/runnables"; const handleRunTimeRequestRunnable = RunnableLambda.from(async (params: { userId: string; query: string; llm: BaseChatModel;
}) => { const { userId, query, llm } = params; if (!llm.bindTools) { throw new Error("Language model does not support tools."); } // Set a context variable accessible to any child runnables called within this one. // You can also set context variables at top level that act as globals. setContextVariable("userId", userId); const tools = [updateFavoritePets]; const llmWithTools = llm.bindTools(tools); const modelResponse = await llmWithTools.invoke(query); // For simplicity, skip checking the tool call's name field and assume // that the model is calling the "updateFavoritePets" tool if (modelResponse.tool_calls.length > 0) { return updateFavoritePets.invoke(modelResponse.tool_calls[0]); } else { return "No tool invoked."; }
});
``` ---------------------------------------- TITLE: Splitting Raw Text into LangChain Documents using RecursiveCharacterTextSplitter (JavaScript)
DESCRIPTION: This snippet demonstrates how to initialize `RecursiveCharacterTextSplitter` and use its `createDocuments` method to split a raw text string into an array of LangChain `Document` objects. It configures the splitter with a `chunkSize` of 10 and `chunkOverlap` of 1, showing the initial documents generated.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/recursive_text_splitter.ipynb#_snippet_0 LANGUAGE: javascript
CODE:
```
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters"; const text = `Hi.\n\nI'm Harrison.\n\nHow? Are? You?\nOkay then f f f f.\nThis is a weird text to write, but gotta test the splittingggg some how.\n\nBye!\n\n-H.`;
const splitter = new RecursiveCharacterTextSplitter({ chunkSize: 10, chunkOverlap: 1,
}); const output = await splitter.createDocuments([text]); console.log(output.slice(0, 3));
``` ---------------------------------------- TITLE: Invoking the RAG Chain with a Query (TypeScript)
DESCRIPTION: This snippet shows how to execute the previously defined RAG chain with a specific question. It invokes the `ragChain` with a query about movies rated higher than 8.5, triggering the retrieval and generation process.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/memory.ipynb#_snippet_6 LANGUAGE: typescript
CODE:
```
await ragChain.invoke("Which movies are rated higher than 8.5?")
``` ---------------------------------------- TITLE: Invoking SelfQueryRetriever for Metadata-Based Search in JavaScript
DESCRIPTION: This snippet demonstrates how to use the `selfQueryRetriever` to perform a natural language query that requires metadata filtering. The retriever processes the input question, translates it into a structured query based on the defined `attributeInfo`, and then executes it against the Supabase vector store to retrieve relevant documents. The example query asks for movies rated higher than 8.5.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/supabase.ipynb#_snippet_6 LANGUAGE: javascript
CODE:
```
await selfQueryRetriever.invoke( "Which movies are rated higher than 8.5?"
);
``` ---------------------------------------- TITLE: Defining Custom Summarization Tool with Config Propagation in LangChain
DESCRIPTION: This snippet defines a corrected version of the custom summarization tool, `specialSummarizationToolWithConfig`, which now accepts a `config` parameter. This `config` object is explicitly passed to the internal chain's `invoke` method, enabling proper propagation of `RunnableConfig` and allowing internal events to be streamed.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_stream_events.ipynb#_snippet_4 LANGUAGE: javascript
CODE:
```
const specialSummarizationToolWithConfig = tool(async (input, config) => { const prompt = ChatPromptTemplate.fromTemplate( "You are an expert writer. Summarize the following text in 10 words or less:\n\n{long_text}" ); const reverse = (x: string) => { return x.split("").reverse().join(""); }; const chain = prompt .pipe(model) .pipe(new StringOutputParser()) .pipe(reverse); // Pass the "config" object as an argument to any executed runnables const summary = await chain.invoke({ long_text: input.long_text }, config); return summary;
}, { name: "special_summarization_tool",
``` ---------------------------------------- TITLE: Performing Basic Document Reranking with CohereRerank (TypeScript)
DESCRIPTION: This snippet demonstrates how to initialize `CohereRerank` and use its `.rerank()` method to reorder a list of documents based on their relevance to a given query. It returns the indices of the reranked documents and their corresponding relevancy scores.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/cohere_rerank.mdx#_snippet_1 LANGUAGE: typescript
CODE:
```
import { CohereRerank } from "@langchain/cohere";
import { Document } from "@langchain/core/documents"; const reranker = new CohereRerank({ apiKey: process.env.COHERE_API_KEY,
}); const query = "What is the capital of the United States?";
const documents = [ new Document({ pageContent: "The capital of France is Paris." }), new Document({ pageContent: "Washington, D.C. is the capital of the United States." }), new Document({ pageContent: "London is the capital of the United Kingdom." })
]; const results = await reranker.rerank({ query, documents });
console.log(results);
// Expected output: [{ index: 1, relevanceScore: 0.987 }, ...]
``` ---------------------------------------- TITLE: Building Query Rewriter Chain in LangChain.js
DESCRIPTION: This snippet constructs the `rewriter` chain using LangChain Expression Language. It sequences the rewrite prompt, an OpenAI chat model (with temperature 0 for consistent rewriting), a string output parser, and the custom `_parse` function to generate a clean, optimized search query.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rewrite.ipynb#_snippet_8 LANGUAGE: JavaScript
CODE:
```
// rewriter = rewrite_prompt | ChatOpenAI(temperature=0) | StrOutputParser() | _parse
const rewriter = RunnableSequence.from([ rewritePrompt, new ChatOpenAI({ temperature: 0 }), new StringOutputParser(), _parse
]);
``` ---------------------------------------- TITLE: Enabling Structured Output for LLM with a Schema (JavaScript)
DESCRIPTION: This snippet demonstrates how to enable structured output for an LLM by calling the `.withStructuredOutput()` method and passing a `personSchema`. This configures the LLM to return data conforming to the specified schema, facilitating information extraction.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/extraction.ipynb#_snippet_4 LANGUAGE: javascript
CODE:
```
const structured_llm = llm.withStructuredOutput(personSchema)
``` ---------------------------------------- TITLE: Generating Audio Output with ChatOpenAI in LangChain.js
DESCRIPTION: This example demonstrates how to configure `ChatOpenAI` to generate audio output using models like `gpt-4o-audio-preview`. It shows how to specify `modalities: ['text', 'audio']` and `audio` parameters (voice, format) during model initialization or binding, and how to extract the audio data from the `additional_kwargs.audio` field of the result.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#_snippet_27 LANGUAGE: JavaScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai"; const modelWithAudioOutput = new ChatOpenAI({ model: "gpt-4o-audio-preview", // You may also pass these fields to `.bind` as a call argument. modalities: ["text", "audio"], // Specifies that the model should output audio. audio: { voice: "alloy", format: "wav", },
}); const audioOutputResult = await modelWithAudioOutput.invoke("Tell me a joke about cats.");
const castAudioContent = audioOutputResult.additional_kwargs.audio as Record<string, any>; console.log({ ...castAudioContent, data: castAudioContent.data.slice(0, 100) // Sliced for brevity
})
``` ---------------------------------------- TITLE: Importing Dependencies for LangChain Agent (TypeScript)
DESCRIPTION: This snippet imports necessary modules for building a LangChain agent within a Next.js React Server Component (RSC). It includes `ChatOpenAI` for the LLM, `ChatPromptTemplate` for prompts, `TavilySearchResults` for tool integration, `AgentExecutor` and `createToolCallingAgent` for agent creation, `pull` for fetching prompts from LangChain Hub, and `createStreamableValue` for streaming data back to the client.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#_snippet_2 LANGUAGE: typescript
CODE:
```
"use server"; import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { TavilySearchResults } from "@langchain/community/tools/tavily_search";
import { AgentExecutor, createToolCallingAgent } from "langchain/agents";
import { pull } from "langchain/hub";
import { createStreamableValue } from "ai/rsc";
``` ---------------------------------------- TITLE: Invoking ChatMistralAI with System and Human Messages (JavaScript)
DESCRIPTION: This snippet shows how to invoke the instantiated `ChatMistralAI` model (`llm`) with a sequence of chat messages. It includes a 'system' message to define the assistant's role and a 'human' message as input. The `invoke` method sends these messages to the model to get an AI response.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/mistral.ipynb#_snippet_3 LANGUAGE: javascript
CODE:
```
const aiMsg = await llm.invoke([ [ "system", "You are a helpful assistant that translates English to French. Translate the user sentence.", ], ["human", "I love programming."],
])
aiMsg
``` ---------------------------------------- TITLE: Handling Agent Interaction in Client Component (TypeScript/React)
DESCRIPTION: This snippet defines the `Page` client component, managing input state and streamed data. The `handleSubmit` function prevents default form submission, calls the `runAgent` server action, and then uses `readStreamableValue` to asynchronously iterate over the streamed data, updating the component's state with each received event.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#_snippet_10 LANGUAGE: typescript
CODE:
```
export default function Page() { const [input, setInput] = useState(""); const [data, setData] = useState<StreamEvent[]>([]); async function handleSubmit(e: React.FormEvent) { e.preventDefault(); const { streamData } = await runAgent(input); for await (const item of readStreamableValue(streamData)) { setData((prev) => [...prev, item]); } }
}
``` ---------------------------------------- TITLE: Executing and Streaming Agent Responses in JavaScript
DESCRIPTION: This code executes the createReactAgent with a sample query, streams the responses, and then processes each event to log either tool calls made by the agent or the final content generated as a response.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/vectorstore.ipynb#_snippet_5 LANGUAGE: javascript
CODE:
```
const exampleQuery = "What did biden say about Ketanji Brown Jackson is the state of the union address?" const events = await agentExecutor.stream( { messages: [["user", exampleQuery]]}, { streamMode: "values", }
) for await (const event of events) { const lastMsg = event.messages[event.messages.length - 1]; if (lastMsg.tool_calls?.length) { console.dir(lastMsg.tool_calls, { depth: null }); } else if (lastMsg.content) { console.log(lastMsg.content); }
}
``` ---------------------------------------- TITLE: Splitting Documents with RecursiveCharacterTextSplitter in LangChain.js
DESCRIPTION: This snippet demonstrates how to split a large document into smaller, manageable chunks using `RecursiveCharacterTextSplitter`. It configures the splitter with a `chunkSize` of 1000 characters and a `chunkOverlap` of 200 characters to maintain context between chunks, then logs the total number of resulting sub-documents. This is crucial for fitting content into model context windows and improving retrieval accuracy.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#_snippet_9 LANGUAGE: JavaScript
CODE:
```
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters"; const splitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200
});
const allSplits = await splitter.splitDocuments(docs);
console.log(`Split blog post into ${allSplits.length} sub-documents.`);
``` ---------------------------------------- TITLE: Creating a Declarative LangChain Tool-Calling Chain - JavaScript
DESCRIPTION: This snippet demonstrates building a declarative LangChain expression language chain. It pipes the output of an LLM (with bound tools) to a function that extracts tool calls, and then maps these tool calls to the `generateRandomInts` tool. This allows for a streamlined workflow where the model's tool suggestions are automatically executed within the chain, returning the tool's output.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_artifacts.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
const extractToolCalls = (aiMessage) => aiMessage.tool_calls; const chain = llmWithTools.pipe(extractToolCalls).pipe(generateRandomInts.map()); await chain.invoke("give me a random number between 1 and 5");
``` ---------------------------------------- TITLE: Invoking a Runnable with Configuration in TypeScript
DESCRIPTION: This snippet demonstrates how to invoke a `Runnable` instance with a `RunnableConfig` object. The configuration includes `runName` for naming the run, `tags` for categorization, and `metadata` for additional key-value pairs, allowing for runtime customization and tracking of the runnable's execution.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/runnables.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
await someRunnable.invoke(someInput, { runName: "myRun", tags: ["tag1", "tag2"], metadata: { key: "value" }
});
``` ---------------------------------------- TITLE: Invoking LangChain Agent with Explicit Chat History (JavaScript)
DESCRIPTION: This snippet demonstrates how to invoke a LangChain agent by explicitly providing a list of chat history messages. The agent uses this `messages` array, containing previous user and assistant interactions, to generate a context-aware, conversational response.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_tools.ipynb#_snippet_7 LANGUAGE: javascript
CODE:
```
await agent.invoke({ messages: [ { role: "user", content: "I'm Nemo!" }, { role: "user", content: "Hello Nemo! How can I assist you today?" }, { role: "user", content: "What is my name?" } ]
})
``` ---------------------------------------- TITLE: Invoking Rewrite-Retrieve-Read Chain with Distracted Query in LangChain.js
DESCRIPTION: This snippet demonstrates the final step: invoking the complete `rewriteRetrieveReadChain` with the 'distracted' query. This showcases the improved performance of the RAG system, as the query is first rewritten, leading to better retrieval and ultimately a more accurate answer.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rewrite.ipynb#_snippet_11 LANGUAGE: JavaScript
CODE:
```
await rewriteRetrieveReadChain.invoke(distractedQuery);
``` ---------------------------------------- TITLE: Instantiating AzionRetriever with OpenAI Embeddings and Chat Model
DESCRIPTION: This comprehensive snippet demonstrates how to initialize the AzionRetriever, configuring it with an embedding model (OpenAIEmbeddings), a chat model (ChatOpenAI), and various database and search parameters such as table names, search type (hybrid), and metadata filters.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/azion-edgesql.mdx#_snippet_4 LANGUAGE: typescript
CODE:
```
import { AzionRetriever } from "@langchain/community/retrievers/azion_edgesql";
import { OpenAIEmbeddings } from "@langchain/openai";
import { ChatOpenAI } from "@langchain/openai"; const embeddingModel = new OpenAIEmbeddings({ model: "text-embedding-3-small",
}); const chatModel = new ChatOpenAI({ model: "gpt-4o-mini", apiKey: process.env.OPENAI_API_KEY,
}); const retriever = new AzionRetriever(embeddingModel, { dbName: "langchain", vectorTable: "documents", // table where the vector embeddings are stored ftsTable: "documents_fts", // table where the fts index is stored searchType: "hybrid", // search type to use for the retriever ftsK: 2, // number of results to return from the fts index similarityK: 2, // number of results to return from the vector index metadataItems: ["language", "topic"], filters: [{ operator: "=", column: "language", value: "en" }], entityExtractor: chatModel,
});
``` ---------------------------------------- TITLE: Full Example: Integrating Upstash Ratelimit Handler with LangChain.js Chain (TypeScript)
DESCRIPTION: This comprehensive example demonstrates the end-to-end integration of UpstashRatelimitHandler with a LangChain.js chain. It covers initializing Redis and Ratelimit, configuring the handler for token-based limiting, creating a simple RunnableSequence with an OpenAI LLM, and invoking the chain with the handler, including error handling for UpstashRatelimitError.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/callbacks/upstash_ratelimit_callback.mdx#_snippet_8 LANGUAGE: tsx
CODE:
```
const UPSTASH_REDIS_REST_URL = "****";
const UPSTASH_REDIS_REST_TOKEN = "****";
const OPENAI_API_KEY = "****"; import { UpstashRatelimitHandler, UpstashRatelimitError,
} from "@langchain/community/callbacks/handlers/upstash_ratelimit";
import { RunnableLambda, RunnableSequence } from "@langchain/core/runnables";
import { OpenAI } from "@langchain/openai";
import { Ratelimit } from "@upstash/ratelimit";
import { Redis } from "@upstash/redis"; // create ratelimit
const ratelimit = new Ratelimit({ redis: new Redis({ url: UPSTASH_REDIS_REST_URL, token: UPSTASH_REDIS_REST_TOKEN, }), // 500 tokens per window, where window size is 60 seconds: limiter: Ratelimit.fixedWindow(500, "60 s"),
}); // create handler
const user_id = "user_id"; // should be a method which gets the user id
const handler = new UpstashRatelimitHandler(user_id, { tokenRatelimit: ratelimit,
}); // create mock chain
const asStr = new RunnableLambda({ func: (str: string): string => str });
const model = new OpenAI({ apiKey: OPENAI_API_KEY,
});
const chain = RunnableSequence.from([asStr, model]); // invoke chain with handler:
try { const response = await chain.invoke("hello world", { callbacks: [handler], }); console.log(response);
} catch (err) { if (err instanceof UpstashRatelimitError) { console.log("Handling ratelimit."); }
}
``` ---------------------------------------- TITLE: Building a LangChain Tool-Using Agent with LangSmith (TypeScript)
DESCRIPTION: This snippet demonstrates how to construct a LangChain agent that uses tools and leverages LangSmith for example selection. It imports necessary components, defines functions for fetching similar examples and constructing prompts, initializes an OpenAI LLM, binds it with previously defined tools, and chains these components together to create a runnable agent.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_langsmith.ipynb#_snippet_8 LANGUAGE: TypeScript
CODE:
```
import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage, SystemMessage, BaseMessage, BaseMessageLike } from "@langchain/core/messages";
import { RunnableLambda } from "@langchain/core/runnables";
import { Client as LangSmithClient, Example } from "langsmith";
import { coerceMessageLikeToMessage } from "@langchain/core/messages"; const client = new LangSmithClient(); async function similarExamples(input: Record<string, any>): Promise<Record<string, any>> { const examples = await client.similarExamples(input, dataset.id, 5); return { ...input, examples };
} function constructPrompt(input: { examples: Example[], input: string }): BaseMessage[] { const instructions = "You are great at using mathematical tools."; let messages: BaseMessage[] = [] for (const ex of input.examples) { // Assuming ex.outputs.output is an array of message-like objects messages = messages.concat(ex.outputs.output.flatMap((msg: BaseMessageLike) => coerceMessageLikeToMessage(msg))); } const examples = messages.filter(msg => msg._getType() !== 'system'); examples.forEach((ex) => { if (ex._getType() === 'human') { ex.name = "example_user"; } else { ex.name = "example_assistant"; } }); return [new SystemMessage(instructions), ...examples, new HumanMessage(input.input)];
} const llm = new ChatOpenAI({ model: "gpt-4o", temperature: 0,
});
const tools = [add, cos, divide, log, multiply, negate, pi, power, sin, subtract];
const llmWithTools = llm.bindTools(tools); const exampleSelector = new RunnableLambda( { func: similarExamples }
).withConfig({ runName: "similarExamples" }); const chain = exampleSelector.pipe( new RunnableLambda({ func: constructPrompt }).withConfig({ runName: "constructPrompt" })
).pipe(llmWithTools);
``` ---------------------------------------- TITLE: Initializing Chroma Vector Store with Documents and Metadata (TypeScript)
DESCRIPTION: This snippet initializes a Chroma vector store by first defining a set of `Document` objects, each with `pageContent` and structured `metadata`. It then defines `AttributeInfo` to describe the queryable metadata fields (genre, year, director, rating, length). Finally, it instantiates `OpenAIEmbeddings` and uses them to populate the `Chroma` vector store from the defined documents, creating a 'movie-collection'.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/chroma.ipynb#_snippet_2 LANGUAGE: typescript
CODE:
```
import { OpenAIEmbeddings } from "@langchain/openai";
import { Chroma } from "@langchain/community/vectorstores/chroma";
import { Document } from "@langchain/core/documents";
import type { AttributeInfo } from "langchain/chains/query_constructor"; /** * First, we create a bunch of documents. You can load your own documents here instead. * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below. */
const docs = [ new Document({ pageContent: "A bunch of scientists bring back dinosaurs and mayhem breaks loose", metadata: { year: 1993, rating: 7.7, genre: "science fiction" }, }), new Document({ pageContent: "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...", metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 }, }), new Document({ pageContent: "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea", metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 }, }), new Document({ pageContent: "A bunch of normal-sized women are supremely wholesome and some men pine after them", metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 }, }), new Document({ pageContent: "Toys come alive and have a blast doing so", metadata: { year: 1995, genre: "animated" }, }), new Document({ pageContent: "Three men walk into the Zone, three men walk out of the Zone", metadata: { year: 1979, director: "Andrei Tarkovsky", genre: "science fiction", rating: 9.9, }, }),
]; /** * Next, we define the attributes we want to be able to query on. * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie. * We also provide a description of each attribute and the type of the attribute. * This is used to generate the query prompts. */
const attributeInfo: AttributeInfo[] = [ { name: "genre", description: "The genre of the movie", type: "string or array of strings", }, { name: "year", description: "The year the movie was released", type: "number", }, { name: "director", description: "The director of the movie", type: "string", }, { name: "rating", description: "The rating of the movie (1-10)", type: "number", }, { name: "length", description: "The length of the movie in minutes", type: "number", },
]; /** * Next, we instantiate a vector store. This is where we store the embeddings of the documents. * We also need to provide an embeddings object. This is used to embed the documents. */
const embeddings = new OpenAIEmbeddings();
const vectorStore = await Chroma.fromDocuments(docs, embeddings, { collectionName: "movie-collection",
});
``` ---------------------------------------- TITLE: Invoking LangGraph Application Synchronously (JavaScript)
DESCRIPTION: This example demonstrates how to synchronously invoke a compiled LangGraph application with specific inputs. It passes a question to the graph, awaits the result, and then logs parts of the retrieved context and the final generated answer.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#_snippet_17 LANGUAGE: javascript
CODE:
```
let inputs = { question: "What is Task Decomposition?" }; const result = await graph.invoke(inputs);
console.log(result.context.slice(0, 2));
console.log(`\nAnswer: ${result["answer"]}`);
``` ---------------------------------------- TITLE: Transforming RedisVectorStore into a Retriever (JavaScript)
DESCRIPTION: This snippet illustrates how to convert the vector store into a LangChain retriever. The `asRetriever` method allows specifying configuration options like `k` (number of documents to retrieve), making it suitable for use in LangChain chains.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/redis.ipynb#_snippet_5 LANGUAGE: JavaScript
CODE:
```
const retriever = vectorStore.asRetriever({ k: 2,
});
await retriever.invoke("biology");
``` ---------------------------------------- TITLE: Customizing MultiQueryRetriever with Custom Prompt and Output Parser (TypeScript)
DESCRIPTION: This snippet demonstrates how to customize the `MultiQueryRetriever` in LangChain.js. It defines a `LineListOutputParser` to parse LLM output into a list of lines, pulls a custom prompt template, initializes a vector store, and then sets up an `LLMChain` with the custom prompt and parser. Finally, it creates a `MultiQueryRetriever` and invokes it with a query, showcasing how to generate and process multiple queries for improved retrieval.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multiple_queries.ipynb#_snippet_1 LANGUAGE: TypeScript
CODE:
```
import { LLMChain } from "langchain/chains";
import { pull } from "langchain/hub";
import { BaseOutputParser } from "@langchain/core/output_parsers";
import { PromptTemplate } from "@langchain/core/prompts"; type LineList = { lines: string[];
}; class LineListOutputParser extends BaseOutputParser<LineList> { static lc_name() { return "LineListOutputParser"; } lc_namespace = ["langchain", "retrievers", "multiquery"]; async parse(text: string): Promise<LineList> { const startKeyIndex = text.indexOf("<questions>"); const endKeyIndex = text.indexOf("</questions>"); const questionsStartIndex = startKeyIndex === -1 ? 0 : startKeyIndex + "<questions>".length; const questionsEndIndex = endKeyIndex === -1 ? text.length : endKeyIndex; const lines = text .slice(questionsStartIndex, questionsEndIndex) .trim() .split("\n") .filter((line) => line.trim() !== ""); return { lines }; } getFormatInstructions(): string { throw new Error("Not implemented."); }
} // Default prompt is available at: https://smith.langchain.com/hub/jacob/multi-vector-retriever-german
const prompt: PromptTemplate = await pull( "jacob/multi-vector-retriever-german"
); const vectorstore = await MemoryVectorStore.fromTexts( [ "Gebäude werden aus Ziegelsteinen hergestellt", "Gebäude werden aus Holz hergestellt", "Gebäude werden aus Stein hergestellt", "Autos werden aus Metall hergestellt", "Autos werden aus Kunststoff hergestellt", "Mitochondrien sind die Energiekraftwerke der Zelle", "Mitochondrien bestehen aus Lipiden", ], [{ id: 1 }, { id: 2 }, { id: 3 }, { id: 4 }, { id: 5 }], embeddings
);
const model = new ChatAnthropic({});
const llmChain = new LLMChain({ llm: model, prompt, outputParser: new LineListOutputParser(),
});
const retriever = new MultiQueryRetriever({ retriever: vectorstore.asRetriever(), llmChain,
}); const query = "What are mitochondria made of?";
const retrievedDocs = await retriever.invoke(query); /* Generated queries: Was besteht ein Mitochondrium?,Aus welchen Komponenten setzt sich ein Mitochondrium zusammen? ,Welche Moleküle finden sich in einem Mitochondrium?
*/ console.log(retrievedDocs);
``` ---------------------------------------- TITLE: Initializing and Invoking Tavily Search Tool (TypeScript)
DESCRIPTION: This TypeScript snippet demonstrates how to import and instantiate the `TavilySearchResults` tool, configuring it to return a maximum of two results. It then shows how to invoke the tool with a specific search query, such as 'what is the weather in SF', and includes a required import for `cheerio` when used in notebooks.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#_snippet_2 LANGUAGE: typescript
CODE:
```
import "cheerio"; // This is required in notebooks to use the `CheerioWebBaseLoader`
import { TavilySearchResults } from "@langchain/community/tools/tavily_search" const search = new TavilySearchResults({ maxResults: 2
}); await search.invoke("what is the weather in SF")
``` ---------------------------------------- TITLE: Loading Multiple Files with MultiFileLoader in LangChain.js (TypeScript)
DESCRIPTION: This snippet demonstrates how to use the `MultiFileLoader` to efficiently load documents from a list of file paths, each potentially having a different extension. It configures a map of file extensions to their corresponding loader factories (e.g., JSONLoader, TextLoader), allowing `MultiFileLoader` to automatically select the correct loader for each file. The loaded documents from all files are then concatenated and logged.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/multi_file.mdx#_snippet_0 LANGUAGE: typescript
CODE:
```
import { MultiFileLoader } from "langchain/document_loaders/fs/multi_file";
import { JSONLoader, JSONLinesLoader,
} from "langchain/document_loaders/fs/json";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { CSVLoader } from "langchain/document_loaders/fs/csv"; const loader = new MultiFileLoader( [ "src/document_loaders/example_data/example/example.txt", "src/document_loaders/example_data/example/example.csv", "src/document_loaders/example_data/example2/example.json", "src/document_loaders/example_data/example2/example.jsonl", ], { ".json": (path) => new JSONLoader(path, "/texts"), ".jsonl": (path) => new JSONLinesLoader(path, "/html"), ".txt": (path) => new TextLoader(path), ".csv": (path) => new CSVLoader(path, "text"), }
);
const docs = await loader.load();
console.log({ docs });
``` ---------------------------------------- TITLE: Initializing ChatOpenAI LLM in JavaScript
DESCRIPTION: This code instantiates a ChatOpenAI language model with a specific model ('gpt-4o-mini') and a temperature of 0, which will serve as the underlying LLM for the VectorStoreToolkit.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/vectorstore.ipynb#_snippet_1 LANGUAGE: javascript
CODE:
```
import { ChatOpenAI } from "@langchain/openai"; const llm = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0,
})
``` ---------------------------------------- TITLE: Piping Final Prompt to Chat Model and Invoking (JavaScript)
DESCRIPTION: This snippet shows how to create a chain by piping the `finalPrompt` to a chat model instance. It then invokes the chain with a new input, demonstrating how the combined system message and few-shot examples guide the model's response to the user's query.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples_chat.ipynb#_snippet_3 LANGUAGE: JavaScript
CODE:
```
const chain = finalPrompt.pipe(model); await chain.invoke({ input: "What's the square of a triangle?" })
``` ---------------------------------------- TITLE: Binding Stop Sequences to a Model (TypeScript)
DESCRIPTION: This example shows how to use `Runnable.withConfig()` to bind a `stop` sequence directly to the `ChatOpenAI` model within a `RunnableSequence`. This allows the model to halt generation when the specified word ('SOLUTION') is encountered, simplifying the caller's interaction by pre-configuring the generation behavior.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/binding.ipynb#_snippet_1 LANGUAGE: TypeScript
CODE:
```
// stop generating after the equation is written
const equationFormatter = prompt .pipe(model.withConfig({ stop: ["SOLUTION"] })) .pipe(new StringOutputParser()); // generate only the equation, without needing to set the stop word
const formattedEquation = await equationFormatter.invoke({ equation_statement: "x raised to the third plus seven equals 12"
}); console.log(formattedEquation);
``` ---------------------------------------- TITLE: Invoking Query Analyzer with All Authors (Error Handling) (TypeScript)
DESCRIPTION: This snippet attempts to invoke the `queryAnalyzerAll` with a misspelled author name, demonstrating that providing an extremely long list of categorical values can lead to context window errors in the LLM. It includes basic error handling to catch and log such exceptions.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_high_cardinality.ipynb#_snippet_11 LANGUAGE: typescript
CODE:
```
try { const res = await queryAnalyzerAll.invoke("what are books about aliens by jess knight")
} catch (e) { console.error(e)
}
``` ---------------------------------------- TITLE: Defining Conversational Agent Prompt (TypeScript)
DESCRIPTION: This code defines a `ChatPromptTemplate` that provides a system message to guide the agent's behavior. The prompt instructs the agent to act as a helpful assistant and clarifies that tool usage is not always necessary, allowing for general conversation.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_tools.ipynb#_snippet_3 LANGUAGE: typescript
CODE:
```
import { ChatPromptTemplate,
} from "@langchain/core/prompts"; // Adapted from https://smith.langchain.com/hub/jacob/tool-calling-agent
const prompt = ChatPromptTemplate.fromMessages([ [ "system", "You are a helpful assistant. You may not need to use tools for every query - the user may just want to chat!", ],
]);
``` ---------------------------------------- TITLE: Initializing Embedding Model with LangChain.js
DESCRIPTION: This code initializes an `OpenAIEmbeddings` instance, specifying 'text-embedding-3-large' as the model. This embedding model is crucial for converting text into numerical vector representations, which are then used by the vector store for efficient similarity searches.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#_snippet_3 LANGUAGE: JavaScript
CODE:
```
// @lc-docs-hide-cell
import { OpenAIEmbeddings } from "@langchain/openai"; const embeddings = new OpenAIEmbeddings({model: "text-embedding-3-large"});
``` ---------------------------------------- TITLE: Adding Documents to PineconeStore in TypeScript
DESCRIPTION: This snippet illustrates how to add multiple `Document` objects to the initialized `PineconeStore`. It defines several example documents with `pageContent` and `metadata`, then uses the `addDocuments` method to ingest them into the vector store, optionally assigning specific IDs to each document for easier management.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pinecone.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
import type { Document } from "@langchain/core/documents"; const document1: Document = { pageContent: "The powerhouse of the cell is the mitochondria", metadata: { source: "https://example.com" }
}; const document2: Document = { pageContent: "Buildings are made out of brick", metadata: { source: "https://example.com" }
}; const document3: Document = { pageContent: "Mitochondria are made out of lipids", metadata: { source: "https://example.com" }
}; const document4: Document = { pageContent: "The 2024 Olympics are in Paris", metadata: { source: "https://example.com" }
} const documents = [document1, document2, document3, document4]; await vectorStore.addDocuments(documents, { ids: ["1", "2", "3", "4"] });
``` ---------------------------------------- TITLE: Invoking RunnableParallel Chain in TypeScript
DESCRIPTION: This snippet shows how to invoke a `RunnableParallel` instance with an input. The `invoke` method executes all contained runnables concurrently, providing the same input to each, and returns an object containing their respective outputs.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/lcel.mdx#_snippet_4 LANGUAGE: TypeScript
CODE:
```
const finalOutput = await chain.invoke(someInput);
``` ---------------------------------------- TITLE: Invoking LangChain RAG Chain (JavaScript)
DESCRIPTION: Invokes the previously defined RAG chain with a specific question, triggering the retrieval and generation process to produce an answer.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_sources.ipynb#_snippet_5 LANGUAGE: javascript
CODE:
```
await ragChain.invoke("What is task decomposition?")
``` ---------------------------------------- TITLE: Using OutputFixingParser to Correct Malformed LLM Output in LangChain.js
DESCRIPTION: This snippet shows how to instantiate and use the OutputFixingParser in LangChain.js. It initializes a ChatAnthropic model and then creates an OutputFixingParser by providing the LLM and the previously defined StructuredOutputParser. This setup allows the system to automatically attempt to fix malformed outputs by re-querying the LLM with the original parsing instructions and the incorrect output.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_fixing.ipynb#_snippet_1 LANGUAGE: JavaScript
CODE:
```
import { ChatAnthropic } from "@langchain/anthropic"; import { OutputFixingParser } from "langchain/output_parsers"; const model = new ChatAnthropic({ model: "claude-3-sonnet-20240229", maxTokens: 512, temperature: 0.1
}); const parserWithFix = OutputFixingParser.fromLLM(model, parser); await parserWithFix.parse(misformatted);
``` ---------------------------------------- TITLE: Streaming and Processing Chain Results (TypeScript)
DESCRIPTION: This snippet initiates a stream from the defined LCEL chain with the given input. It then asynchronously iterates over the `streamResult`, parses each item, stringifies it with pretty printing, and updates the `streamableValue` with the processed data. Finally, `stream.done()` signals the completion of the stream, and the `stream.value` is returned.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_tool_client.mdx#_snippet_7 LANGUAGE: typescript
CODE:
``` const streamResult = await chain.stream({ input, }); for await (const item of streamResult) { stream.update(JSON.parse(JSON.stringify(item, null, 2))); } stream.done(); })(); return { streamData: stream.value };
}
``` ---------------------------------------- TITLE: Integrating Qdrant Self-Query Retriever into a LangChain.js RAG Chain
DESCRIPTION: This snippet demonstrates how to construct a Retrieval-Augmented Generation (RAG) chain in LangChain.js using a Qdrant self-query retriever. It defines a chat prompt, a document formatter, and a `RunnableSequence` to process questions by retrieving relevant context and generating answers.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/qdrant.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers"; import type { Document } from "@langchain/core/documents"; const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided. Context: {context} Question: {question}`); const formatDocs = (docs: Document[]) => { return docs.map((doc) => JSON.stringify(doc)).join("\n\n");
} // See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([ { context: selfQueryRetriever.pipe(formatDocs), question: new RunnablePassthrough(), }, prompt, llm, new StringOutputParser(),
]);
``` ---------------------------------------- TITLE: Defining a History-Aware Question Contextualization Prompt - LangChain.js
DESCRIPTION: This snippet defines `contextualizeQPrompt` using `ChatPromptTemplate` and `MessagesPlaceholder` from `@langchain/core/prompts`. It creates a system prompt to rephrase user questions into standalone queries, incorporating chat history, and includes a placeholder for `chat_history` to maintain conversational context. This prompt is crucial for the `historyAwareRetriever` to understand the current query in the context of past conversation.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#_snippet_5 LANGUAGE: JavaScript
CODE:
```
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts"; const contextualizeQSystemPrompt = ( "Given a chat history and the latest user question " + "which might reference context in the chat history, " + "formulate a standalone question which can be understood " + "without the chat history. Do NOT answer the question, " + "just reformulate it if needed and otherwise return it as is."
) const contextualizeQPrompt = ChatPromptTemplate.fromMessages( [ ["system", contextualizeQSystemPrompt], new MessagesPlaceholder("chat_history"), ["human", "{input}"], ]
)
``` ---------------------------------------- TITLE: Performing Similarity Search with Score in LangChain.js Vector Store (TypeScript)
DESCRIPTION: This example shows how to perform a similarity search that also returns a relevance score for each document using vectorStore.similaritySearchWithScore. It applies a filter and then iterates through the results, logging both the document's content and metadata, along with its associated similarity score, formatted to three decimal places.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/google_cloudsql_pg.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
const filter = "\"source\" = \"https://example.com\"";
const resultsWithScores = await vectorStore.similaritySearchWithScore( "biology", 2, filter
); for (const [doc, score] of resultsWithScores) { console.log( `* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]` );
} ``` ---------------------------------------- TITLE: Performing Similarity Search on MongoDB Atlas Vector Store in TypeScript
DESCRIPTION: This snippet demonstrates a direct similarity search on the MongoDBAtlasVectorSearch instance. The similaritySearch method takes a query string and the desired number of results (k). It returns an array of Document objects that are most semantically similar to the query, which are then logged to the console.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
const similaritySearchResults = await vectorStore.similaritySearch("biology", 2); for (const doc of similaritySearchResults) { console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
``` ---------------------------------------- TITLE: Creating and Invoking a ChatPromptTemplate in LangChain.js
DESCRIPTION: This snippet illustrates how to construct a `ChatPromptTemplate` using `ChatPromptTemplate.fromMessages` to define a list of messages, including a system message and a user message with a variable. It shows how the template is invoked to generate a list of structured chat messages for a language model.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/prompt_templates.mdx#_snippet_1 LANGUAGE: typescript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts"; const promptTemplate = ChatPromptTemplate.fromMessages([ ["system", "You are a helpful assistant"], ["user", "Tell me a joke about {topic}"],
]); await promptTemplate.invoke({ topic: "cats" });
``` ---------------------------------------- TITLE: Splitting Documents into Chunks
DESCRIPTION: This snippet initializes a `RecursiveCharacterTextSplitter` to break down large documents into smaller, manageable chunks. It configures the splitter with a `chunkSize` of 500 characters and no `chunkOverlap`, then applies it to the `rawDocs` obtained from the document loader, producing an array of `allSplits`.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#_snippet_2 LANGUAGE: JavaScript
CODE:
```
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter"; const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 500, chunkOverlap: 0,
}); const allSplits = await textSplitter.splitDocuments(rawDocs);
``` ---------------------------------------- TITLE: Splitting Markdown Text with RecursiveCharacterTextSplitter (JS)
DESCRIPTION: This example demonstrates splitting Markdown formatted text using `RecursiveCharacterTextSplitter`. It shows how to configure the splitter for 'markdown' language, allowing it to intelligently break down Markdown content based on its structural elements like headings and code blocks.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/code_splitter.ipynb#_snippet_3 LANGUAGE: javascript
CODE:
```
const markdownText = `
# 🦜️🔗 LangChain ⚡ Building applications with LLMs through composability ⚡ ## Quick Install ```bash
# Hopefully this code block isn't split
pip install langchain
``` As an open-source project in a rapidly developing field, we are extremely open to contributions.
` const mdSplitter = RecursiveCharacterTextSplitter.fromLanguage( "markdown", { chunkSize: 60, chunkOverlap: 0, }
)
const mdDocs = await mdSplitter.createDocuments([markdownText]) mdDocs
``` ---------------------------------------- TITLE: Composing Chains with RunnableSequence.from (JavaScript)
DESCRIPTION: This snippet demonstrates an alternative way to compose chains using `RunnableSequence.from`, which allows for a list of runnables and inline functions. It achieves the same logical flow as the previous example, transforming the input for the analysis prompt and then invoking the combined chain.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sequence.ipynb#_snippet_4 LANGUAGE: JavaScript
CODE:
```
import { RunnableSequence } from "@langchain/core/runnables"; const composedChainWithLambda = RunnableSequence.from([ chain, (input) => ({ joke: input }), analysisPrompt, model, new StringOutputParser()
]) await composedChainWithLambda.invoke({ topic: "beets" })
``` ---------------------------------------- TITLE: Creating a LangChain Tool-Calling Chain
DESCRIPTION: This complex snippet constructs a LangChain expression language chain that integrates the Tavily tool with a language model. It defines a prompt, binds the tool to the LLM, and creates a RunnableLambda to handle user input, invoke the LLM, process tool calls, and return a final response.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search.ipynb#_snippet_6 LANGUAGE: typescript
CODE:
```
import { HumanMessage } from "@langchain/core/messages";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableLambda } from "@langchain/core/runnables"; const prompt = ChatPromptTemplate.fromMessages( [ ["system", "You are a helpful assistant."], ["placeholder", "{messages}"], ]
) const llmWithTools = llm.bindTools([tool]); const chain = prompt.pipe(llmWithTools); const toolChain = RunnableLambda.from( async (userInput: string, config) => { const humanMessage = new HumanMessage(userInput,); const aiMsg = await chain.invoke({ messages: [new HumanMessage(userInput)], }, config); const toolMsgs = await tool.batch(aiMsg.tool_calls, config); return chain.invoke({ messages: [humanMessage, aiMsg, ...toolMsgs], }, config); }
); const toolChainResult = await toolChain.invoke("what is the current weather in sf?");
``` ---------------------------------------- TITLE: Defining Zod Schema and Extraction Chain (JavaScript)
DESCRIPTION: This snippet defines Zod schemas for extracting key historical developments from text, including year, description, and evidence. It then constructs a ChatPromptTemplate for a system prompt and initializes a ChatOpenAI model. Finally, it creates an extractionChain by piping the prompt to the LLM with structured output based on the defined schema, suitable for tool calling.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_long_text.ipynb#_snippet_1 LANGUAGE: JavaScript
CODE:
```
import { z } from "zod";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai"; const keyDevelopmentSchema = z.object({ year: z.number().describe("The year when there was an important historic development."), description: z.string().describe("What happened in this year? What was the development?"), evidence: z.string().describe("Repeat verbatim the sentence(s) from which the year and description information were extracted"),
}).describe("Information about a development in the history of cars."); const extractionDataSchema = z.object({ key_developments: z.array(keyDevelopmentSchema),
}).describe("Extracted information about key developments in the history of cars"); const SYSTEM_PROMPT_TEMPLATE = [ "You are an expert at identifying key historic development in text.", "Only extract important historic developments. Extract nothing if no important information can be found in the text."
].join("\n"); // Define a custom prompt to provide instructions and any additional context.
// 1) You can add examples into the prompt template to improve extraction quality
// 2) Introduce additional parameters to take context into account (e.g., include metadata
// about the document from which the text was extracted.)
const prompt = ChatPromptTemplate.fromMessages([ [ "system", SYSTEM_PROMPT_TEMPLATE, ], // Keep on reading through this use case to see how to use examples to improve performance // MessagesPlaceholder('examples'), ["human", "{text}"],
]); // We will be using tool calling mode, which
// requires a tool calling capable model.
const llm = new ChatOpenAI({ model: "gpt-4-0125-preview", temperature: 0,
}); const extractionChain = prompt.pipe(llm.withStructuredOutput(extractionDataSchema));
``` ---------------------------------------- TITLE: Setting Up LangChain Components for QA Chain (JS)
DESCRIPTION: This snippet imports necessary LangChain components for building a question-answering chain. It defines a ChatPromptTemplate for structuring the input to the LLM and initializes a ChatOpenAI model, preparing the core elements for the RAG pipeline.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_per_user.ipynb#_snippet_5 LANGUAGE: JavaScript
CODE:
```
import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableBinding, RunnableLambda, RunnablePassthrough,
} from "@langchain/core/runnables";
import { ChatOpenAI, OpenAIEmbeddings } from "@langchain/openai"; const template = `Answer the question based only on the following context:
{context}
Question: {question}`; const prompt = ChatPromptTemplate.fromTemplate(template); const model = new ChatOpenAI({ model: "gpt-3.5-turbo-0125", temperature: 0,
});
``` ---------------------------------------- TITLE: Invoking Chat Model Without History (TypeScript)
DESCRIPTION: This snippet demonstrates a direct invocation of the `llm` (Chat Model) with a single user message using the `.invoke` method. It illustrates how the model processes an input without any prior conversational context, highlighting its stateless nature when called in isolation.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#_snippet_2 LANGUAGE: typescript
CODE:
```
await llm.invoke([{ role: "user", content: "Hi im bob" }])
``` ---------------------------------------- TITLE: Invoking a LangChain RAG Chain
DESCRIPTION: This snippet shows how to invoke the previously constructed ragChain. By passing a query string (represented by ...), the chain will execute the full RAG workflow: retrieving context, formatting it, passing it to the LLM with the question, and parsing the final output.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/bedrock-knowledge-bases.ipynb#_snippet_7 LANGUAGE: JavaScript
CODE:
```
await ragChain.invoke("...")
``` ---------------------------------------- TITLE: Invoking Prompt Template with Input
DESCRIPTION: Invokes the `promptTemplate` with example input values for `language` and `text`. This action generates a `ChatPromptValue` object, which contains the fully formatted messages ready to be sent to a language model.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/llm_chain.ipynb#_snippet_9 LANGUAGE: JavaScript
CODE:
```
const promptValue = await promptTemplate.invoke({ language: "italian", text: "hi!" }) promponentValue
``` ---------------------------------------- TITLE: Defining Zod Schemas for Person and People Extraction (JavaScript)
DESCRIPTION: This snippet defines two Zod schemas: `personSchema` and `peopleSchema`. `personSchema` describes the structure for extracting individual person information including name, hair color, and height. `peopleSchema` is an array of `personSchema` objects, suitable for extracting multiple person entities from text.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_examples.ipynb#_snippet_2 LANGUAGE: JavaScript
CODE:
```
import { z } from "zod"; const personSchema = z.object({ name: z.optional(z.string()).describe("The name of the person"), hair_color: z.optional(z.string()).describe("The color of the person's hair, if known"), height_in_meters: z.optional(z.string()).describe("Height measured in meters")
}).describe("Information about a person."); const peopleSchema = z.object({ people: z.array(personSchema)
});
``` ---------------------------------------- TITLE: Generating User-Specific Tools in TypeScript
DESCRIPTION: This snippet defines a `generateToolsForUser` function that dynamically creates a LangChain tool (`update_favorite_pets`) for a given user ID. It uses `zod` for schema validation and stores user-specific data in `userToPets`. This pattern is useful for binding runtime values without `async_hooks`.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_runtime.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
import { z } from "zod";
import { tool } from "@langchain/core/tools"; userToPets = {}; function generateToolsForUser(userId: string) { const updateFavoritePets = tool(async (input) => { userToPets[userId] = input.pets; return "update_favorite_pets called." }, { name: "update_favorite_pets", description: "add to the list of favorite pets.", schema: z.object({ pets: z.array(z.string()) }), }); // You can declare and return additional tools as well: return [updateFavoritePets];
}
``` ---------------------------------------- TITLE: Loading PDF Documents with PDFLoader (JavaScript)
DESCRIPTION: This snippet uses the `PDFLoader` from `@langchain/community` to parse a PDF file into a sequence of `Document` objects. It initializes the loader with a file path and then calls `load()` to asynchronously retrieve the documents. The number of loaded documents is then logged, typically one document per PDF page.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#_snippet_2 LANGUAGE: javascript
CODE:
```
import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf"; const loader = new PDFLoader("../../data/nke-10k-2023.pdf"); const docs = await loader.load();
console.log(docs.length)
``` ---------------------------------------- TITLE: Initializing ChatOpenAI Model in LangChain.js
DESCRIPTION: This code initializes a `ChatOpenAI` instance from `@langchain/openai`, configuring it to use the 'gpt-3.5-turbo' model. This `llm` object serves as the core language model that will be bound with tools for specific behaviors.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_choice.ipynb#_snippet_1 LANGUAGE: javascript
CODE:
```
import { ChatOpenAI } from '@langchain/openai'; const llm = new ChatOpenAI({ model: "gpt-3.5-turbo",
})
``` ---------------------------------------- TITLE: Streaming Agent Events to Client (TypeScript)
DESCRIPTION: This code iterates over the `streamEvents` from the `AgentExecutor`, which provides real-time updates on the agent's progress. Each event item is stringified and then parsed (due to a known RSC streaming bug) before being pushed to the `stream` variable, and `stream.done()` is called upon completion.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#_snippet_7 LANGUAGE: typescript
CODE:
``` const streamingEvents = agentExecutor.streamEvents( { input }, { version: "v2" }, ); for await (const item of streamingEvents) { stream.update(JSON.parse(JSON.stringify(item, null, 2))); } stream.done(); })();
``` ---------------------------------------- TITLE: Loading and Chunking Documents with CheerioWebBaseLoader (JS)
DESCRIPTION: This snippet demonstrates how to load web content from a specified URL using `CheerioWebBaseLoader` and then split the loaded documents into smaller, manageable chunks using `RecursiveCharacterTextSplitter`. It prepares the text data for indexing in a vector store by defining chunk size and overlap.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#_snippet_4 LANGUAGE: javascript
CODE:
```
import "cheerio";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio"; // Load and chunk contents of the blog
const pTagSelector = "p";
const cheerioLoader = new CheerioWebBaseLoader( "https://lilianweng.github.io/posts/2023-06-23-agent/", { selector: pTagSelector }
); const docs = await cheerioLoader.load(); const splitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200
});
const allSplits = await splitter.splitDocuments(docs);
``` ---------------------------------------- TITLE: Creating and Invoking LangChain Extraction Chain
DESCRIPTION: This snippet constructs a LangChain expression language chain by piping the `partialedPrompt`, the `model`, and the `parser` together. It then invokes this chain with the query to execute the full extraction process, from prompt formatting to model inference and output parsing.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_parse.ipynb#_snippet_5 LANGUAGE: javascript
CODE:
```
const chain = partialedPrompt.pipe(model).pipe(parser); await chain.invoke({ query });
``` ---------------------------------------- TITLE: Initializing LangGraph Workflow with MemorySaver - TypeScript
DESCRIPTION: This snippet defines the state schema for a LangGraph application, creates a 'callModel' function to process RAG chain responses and update chat history, and then constructs and compiles a LangGraph workflow. It uses 'MemorySaver' for in-memory persistence of chat history, enabling stateful management for multi-turn conversations.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#_snippet_8 LANGUAGE: TypeScript
CODE:
```
import { AIMessage, BaseMessage, HumanMessage } from "@langchain/core/messages";
import { StateGraph, START, END, MemorySaver, messagesStateReducer, Annotation } from "@langchain/langgraph"; // Define the State interface
const GraphAnnotation = Annotation.Root({ input: Annotation<string>(), chat_history: Annotation<BaseMessage[]> ({ reducer: messagesStateReducer, default: () => [], }), context: Annotation<string>(), answer: Annotation<string>(),
}) // Define the call_model function
async function callModel(state: typeof GraphAnnotation.State) { const response = await ragChain.invoke(state); return { chat_history: [ new HumanMessage(state.input), new AIMessage(response.answer), ], context: response.context, answer: response.answer, };
} // Create the workflow
const workflow = new StateGraph(GraphAnnotation) .addNode("model", callModel) .addEdge(START, "model") .addEdge("model", END); // Compile the graph with a checkpointer object
const memory = new MemorySaver();
const app = workflow.compile({ checkpointer: memory });
``` ---------------------------------------- TITLE: Binding Tools to a Model with `bindTools` in TypeScript
DESCRIPTION: This snippet illustrates the general usage of the `.bindTools()` method to connect a list of tools to a model. This method makes the specified tools available for the model to call, providing a standardized interface across different model providers.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tool_calling.mdx#_snippet_3 LANGUAGE: TypeScript
CODE:
```
const modelWithTools = model.bindTools([toolsList]);
``` ---------------------------------------- TITLE: Initializing LangGraph ReAct Agent (JavaScript)
DESCRIPTION: This code initializes a ReAct agent using LangGraph's pre-built 'createReactAgent' function. It configures the agent with the language model (llm), the set of SQL tools, and the formatted system message to guide its decision-making process.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#_snippet_23 LANGUAGE: JavaScript
CODE:
```
import { createReactAgent } from "@langchain/langgraph/prebuilt"; const agent = createReactAgent({ llm: llm, tools: tools, stateModifier: systemMessage });
``` ---------------------------------------- TITLE: Performing Similarity Search on LangChain Vector Store (TypeScript)
DESCRIPTION: This snippet shows how to perform a basic similarity search on a LangChain vector store using `vectorStore.similaritySearch`. It queries for relevant documents based on a text input, limits the number of results, and applies a metadata filter, then logs the content and metadata of each found document.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/supabase.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
const filter = { source: "https://example.com" }; const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, filter); for (const doc of similaritySearchResults) { console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
``` ---------------------------------------- TITLE: Creating USearch Index from Document Loader - TypeScript
DESCRIPTION: Illustrates how to populate a USearch vector index using documents loaded from a source, such as a text file. It leverages a `TextLoader` to process documents and `OpenAIEmbeddings` to convert them into vectors before indexing them into USearch.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/usearch.mdx#_snippet_3 LANGUAGE: typescript
CODE:
```
import { USearch } from "@langchain/community/vectorstores/usearch";
import { OpenAIEmbeddings } from "@langchain/openai/embeddings";
import { TextLoader } from "langchain/document_loaders/fs/text"; // Assuming 'src/document_loaders/example.txt' exists with some text content
const loader = new TextLoader("src/document_loaders/example.txt");
const docs = await loader.load();
const embeddings = new OpenAIEmbeddings(); const vectorStore = await USearch.fromDocuments(docs, embeddings); const result = await vectorStore.similaritySearch("example", 1);
console.log(result);
``` ---------------------------------------- TITLE: Applying Advanced Filtering to Similarity Searches in TypeScript
DESCRIPTION: This TypeScript example demonstrates how to use advanced filtering capabilities during similarity searches in the SAP HANA Cloud Vector Store. It shows how to apply various operators like `$eq`, `$gt`, and logical `$and` to filter results based on document metadata, allowing for more precise and contextual retrievals.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hanavector.mdx#_snippet_6 LANGUAGE: typescript
CODE:
```
import { SAPHanaCloudVectorStore } from "@langchain/community/vectorstores/sap_hana";
import { OpenAIEmbeddings } from "@langchain/openai"; const embeddings = new OpenAIEmbeddings();
const vectorStore = new SAPHanaCloudVectorStore(embeddings, { user: process.env.HANA_USER, password: process.env.HANA_PASSWORD, host: process.env.HANA_HOST, port: process.env.HANA_PORT, schema: process.env.HANA_SCHEMA,
}); await vectorStore.addDocuments([ { pageContent: "Apple is a fruit", metadata: { type: "fruit", color: "red", weight: 100 } }, { pageContent: "Banana is a fruit", metadata: { type: "fruit", color: "yellow", weight: 120 } }, { pageContent: "Carrot is a vegetable", metadata: { type: "vegetable", color: "orange", weight: 50 } }
]); // Example: Filter by equality
const resultEq = await vectorStore.similaritySearch("fruit", 1, { type: { $eq: "fruit" }
});
console.log("Filtered by type (fruit):", resultEq); // Example: Filter by greater than
const resultGt = await vectorStore.similaritySearch("heavy", 1, { weight: { $gt: 100 }
});
console.log("Filtered by weight (>100):", resultGt); // Example: Filter with $and
const resultAnd = await vectorStore.similaritySearch("red fruit", 1, { $and: [{ type: { $eq: "fruit" } }, { color: { $eq: "red" } }]
});
console.log("Filtered by type and color:", resultAnd);
``` ---------------------------------------- TITLE: Embedding Multiple Texts using embedDocuments in Langchain.js
DESCRIPTION: This snippet demonstrates how to embed multiple text strings into vector representations using the `embedDocuments` method. It shows how to pass an array of texts (e.g., `text` and `text2`) to the embeddings instance and then logs a slice of the resulting vectors for verification. This method is suitable for indexing large batches of documents.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/cohere.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs"; const vectors = await embeddings.embedDocuments([text, text2]); console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
``` ---------------------------------------- TITLE: Embedding Multiple Documents with Azure OpenAI Embeddings in LangChain.js
DESCRIPTION: This snippet shows how to use the embedDocuments method to create embeddings for multiple texts, typically for indexing purposes. The internal implementation for this method might differ from embedQuery but serves to vectorize a collection of documents efficiently.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/azure_openai.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs"; const vectors = await embeddings.embedDocuments([text, text2]); console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
``` ---------------------------------------- TITLE: Configuring LangChain SelfQueryRetriever with Qdrant (TypeScript)
DESCRIPTION: This snippet demonstrates how to instantiate a `SelfQueryRetriever` using the previously defined LLM and Qdrant vector store. It configures the retriever with `documentContents` for context, `attributeInfo` for metadata querying, and a `QdrantTranslator` to convert natural language queries into structured Qdrant filters.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/qdrant.ipynb#_snippet_5 LANGUAGE: typescript
CODE:
```
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { QdrantTranslator } from "@langchain/community/structured_query/qdrant"; const selfQueryRetriever = SelfQueryRetriever.fromLLM({ llm: llm, vectorStore: vectorStore, /** A short summary of what the document contents represent. */ documentContents: "Brief summary of a movie", attributeInfo: attributeInfo, structuredQueryTranslator: new QdrantTranslator(),
});
``` ---------------------------------------- TITLE: Initializing PineconeStore and Adding Namespaced Documents (JS)
DESCRIPTION: This snippet initializes OpenAIEmbeddings and PineconeStore using an existing Pinecone index. It demonstrates adding documents to the vector store, explicitly assigning them to different namespaces ('harrison' and 'ankush') to partition data for multiple users. This setup is crucial for isolating user-specific information within a shared index.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_per_user.ipynb#_snippet_2 LANGUAGE: JavaScript
CODE:
```
import { OpenAIEmbeddings } from "@langchain/openai";
import { PineconeStore } from "@langchain/pinecone";
import { Pinecone } from "@pinecone-database/pinecone";
import { Document } from "@langchain/core/documents"; const embeddings = new OpenAIEmbeddings(); const pinecone = new Pinecone(); const pineconeIndex = pinecone.Index(process.env.PINECONE_INDEX); /** * Pinecone allows you to partition the records in an index into namespaces. * Queries and other operations are then limited to one namespace, * so different requests can search different subsets of your index. * Read more about namespaces here: https://docs.pinecone.io/guides/indexes/use-namespaces * * NOTE: If you have namespace enabled in your Pinecone index, you must provide the namespace when creating the PineconeStore. */
const namespace = "pinecone"; const vectorStore = await PineconeStore.fromExistingIndex( new OpenAIEmbeddings(), { pineconeIndex, namespace },
); await vectorStore.addDocuments( [new Document({ pageContent: "i worked at kensho" })], { namespace: "harrison" },
); await vectorStore.addDocuments( [new Document({ pageContent: "i worked at facebook" })], { namespace: "ankush" },
);
``` ---------------------------------------- TITLE: Embedding Multiple Texts for Indexing (TypeScript)
DESCRIPTION: This snippet shows how to use the `embedDocuments` method to generate vector representations for multiple texts. This method is typically used for indexing data, and its internal implementation might differ from `embedQuery`.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/openai.ipynb#_snippet_5 LANGUAGE: typescript
CODE:
```
const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs"; const vectors = await embeddings.embedDocuments([text, text2]); console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
``` ---------------------------------------- TITLE: Performing Similarity Search in Convex Vector Store (TypeScript)
DESCRIPTION: This TypeScript action demonstrates how to perform a similarity search within the Convex vector store. It initializes `ConvexVectorStore` with an embeddings model (e.g., OpenAIEmbeddings) and then uses the `similaritySearch` method to find documents most similar to a given query, returning the top 'k' results.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/convex.mdx#_snippet_5 LANGUAGE: typescript
CODE:
```
import { ConvexVectorStore } from "@langchain/community/vectorstores/convex";
import { OpenAIEmbeddings } from "@langchain/openai";
import { action } from "convex/server"; export const search = action({ handler: async (ctx) => { const embeddings = new OpenAIEmbeddings(); const vectorStore = new ConvexVectorStore(ctx, embeddings); const query = "Hello"; const results = await vectorStore.similaritySearch(query, 1); return results; }
});
``` ---------------------------------------- TITLE: Embedding a Single Text with ByteDanceDoubaoEmbeddings (JavaScript)
DESCRIPTION: This snippet demonstrates direct usage of `embeddings.embedQuery` to generate a vector representation for a single text query. This method is typically used for search operations.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/bytedance_doubao.ipynb#_snippet_3 LANGUAGE: javascript
CODE:
```
const singleVector = await embeddings.embedQuery(text); console.log(singleVector.slice(0, 100));
``` ---------------------------------------- TITLE: Performing Similarity Search on PGVector Store (TypeScript)
DESCRIPTION: This snippet demonstrates a direct similarity search on the vector store. It queries for documents related to 'biology', limits results to 2, and applies a metadata filter. The results are then iterated and logged, showing `pageContent` and `metadata`.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pgvector.ipynb#_snippet_6 LANGUAGE: typescript
CODE:
```
const filter = { source: "https://example.com" }; const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, filter); for (const doc of similaritySearchResults) { console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
``` ---------------------------------------- TITLE: Continuing Conversation with LangGraph Agent to Test Memory (TypeScript)
DESCRIPTION: This snippet demonstrates a follow-up interaction with the previously initialized LangGraph agent. It sends a new user message asking the agent to recall information from the prior turn, specifically the user's name. The streamed output confirms the agent's ability to maintain conversational context and memory.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/conversation_buffer_window_memory.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
const followUpMessage2 = { role: "user", content: "do you remember my name?",
}; for await (const event of await app2.stream({ messages: [followUpMessage2] }, config2)) { const lastMessage = event.messages[event.messages.length - 1]; console.log(lastMessage.content);
}
``` ---------------------------------------- TITLE: Defining LangGraph Application Flow in JavaScript
DESCRIPTION: This snippet defines the structure of a LangGraph application using `StateGraph`. It adds nodes for `writeQuery`, `executeQuery`, and `generateAnswer`, and then defines the sequential edges connecting these steps from start to end, establishing the control flow.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#_snippet_12 LANGUAGE: JavaScript
CODE:
```
import { StateGraph } from "@langchain/langgraph"; const graphBuilder = new StateGraph({ stateSchema: StateAnnotation,
}) .addNode("writeQuery", writeQuery) .addNode("executeQuery", executeQuery) .addNode("generateAnswer", generateAnswer) .addEdge("__start__", "writeQuery") .addEdge("writeQuery", "executeQuery") .addEdge("executeQuery", "generateAnswer") .addEdge("generateAnswer", "__end__")
``` ---------------------------------------- TITLE: Setting LangSmith Observability Environment Variables
DESCRIPTION: This snippet provides shell commands to set environment variables for LangSmith, enabling best-in-class observability for LangChain applications. It includes `LANGSMITH_API_KEY` for authentication and `LANGSMITH_TRACING=true` to activate tracing. An optional variable `LANGCHAIN_CALLBACKS_BACKGROUND` is also shown to reduce tracing latency in non-serverless environments.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_few_shot.ipynb#_snippet_1 LANGUAGE: shell
CODE:
```
LANGSMITH_API_KEY=your-api-key
LANGSMITH_TRACING=true # Reduce tracing latency if you are not in a serverless environment
# LANGCHAIN_CALLBACKS_BACKGROUND=true
``` ---------------------------------------- TITLE: Defining Personal Information Schema with Zod (JavaScript)
DESCRIPTION: Defines a Zod schema for extracting personal information, including name, hair color, and height. Attributes are optional (`z.optional`) to prevent the LLM from fabricating data, and are documented with `.describe()` for better LLM understanding. This schema is used to structure extracted data from unstructured text.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/extraction.ipynb#_snippet_1 LANGUAGE: javascript
CODE:
```
import { z } from "zod"; const personSchema = z.object({ name: z.optional(z.string()).describe("The name of the person"), hair_color: z.optional(z.string()).describe("The color of the person's hair if known"), height_in_meters: z.optional(z.string()).describe('Height measured in meters')
});
``` ---------------------------------------- TITLE: Indexing and Retrieving Documents with MemoryVectorStore (TypeScript)
DESCRIPTION: This snippet illustrates how to use `OpenAIEmbeddings` with `MemoryVectorStore` for indexing and retrieving documents. It creates a vector store from a sample text, configures it as a retriever, and then performs a similarity search to retrieve the most relevant document.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/openai.ipynb#_snippet_3 LANGUAGE: typescript
CODE:
```
// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory"; const text = "LangChain is the framework for building context-aware reasoning applications"; const vectorstore = await MemoryVectorStore.fromDocuments( [{ pageContent: text, metadata: {} }], embeddings,
); // Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1); // Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?"); retrievedDocuments[0].pageContent;
``` ---------------------------------------- TITLE: Invoking a LangChain RAG Chain (TypeScript)
DESCRIPTION: This snippet shows how to invoke the previously defined `ragChain` with a specific query. It demonstrates the execution of the RAG process, where the retriever will first find relevant documents based on the query, and then the LLM will generate an answer using those documents.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/supabase.ipynb#_snippet_8 LANGUAGE: TypeScript
CODE:
```
await ragChain.invoke("Which movies are rated higher than 8.5?");
``` ---------------------------------------- TITLE: Invoking LangGraph with Initial Query (TypeScript)
DESCRIPTION: This snippet shows how to invoke the graphWithMemory for the first conversational turn. It sends an initial user query about 'Task Decomposition' and streams the responses, printing the last message from each step. The threadConfig ensures the history is associated with the specified thread_id.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#_snippet_16 LANGUAGE: TypeScript
CODE:
```
let inputs3 = { messages: [{ role: "user", content: "What is Task Decomposition?" }] }; for await ( const step of await graphWithMemory.stream(inputs3, threadConfig)
) { const lastMessage = step.messages[step.messages.length - 1]; prettyPrint(lastMessage); console.log("-----\n");
}
``` ---------------------------------------- TITLE: Observing All Streaming Events from a LangChain Chain (JS)
DESCRIPTION: This snippet demonstrates how to initialize a LangChain chain with a model and a JSON output parser, then stream all events using `streamEvents` with `version: "v2"`. It collects all events into an array to show the total count, illustrating the comprehensive event stream from the chain, model, and parser components.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#_snippet_13 LANGUAGE: JavaScript
CODE:
```
const chain = model.pipe(new JsonOutputParser());
const eventStream = await chain.streamEvents( `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`, { version: "v2" }
); const events = [];
for await (const event of eventStream) { events.push(event);
} console.log(events.length)
``` ---------------------------------------- TITLE: Streaming LangGraph Execution Results in JavaScript
DESCRIPTION: This snippet shows how to stream the results of a LangGraph application's execution. It initializes an input question and then iterates through the streamed updates from the `graph.stream` method, logging each step's output as it becomes available.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#_snippet_15 LANGUAGE: JavaScript
CODE:
```
let inputs = { question: "How many employees are there?" } console.log(inputs)
console.log("\n====\n");
for await ( const step of await graph.stream(inputs, { streamMode: "updates", })
) { console.log(step); console.log("\n====\n");
}
``` ---------------------------------------- TITLE: Starting a New LangGraph Conversation Thread (JavaScript)
DESCRIPTION: This snippet demonstrates how to initiate a *new* conversation thread by creating a new `config` object with a different `thread_id`. When invoked, the application will treat this as a fresh conversation, as indicated by the model not remembering previous context from other threads.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#_snippet_9 LANGUAGE: javascript
CODE:
```
const config2 = { configurable: { thread_id: uuidv4() } }
const input3 = [ { role: "user", content: "What's my name?", }
]
const output3 = await app.invoke({ messages: input3 }, config2)
console.log(output3.messages[output3.messages.length - 1]);
``` ---------------------------------------- TITLE: Invoking Chat Model with Formatted Prompt
DESCRIPTION: Invokes the `model` (an instance of a language model) with the `promptValue` obtained from the prompt template. After the model processes the prompt, the `content` of the `response` is logged to the console, demonstrating the final interaction with the LLM.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/llm_chain.ipynb#_snippet_11 LANGUAGE: JavaScript
CODE:
```
const response = await model.invoke(promptValue)
console.log(`${response.content}`)
``` ---------------------------------------- TITLE: Invoking the RAG Chain (TypeScript)
DESCRIPTION: This code snippet shows how to execute the `ragChain` defined previously, passing a specific question as input. The chain will internally use the self-query retriever to find relevant documents, format them, and then generate an answer using the LLM based on the prompt and retrieved context. The output will be the LLM's response to the question.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/chroma.ipynb#_snippet_6 LANGUAGE: TypeScript
CODE:
```
await ragChain.invoke("Which movies are rated higher than 8.5?")
``` ---------------------------------------- TITLE: Invoking ChatAnthropic Model - JavaScript
DESCRIPTION: This example demonstrates how to invoke the `ChatAnthropic` model with a list of messages, including a system prompt and a human prompt, to generate a chat completion.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#_snippet_4 LANGUAGE: javascript
CODE:
```
const aiMsg = await llm.invoke([ [ "system", "You are a helpful assistant that translates English to French. Translate the user sentence.", ], ["human", "I love programming."],
])
aiMsg
``` ---------------------------------------- TITLE: Instantiating AzureOpenAI LLM in LangChain.js (TypeScript/JavaScript)
DESCRIPTION: This snippet demonstrates how to create an instance of the `AzureOpenAI` Large Language Model (LLM) in LangChain.js. It shows how to configure essential parameters like the model name, API key, instance name, deployment name, and API version. These parameters can be provided directly or default to environment variables, allowing for flexible credential management.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/azure.ipynb#_snippet_3 LANGUAGE: TypeScript
CODE:
```
import { AzureOpenAI } from "@langchain/openai" const llm = new AzureOpenAI({ model: "gpt-3.5-turbo-instruct", azureOpenAIApiKey: "<your_key>", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY azureOpenAIApiInstanceName: "<your_instance_name>", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME azureOpenAIApiDeploymentName: "<your_deployment_name>", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME azureOpenAIApiVersion: "<api_version>", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION temperature: 0, maxTokens: undefined, timeout: undefined, maxRetries: 2, // other params...
})
``` ---------------------------------------- TITLE: Initializing ChatOpenAI Model for LLM Chain
DESCRIPTION: This snippet demonstrates how to import and instantiate a `ChatOpenAI` model from `@langchain/openai`. This model serves as the Large Language Model (LLM) component within a LangChain application, configured with a specific model and temperature.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/exa.ipynb#_snippet_4 LANGUAGE: typescript
CODE:
```
import { ChatOpenAI } from "@langchain/openai"; const llm = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0,
});
``` ---------------------------------------- TITLE: Chaining Cohere LLM with Prompt Template - JavaScript
DESCRIPTION: Illustrates how to chain the Cohere LLM with a `PromptTemplate` from `@langchain/core/prompts`. This allows dynamic prompt construction based on input variables before sending to the LLM, enabling more complex interactions.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/cohere.ipynb#_snippet_5 LANGUAGE: javascript
CODE:
```
import { PromptTemplate } from "@langchain/core/prompts" const prompt = new PromptTemplate({ template: "How to say {input} in {output_language}:\n", inputVariables: ["input", "output_language"],
}) const chain = prompt.pipe(llm);
await chain.invoke( { output_language: "German", input: "I love programming.", }
)
``` ---------------------------------------- TITLE: Merging Input and Output with RunnablePassthrough.assign() (TypeScript)
DESCRIPTION: Illustrates how to extend an input dictionary with new keys derived from runnable outputs using `RunnablePassthrough.assign()`. This method allows adding new fields to the output while preserving or transforming existing input fields, effectively merging results.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { RunnableLambda, RunnablePassthrough } from "@langchain/core/runnables"; const runnable = RunnableLambda.from((x: { foo: number }) => { return x.foo + 7;
}); const chain = RunnablePassthrough.assign({ bar: runnable,
}); await chain.invoke({ foo: 10 });
``` ---------------------------------------- TITLE: Streaming AIMessageChunks from Model (TypeScript)
DESCRIPTION: This snippet demonstrates how to stream responses from a LangChain chat model using `model.stream`. It iterates over `AIMessageChunk` objects as they are generated, allowing for real-time display of the AI's response.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/messages.mdx#_snippet_4 LANGUAGE: typescript
CODE:
```
for await (const chunk of model.stream([ new HumanMessage("what color is the sky?"),
])) { console.log(chunk);
}
``` ---------------------------------------- TITLE: Streaming Partial JSON Output with LangChain (TypeScript)
DESCRIPTION: This snippet shows how to stream partial JSON output from a LangChain chain that uses `JsonOutputParser`. It iterates over the streamed chunks, allowing for real-time processing of the model's output as it's generated, which is useful for large or incrementally generated JSON.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_json.ipynb#_snippet_1 LANGUAGE: TypeScript
CODE:
```
for await (const s of await chain.stream({ query: jokeQuery })) { console.log(s)
}
``` ---------------------------------------- TITLE: Initializing MCP Client and Running LangGraph Agent with Custom Tools (TypeScript)
DESCRIPTION: This TypeScript example demonstrates the full workflow of integrating an MCP client with LangChain and LangGraph. It initializes a `ChatOpenAI` model, sets up a `StdioClientTransport` to connect to an MCP server, and then uses `loadMcpTools` to fetch tools with custom configurations like `throwOnLoadError` and `prefixToolNameWithServerName`. Finally, it creates and invokes a `createReactAgent` with the loaded tools and handles potential errors, ensuring proper client cleanup.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mcp-adapters/README.md#_snippet_3 LANGUAGE: typescript
CODE:
```
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";
import { ChatOpenAI } from "@langchain/openai";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { loadMcpTools } from "@langchain/mcp-adapters"; // Initialize the ChatOpenAI model
const model = new ChatOpenAI({ modelName: "gpt-4" }); // Automatically starts and connects to a MCP reference server
const transport = new StdioClientTransport({ command: "npx", args: ["-y", "@modelcontextprotocol/server-math"],
}); // Initialize the client
const client = new Client({ name: "math-client", version: "1.0.0",
}); try { // Connect to the transport await client.connect(transport); // Get tools with custom configuration const tools = await loadMcpTools("math", client, { // Whether to throw errors if a tool fails to load (optional, default: true) throwOnLoadError: true, // Whether to prefix tool names with the server name (optional, default: false) prefixToolNameWithServerName: false, // Optional additional prefix for tool names (optional, default: "mcp") additionalToolNamePrefix: "mcp", // Use standardized content block format in tool outputs useStandardContentBlocks: true, }); // Create and run the agent const agent = createReactAgent({ llm: model, tools }); const agentResponse = await agent.invoke({ messages: [{ role: "user", content: "what's (3 + 5) x 12?" }], }); console.log(agentResponse);
} catch (e) { console.error(e);
} finally { // Clean up connection await client.close();
}
``` ---------------------------------------- TITLE: Defining Map Prompt for Map-Reduce - LangChain.js
DESCRIPTION: This snippet defines a `ChatPromptTemplate` to be used as the 'map' prompt in a Map-Reduce summarization workflow. This prompt instructs the LLM to write a concise summary of the provided context, which will be applied to individual documents in the map step.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#_snippet_6 LANGUAGE: JavaScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts"; const mapPrompt = ChatPromptTemplate.fromMessages( [ ["user", "Write a concise summary of the following: \n\n{context}"] ]
)
``` ---------------------------------------- TITLE: Defining the `runAgent` Server Action (TypeScript)
DESCRIPTION: This defines the asynchronous `runAgent` function, marked as a server action (`"use server"`), which will encapsulate all the logic for the LangChain agent and handle streaming data back to the client. It takes a single string input as the agent's query.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#_snippet_3 LANGUAGE: typescript
CODE:
```
export async function runAgent(input: string) { "use server";
}
``` ---------------------------------------- TITLE: Indexing and Retrieving Documents with MemoryVectorStore in TypeScript
DESCRIPTION: This example demonstrates a common RAG (Retrieval-Augmented Generation) flow, showing how to index a sample document into a `MemoryVectorStore` using the `PineconeEmbeddings` instance. It then configures the vector store as a retriever and invokes it to find the most similar document based on a query.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/pinecone.ipynb#_snippet_3 LANGUAGE: typescript
CODE:
```
// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory"; const text = "LangChain is the framework for building context-aware reasoning applications"; const vectorstore = await MemoryVectorStore.fromDocuments( [{ pageContent: text, metadata: {} }], embeddings,
); // Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1); // Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?"); retrievedDocuments[0].pageContent;
``` ---------------------------------------- TITLE: Invoking LangChain Chat Model with Prompt (TypeScript)
DESCRIPTION: This snippet shows how to create a simple LangChain chain by piping a `ChatPromptTemplate` to a `ChatAnthropic` model. It then invokes the chain with an input object `{ topic: "bears" }`, sending the formatted prompt to the Anthropic model and awaiting its response.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/anthropic.mdx#_snippet_3 LANGUAGE: typescript
CODE:
```
const chain = prompt.pipe(model);
await chain.invoke({ topic: "bears" });
``` ---------------------------------------- TITLE: Creating a Document Combination Chain
DESCRIPTION: This code sets up a `createStuffDocumentsChain` to combine retrieved documents with a chat model and a prompt. It defines a system template for answering questions based on provided context, including a fallback for irrelevant information. The `ChatPromptTemplate` is constructed to accept both system context and chat history messages, which are then used to initialize the `documentChain`.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#_snippet_5 LANGUAGE: JavaScript
CODE:
```
import { createStuffDocumentsChain } from "langchain/chains/combine_documents";
import { ChatPromptTemplate, MessagesPlaceholder,
} from "@langchain/core/prompts"; const SYSTEM_TEMPLATE = `Answer the user's questions based on the below context. If the context doesn't contain any relevant information to the question, don't make something up and just say "I don't know": <context>
{context}
</context>
`; const questionAnsweringPrompt = ChatPromptTemplate.fromMessages([ ["system", SYSTEM_TEMPLATE], new MessagesPlaceholder("messages"),
]); const documentChain = await createStuffDocumentsChain({ llm, prompt: questionAnsweringPrompt,
});
``` ---------------------------------------- TITLE: Implementing Session-Based InMemoryChatMessageHistory in TypeScript
DESCRIPTION: This TypeScript code defines a mechanism to manage InMemoryChatMessageHistory instances, associating each with a unique sessionId. It ensures that a dedicated chat history is available for each session, creating a new one if it doesn't already exist, which is crucial for tracking distinct conversations.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/chat_history.ipynb#_snippet_1 LANGUAGE: typescript
CODE:
```
import { InMemoryChatMessageHistory } from "@langchain/core/chat_history"; const chatsBySessionId: Record<string, InMemoryChatMessageHistory> = {} const getChatHistory = (sessionId: string) => { let chatHistory: InMemoryChatMessageHistory | undefined = chatsBySessionId[sessionId] if (!chatHistory) { chatHistory = new InMemoryChatMessageHistory() chatsBySessionId[sessionId] = chatHistory } return chatHistory
}
``` ---------------------------------------- TITLE: Setting OpenAI API Key (Bash)
DESCRIPTION: This snippet demonstrates how to set the `OPENAI_API_KEY` environment variable in a Bash shell. This key is required to authenticate with the OpenAI API for accessing embedding models.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/openai.ipynb#_snippet_0 LANGUAGE: bash
CODE:
```
export OPENAI_API_KEY="your-api-key"
``` ---------------------------------------- TITLE: Integrating Weaviate Self-Query Retriever into a LangChain.js RAG Chain
DESCRIPTION: This snippet demonstrates how to incorporate a Weaviate self-query retriever into a LangChain.js Retrieval-Augmented Generation (RAG) chain. It defines a prompt template, a document formatting function, and constructs a `RunnableSequence` to process user questions, retrieve relevant documents, and generate answers. The `formatDocs` function ensures metadata is included in the context.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/weaviate.ipynb#_snippet_7 LANGUAGE: TypeScript
CODE:
```
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers"; import type { Document } from "@langchain/core/documents"; const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided. Context: {context} Question: {question}`); const formatDocs = (docs: Document[]) => { return docs.map((doc) => JSON.stringify(doc)).join("\n\n");
} // See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([ { context: selfQueryRetriever.pipe(formatDocs), question: new RunnablePassthrough(), }, prompt, llm, new StringOutputParser(),
]);
``` ---------------------------------------- TITLE: Configuring LangSmith Tracing Environment Variables
DESCRIPTION: This snippet demonstrates how to set environment variables to enable LangSmith tracing for monitoring and debugging LangChain applications. It includes `LANGSMITH_TRACING` to activate tracing and `LANGSMITH_API_KEY` for authentication, with an optional variable to reduce latency.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#_snippet_1 LANGUAGE: shell
CODE:
```
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..." # Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
``` ---------------------------------------- TITLE: Configuring LangSmith Tracing in TypeScript
DESCRIPTION: This snippet shows how to enable LangSmith tracing and set the LangSmith API key as environment variables for automated tracing of model calls. The lines are commented out, indicating they are optional.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/faiss.ipynb#_snippet_1 LANGUAGE: typescript
CODE:
```
// process.env.LANGSMITH_TRACING="true"
// process.env.LANGSMITH_API_KEY="your-api-key"
``` ---------------------------------------- TITLE: Indexing and Retrieving Data with CohereEmbeddings and MemoryVectorStore
DESCRIPTION: This snippet demonstrates a basic Retrieval-Augmented Generation (RAG) flow, showing how to index a sample document into a `MemoryVectorStore` using `CohereEmbeddings` and then retrieve the most similar text. It initializes a vector store, converts it to a retriever, and invokes it with a query.
SOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/cohere.ipynb#_snippet_4 LANGUAGE: javascript
CODE:
```
// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory"; const text = "LangChain is the framework for building context-aware reasoning applications"; const vectorstore = await MemoryVectorStore.fromDocuments( [{ pageContent: text, metadata: {} }], embeddings,
); // Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1); // Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?"); retrievedDocuments[0].pageContent;
```